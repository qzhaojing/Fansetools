#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fansetools count ç»„ä»¶ - ç”¨äºå¤„ç†fanse3æ–‡ä»¶çš„readè®¡æ•°

-count gene level
-count transcript level
-count exon level
-count cds level 
-count 5utr
-count 3utr
å¦‚æœæƒ³å®ç°è¿™ä¸ªï¼Œå¯èƒ½å¾—è½¬æ¢åæ ‡ï¼Œå°†åŸºå› ç»„åæ ‡è½¬æ¢ä¸ºè½¬å½•ç»„åæ ‡ï¼Œrefflatæ–‡ä»¶ä¸­å¯¹åº”çš„åæ ‡éƒ½è¿›è¡Œæ›´æ”¹ï¼Œå…¨éƒ¨éƒ½å‡å»ç¬¬ä¸€ä¸ªstartæ¥è½¬æ¢ã€‚
é•¿åº¦éœ€è¦æœ‰å¤šä¸ªï¼Œ
"""

import os
import sys
import argparse
import pandas as pd
import glob
from collections import Counter,defaultdict
from tqdm import tqdm
import time
from pathlib import Path
# import defaultdict

# å¯¼å…¥æ–°çš„è·¯å¾„å¤„ç†å™¨
from fansetools.utils.path_utils import PathProcessor
# å¯¼å…¥æ–°çš„fanse_parser
from fansetools.parser import fanse_parser, FANSeRecord,fanse_parser_high_performance
from fansetools.gxf2refflat_plus import convert_gxf_to_refflat, load_annotation_to_dataframe


# åœ¨æ‚¨çš„FanseCounterç±»ä¸­æ·»åŠ å¹¶è¡Œå¤„ç†æ–¹æ³•
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm.contrib.concurrent import process_map

class ParallelFanseCounter:
    """å¹¶è¡Œå¤„ç†å¤šä¸ªfanse3æ–‡ä»¶çš„è®¡æ•°å™¨"""
    
    def __init__(self, max_workers=None):
        self.max_workers = max_workers or min(mp.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
        print(f"åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨: {self.max_workers} ä¸ªè¿›ç¨‹")
    
    def process_files_parallel(self, file_list, output_base_dir, gxf_file=None, level='gene', paired_end=None, annotation_df=None):
        """å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶ - ä¿®å¤ç‰ˆæœ¬"""
        print(f"ğŸ¯ å¼€å§‹å¹¶è¡Œå¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªè¿›ç¨‹")
        
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        tasks = []
        for input_file in file_list:
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
            file_stem = input_file.stem
            output_dir = Path(output_base_dir) / file_stem
            output_dir.mkdir(parents=True, exist_ok=True)
            
            task = {
                'input_file': str(input_file),
                'output_dir': str(output_dir),
                'gxf_file': gxf_file,
                'level': level,
                'paired_end': paired_end,
                'file_stem': file_stem  # ç”¨äºè¿›åº¦æ˜¾ç¤º
            }
            tasks.append(task)
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        results = []
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {}
            for task in tasks:
                future = executor.submit(self._process_single_file, task, annotation_df)
                future_to_task[future] = task
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»ä½“è¿›åº¦
            with tqdm(total=len(tasks), desc="æ€»ä½“è¿›åº¦", position=0) as pbar:
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    try:
                        result = future.result()
                        results.append((task['input_file'], True, result))
                        pbar.set_description(f"âœ… å®Œæˆ: {task['file_stem']}")
                    except Exception as e:
                        results.append((task['input_file'], False, str(e)))
                        pbar.set_description(f"âŒ å¤±è´¥: {task['file_stem']}")
                    finally:
                        pbar.update(1)
        
        return results
    
    def _process_single_file(self, task, annotation_df=None):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå·¥ä½œè¿›ç¨‹å‡½æ•°ï¼‰"""
        try:
            # åœ¨å·¥ä½œè¿›ç¨‹ä¸­é‡æ–°åŠ è½½æ³¨é‡Šæ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if task['gxf_file'] and annotation_df is None:
                # è¿™é‡Œå¯ä»¥æ·»åŠ åœ¨å·¥ä½œè¿›ç¨‹ä¸­åŠ è½½æ³¨é‡Šçš„é€»è¾‘
                pass
            
            counter = FanseCounter(
                input_file=task['input_file'],
                output_dir=task['output_dir'],
                gxf_file=task['gxf_file'],
                level=task['level'],
                paired_end=task['paired_end'],
                annotation_df=annotation_df  # ä¼ é€’å·²åŠ è½½çš„æ³¨é‡Šæ•°æ®
            )
            
            # è¿è¡Œè®¡æ•°å¤„ç†
            result = counter.run()
            return f"æˆåŠŸå¤„ç† {task['file_stem']}"
            
        except Exception as e:
            raise Exception(f"å¤„ç†æ–‡ä»¶ {task['input_file']} å¤±è´¥: {str(e)}")

def count_main_parallel(args):
    """æ”¯æŒå¹¶è¡Œçš„ä¸»å‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    print_mini_fansetools()
    processor = PathProcessor()
    
    try:
        # 1. è§£æè¾“å…¥æ–‡ä»¶
        input_files = processor.parse_input_paths(args.input, ['.fanse','.fanse3', '.fanse3.gz', '.fanse.gz'])
        if not input_files:
            print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
            return
        
        print(f"æ‰¾åˆ° {len(input_files)} ä¸ªè¾“å…¥æ–‡ä»¶")
        
        # 2. åŠ è½½æ³¨é‡Šæ–‡ä»¶ï¼ˆä¸»è¿›ç¨‹åŠ è½½ï¼Œç„¶åä¼ é€’ç»™å·¥ä½œè¿›ç¨‹ï¼‰
        annotation_df = None
        if args.gxf:
            annotation_df = load_annotation_data(args)
            if annotation_df is None:
                print("é”™è¯¯: æ— æ³•åŠ è½½æ³¨é‡Šæ•°æ®")
                return
            print(f"å·²åŠ è½½æ³¨é‡Šæ•°æ®: {len(annotation_df)} ä¸ªè½¬å½•æœ¬")
        else:
            print("è­¦å‘Š: æœªæä¾›æ³¨é‡Šæ–‡ä»¶ï¼Œå°†åªç”Ÿæˆisoformæ°´å¹³è®¡æ•°")
        
        # 3. è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(args.output) if args.output else Path.cwd() / "fansetools_results"
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # 4. æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
        files_to_process = []
        skipped_files = 0
        
        for input_file in input_files:
            file_stem = input_file.stem
            individual_output_dir = output_dir / file_stem
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            output_files_to_check = []
            if args.level in ['isoform', 'both']:
                output_files_to_check.append(individual_output_dir / f"{file_stem}_isoform_level.counts.csv")
            if args.level in ['gene', 'both'] and args.gxf:
                output_files_to_check.append(individual_output_dir / f"{file_stem}_gene_level.counts.csv")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            all_files_exist = all(f.exists() for f in output_files_to_check)
            
            if args.resume and all_files_exist:
                print(f"  è·³è¿‡: {input_file.name} - è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨")
                skipped_files += 1
            else:
                files_to_process.append(input_file)
        
        if not files_to_process:
            print("æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†å®Œæˆ")
            return
        
        print(f"æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {skipped_files} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(files_to_process)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
        # 5. å¹¶è¡Œå¤„ç†
        max_workers = args.processes if hasattr(args, 'processes') and args.processes > 1 else min(mp.cpu_count(), len(files_to_process))
        
        if max_workers == 1:
            print("ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼")
            return count_main_serial(args)  # å›é€€åˆ°ä¸²è¡Œå¤„ç†
        
        parallel_counter = ParallelFanseCounter(max_workers=max_workers)
        
        print("ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†...")
        print("=" * 60)
        
        start_time = time.time()
        results = parallel_counter.process_files_parallel(
            file_list=files_to_process,
            output_base_dir=output_dir,
            gxf_file=args.gxf,
            level=args.level,
            paired_end=args.paired_end,
            annotation_df=annotation_df
        )
        
        duration = time.time() - start_time
        
        # 6. è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š å¤„ç†ç»“æœæ‘˜è¦")
        print("=" * 60)
        
        success_count = sum(1 for _, success, _ in results if success)
        failed_count = len(results) - success_count
        
        print(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        print(f"âŒ å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        
        if failed_count > 0:
            print("\nå¤±è´¥è¯¦æƒ…:")
            for input_file, success, result in results:
                if not success:
                    print(f"  - {Path(input_file).name}: {result}")
        
        print(f"\nğŸ‰ å¤„ç†å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
        
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

def count_main_serial(args):
    """ä¸²è¡Œå¤„ç†ç‰ˆæœ¬ï¼ˆåŸæœ‰çš„count_mainå‡½æ•°ï¼‰"""
    print("ä½¿ç”¨å•ä»»åŠ¡å¤„ç†æ¨¡å¼...")
    processor = PathProcessor()
    
    try:
        # åŸæœ‰çš„ä¸²è¡Œå¤„ç†é€»è¾‘...
        input_files = processor.parse_input_paths(args.input, ['.fanse','.fanse3', '.fanse3.gz', '.fanse.gz'])       
        if not input_files:
            print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
            return
            
        # åŠ è½½æ³¨é‡Šæ–‡ä»¶
        annotation_df = None
        if args.gxf:
            annotation_df = load_annotation_data(args)
            if annotation_df is None:
                print("é”™è¯¯: æ— æ³•åŠ è½½æ³¨é‡Šæ•°æ®")
                return  
        else:
            print("è­¦å‘Š: æœªæä¾›æ³¨é‡Šæ–‡ä»¶ï¼Œå°†åªç”Ÿæˆisoformæ°´å¹³è®¡æ•°")
            
        # ç”Ÿæˆè¾“å‡ºæ˜ å°„
        output_map = processor.generate_output_mapping(input_files, args.output, '.counts.csv')

        # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
        skipped_files = 0
        if args.resume:
            print("å¯ç”¨æ–­ç‚¹ç»­ä¼ æ¨¡å¼ï¼Œæ£€æŸ¥å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶...")
            files_to_process = {}
            
            for input_file, output_file in output_map.items():
                output_dir = Path(output_file).parent
                input_stem = input_file.stem
                
                output_files_to_check = []
                if args.level in ['isoform', 'both']:
                    output_files_to_check.append(output_dir / f"{input_stem}_isoform_level.counts.csv")
                if args.level in ['gene', 'both']:
                    output_files_to_check.append(output_dir / f"{input_stem}_gene_level.counts.csv")
                    output_files_to_check.append(output_dir / f"{input_stem}_multi_genes_level.counts.csv")
                
                all_files_exist = all(f.exists() for f in output_files_to_check)
                if all_files_exist:
                    print(f"  è·³è¿‡: {input_file.name} - è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨")
                    skipped_files += 1
                else:
                    files_to_process[input_file] = output_file
            
            output_map = files_to_process
            print(f"æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {skipped_files} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(output_map)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
            
            if not output_map:
                print("æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†å®Œæˆ")
                return
        
        # ä¸²è¡Œå¤„ç†æ¯ä¸ªæ–‡ä»¶
        for i, (input_file, output_file) in enumerate(output_map.items(), 1):
            print(f"\n[{i + skipped_files}/{len(input_files)}] å¤„ç†: {input_file.name}")
            print(f"  è¾“å‡º: {output_file}")
            
            try:
                counter = FanseCounter(
                    input_file=str(input_file),
                    output_dir=str(output_file.parent),
                    output_filename=output_file.name,                    
                    gxf_file=args.gxf,
                    level=args.level if annotation_df is not None else 'isoform',
                    paired_end=args.paired_end,
                    annotation_df=annotation_df,
                )
                count_files = counter.run()
                print("âœ… å®Œæˆ")
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        
        print(f"\nå¤„ç†å®Œæˆ: æ€»å…± {len(input_files)} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")

def count_main(args):
    """ä¸»å…¥å£å‡½æ•°ï¼Œæ ¹æ®å‚æ•°é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œ"""
    if hasattr(args, 'processes') and args.processes != 1:
        return count_main_parallel(args)
    else:
        return count_main_serial(args)



class FanseCounter:
    """fanse3æ–‡ä»¶è®¡æ•°å¤„ç†å™¨"""
    
    def __init__(self, input_file, output_dir, level='isoform', 
                 # minreads=0, 
                 rpkm=0, 
                 gxf_file=None,  
                 paired_end=None, 
                 output_filename=None, 
                 annotation_df=None):
        
        self.input_file = Path(input_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.level = level
        # self.minreads = minreads
        # self.rpkm = rpkm
        self.gxf_file = gxf_file
        self.paired_end = paired_end
        self.output_filename = output_filename  # æ–°å¢ï¼šæ”¯æŒè‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶å
        self.annotation_df = annotation_df  # æ–°å¢ï¼šæ³¨é‡Šæ•°æ®æ¡†
        
        # å­˜å‚¨è®¡æ•°ç»“æœ
        self.counts_data = {}
        self.summary_stats = {}
        # self.multi_mapping_info = defaultdict(list)  # å­˜å‚¨å¤šæ˜ å°„ä¿¡æ¯
        
    def judge_sequence_mode(self):
        """åˆ¤æ–­æµ‹åºæ¨¡å¼ï¼ˆå•ç«¯/åŒç«¯ï¼‰"""
        if self.paired_end and os.path.isfile(self.paired_end):
            print('Pair-End mode detected.')
            return True
        else:
            print('Single-End mode detected.')
            return False
      
 

    def parse_fanse_file(self):
        """
        ä¸“é—¨è´Ÿè´£è§£æfanse3æ–‡ä»¶ï¼Œç›´æ¥è¿›è¡ŒåŸºæœ¬è®¡æ•°

            'raw': Counter(),     #uniqueå’Œmultiéƒ½åŒ…æ‹¬åœ¨å†…ï¼Œå…¨éƒ¨
            'multi': Counter(),   #åªä¿å­˜multi idçš„
            'unique': Counter(),  #åªä¿å­˜unique ID çš„
            'firstID': Counter(), #åªä¿å­˜rawä¸­ç¬¬ä¸€ä¸ªIDï¼Œmultiåªå–ç¬¬ä¸€ä¸ªIDæ¥è¿›è¡Œç»Ÿè®¡
            'multi2all': Counter(), #multiä¸­çš„æ¯ä¸€ä¸ªIDï¼Œç»Ÿè®¡æ—¶å€™éƒ½+1
            'multi_equal': Counter(),  # é¢„å…ˆåˆå§‹åŒ–ï¼Œåé¢å¡«å……ã€‚multiä¸­çš„æ¯ä¸€ä¸ªIDéƒ½æœ‰ï¼Œç»Ÿè®¡æ—¶å€™å¹³å‡åˆ†é…count
            'multi_EM': Counter(),     # é¢„å…ˆåˆå§‹åŒ–ï¼Œåé¢å¡«å……ã€‚multiä¸­çš„æ¯ä¸€ä¸ªIDï¼Œä»…å…·æœ‰unique readçš„ç»Ÿè®¡æ—¶å€™æœ‰æƒé‡åˆ†é…æ¯”ä¾‹ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…ã€‚æ²¡æœ‰unique çš„ä¸åˆ†é…ã€‚
            'multi_EM_cannot_allocate_tpm': multi ä¸­çš„æ‰€æœ‰IDï¼Œå‡æ²¡æœ‰unique readsçš„éƒ¨åˆ†ã€‚
        
        """
        
        # é€‰æ‹©ä¼˜åŒ–ç‰ˆæœ¬
        if self.input_file.stat().st_size > 1024 * 1024 * 1024:  # å¤§äº1024 MB
            fanse_parser = fanse_parser_high_performance
        else:
            fanse_parser = fanse_parser
    
    
        print(f'Parsing {self.input_file.name}')
        start_time = time.time()
        
        # åˆå§‹åŒ–æ‰€æœ‰è®¡æ•°å™¨
        counts_data = {
            'raw': Counter(),     #uniqueå’Œmultiéƒ½åŒ…æ‹¬åœ¨å†…ï¼Œå…¨éƒ¨
            'multi': Counter(),   #åªä¿å­˜multi idçš„
            'unique': Counter(),  #åªä¿å­˜unique ID çš„
            'firstID': Counter(), #åªä¿å­˜rawä¸­ç¬¬ä¸€ä¸ªIDï¼Œmultiåªå–ç¬¬ä¸€ä¸ªIDæ¥è¿›è¡Œç»Ÿè®¡
            'multi2all': Counter(), #multiä¸­çš„æ¯ä¸€ä¸ªIDï¼Œç»Ÿè®¡æ—¶å€™éƒ½+1
            'multi_equal': Counter(),  # é¢„å…ˆåˆå§‹åŒ–ï¼Œåé¢å¡«å……ã€‚multiä¸­çš„æ¯ä¸€ä¸ªIDéƒ½æœ‰ï¼Œç»Ÿè®¡æ—¶å€™å¹³å‡åˆ†é…count
            'multi_EM': Counter(),     # é¢„å…ˆåˆå§‹åŒ–ï¼Œåé¢å¡«å……ã€‚multiä¸­çš„æ¯ä¸€ä¸ªIDï¼Œä»…å…·æœ‰unique readçš„ç»Ÿè®¡æ—¶å€™æœ‰æƒé‡åˆ†é…æ¯”ä¾‹ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…ã€‚æ²¡æœ‰unique çš„ä¸åˆ†é…ã€‚
            'multi_EM_cannot_allocate_tpm': Counter(), 
            'counts_em': Counter(),    #åˆå¹¶rawå’Œmulti_em
            'counts_eq': Counter(),     #åˆå¹¶rawå’Œmulti_equal           
        }
        
        total_count = 0
        
        files_to_process = [self.input_file]
        if self.paired_end:
            files_to_process.append(Path(self.paired_end))
        
        for fanse_file in files_to_process:
            if not fanse_file.exists():
                continue
                
            # print(f'Reading {fanse_file.name}')
            try:
                file_size = fanse_file.stat().st_size
                estimated_records = max(1, file_size // 527)
                
                with tqdm(total=estimated_records, unit='reads', mininterval=5, unit_scale=True) as pbar:
                    for record in fanse_parser(str(fanse_file)):
                        if record.ref_names:
                            transcript_ids = record.ref_names
                            is_multi = record.is_multi
                            
                            # ç›´æ¥æ›´æ–°è®¡æ•°å™¨
                            raw_id = transcript_ids[0] if len(transcript_ids) == 1 else ','.join(transcript_ids)
                            #ä¸ç®¡ç¥ä¹ˆæ ·ï¼Œraw éƒ½è¦ç»Ÿè®¡åˆ°ä½
                            counts_data['raw'][raw_id] += 1
                            
                            #firstIDåªå–ç¬¬ä¸€ä¸ªIDï¼Œå…¶ä»–çš„IDèˆå¼ƒï¼Œä¸è®ºæ˜¯å¦multiï¼Œå› æ­¤æ”¾åœ¨è¿™é‡Œè¶³å¤Ÿäº†
                            counts_data['firstID'][transcript_ids[0]] += 1
                            
                            if is_multi:
                                #multiçš„ä»¥å­—ç¬¦ä¸²å½¢å¼åŠ å…¥multi ç»Ÿè®¡
                                counts_data['multi'][raw_id] += 1
                                
                                #æ¯ä¸€ä¸ªmultiIDçš„æˆå‘˜éƒ½ç»™multi2all è´¡çŒ®ä¸€ä¸ªç‚¹
                                for tid in transcript_ids:
                                    counts_data['multi2all'][tid] += 1
                            
                            else:   #unique reads éƒ¨åˆ†è¿™é‡Œæ˜¯
                                # tid = transcript_ids[0]    #ä¸ç”¨é‡æ–°èµ‹å€¼äº†ï¼Œç›´æ¥ç”¨ä¸Šé¢raw_idå³å¯
                                counts_data['unique'][raw_id] += 1
                                
                                # counts_data['firstID'][raw_id] += 1
                            
                            total_count += 1
                            pbar.update(1)
                            
            except Exception as e:
                print(f"Error parsing file {fanse_file}: {str(e)}")
                continue
        
        parsing_time = time.time() - start_time
        print(f"Parsing completed in {parsing_time:.2f} seconds, {total_count} records")
        
        return counts_data, total_count



    def calculate_average_record_size(self, file_path, sample_size=100000):
        """
        é€šè¿‡é‡‡æ ·è®¡ç®—fanse3æ–‡ä»¶çš„å¹³å‡è®°å½•å¤§å°
        
        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·è®°å½•æ•°ï¼ˆé»˜è®¤10000æ¡ï¼‰
        
        è¿”å›:
            å¹³å‡æ¯æ¡è®°å½•çš„å­—èŠ‚æ•°
        """
        print(f"é‡‡æ ·è®¡ç®—å¹³å‡è®°å½•å¤§å°ï¼Œé‡‡æ ·æ•°: {sample_size}")
        
        try:
            total_bytes = 0
            record_count = 0
            
            # ä½¿ç”¨fanse_parserè¿›è¡Œé‡‡æ ·
            for i, record in enumerate(fanse_parser(str(file_path))):
                if i >= sample_size:
                    break
                    
                # ä¼°ç®—å½“å‰è®°å½•çš„å¤§å°ï¼ˆåŸºäºè®°å½•å†…å®¹çš„å­—ç¬¦ä¸²é•¿åº¦ï¼‰
                record_size = len(str(record))  # åŸºæœ¬ä¼°ç®—
                total_bytes += record_size
                record_count += 1
            
            if record_count > 0:
                avg_size = total_bytes / record_count
                print(f"é‡‡æ ·å®Œæˆ: {record_count} æ¡è®°å½•ï¼Œå¹³å‡å¤§å°: {avg_size:.1f} å­—èŠ‚")
                return avg_size
            else:
                print("è­¦å‘Š: æ— æ³•é‡‡æ ·è®°å½•ï¼Œä½¿ç”¨é»˜è®¤å€¼527")
                return 527
                
        except Exception as e:
            print(f"é‡‡æ ·å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼527")
            return 527
    
    def calculate_file_record_estimate(self, file_path, sample_size=100000):
        """
        ç»¼åˆä¼°ç®—æ–‡ä»¶ä¸­çš„è®°å½•æ•°é‡
        
        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·å¤§å°
        
        è¿”å›:
            ä¼°è®¡çš„è®°å½•æ•°é‡
        """
        if not file_path.exists():
            return 0
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = file_path.stat().st_size
        
        # å¦‚æœæ˜¯å°æ–‡ä»¶ï¼Œç›´æ¥è§£æè®¡æ•°
        if file_size < 100 * 1024 * 1024:  # å°äº10MBçš„æ–‡ä»¶
            print("å°æ–‡ä»¶ï¼Œç›´æ¥è®¡æ•°...")
            try:
                record_count = sum(1 for _ in fanse_parser(str(file_path)))
                print(f"ç›´æ¥è®¡æ•°å®Œæˆ: {record_count} æ¡è®°å½•")
                return record_count
            except:
                pass
        
        # å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨é‡‡æ ·ä¼°ç®—
        avg_size = self.calculate_average_record_size(file_path, sample_size)-50   #ç»éªŒå‡å»50å­—èŠ‚ï¼Œäººä¸ºå¢å¤§ä¸€ç‚¹ä¼°ç®—çš„readsæ€»æ•°ï¼Œåè€Œæ¯”è¾ƒç¬¦åˆå®é™…
        estimated_records = max(1, int(file_size / avg_size))
        
        print(f"æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        print(f"å¹³å‡è®°å½•å¤§å°: {avg_size:.1f} å­—èŠ‚")
        print(f"ä¼°è®¡Fanseè®°å½•æ•°: {estimated_records} æ¡")
        
        return estimated_records

    def parse_fanse_file_optimized_final(self, position=0):
        """ç»¼åˆä¼˜åŒ–ç‰ˆæœ¬"""
        print(f'Parsing {self.input_file.name}')
        start_time = time.time()
        
        # é¢„åˆå§‹åŒ–æ•°æ®ç»“æ„
        counts_data = {
            'raw': Counter(), 'multi': Counter(), 'unique': Counter(),
            'firstID': Counter(), 'multi2all': Counter()
        }
        
        total_count = 0
        batch_size = 600000
        # update_interval = 10000
        
        # ä½¿ç”¨å±€éƒ¨å˜é‡åŠ é€Ÿ
        raw, multi, unique, firstID, multi2all = (
            counts_data['raw'], counts_data['multi'], counts_data['unique'],
            counts_data['firstID'], counts_data['multi2all']
        )
        
        for position, fanse_file in enumerate([self.input_file] + ([Path(self.paired_end)] if self.paired_end else []) ):
            if not fanse_file.exists():
                continue
                
            try:
                batch = []
                # last_update = 0
                
                # file_size = fanse_file.stat().st_size
                # estimated_records = max(1, file_size // 527)
                # æ™ºèƒ½ä¼°ç®—è®°å½•æ•°
                sample_size = 100000    #é‡‡æ ·æ•°ç›®ï¼Œç”¨æ¥ä¼°ç®—æ€»readsæ•°
                estimated_records = self.calculate_file_record_estimate(fanse_file, sample_size)
                
                with tqdm(total=estimated_records, unit='reads', mininterval=5, unit_scale=True, position=position, leave=False) as pbar:
                    #è¿›åº¦æ¡æ›´æ–°é¢‘ç‡æ§åˆ¶
                    update_interval = 1000
                    update_counter = 0
                    
                    for i, record in enumerate(fanse_parser(str(fanse_file))):
                        if record.ref_names:
                            total_count += 1
                            
                            # æ‰¹é‡å¤„ç†
                            batch.append(record)
                            if len(batch) >= batch_size:
                                self._fast_batch_process(batch, raw, multi, unique, firstID, multi2all)
                                batch = []
                            
                            # æ™ºèƒ½æ›´æ–°
                            update_counter += 1
                            if update_counter >= update_interval:
                                pbar.update(update_counter)
                                update_counter = 0
                            # å‡å°‘è¿›åº¦æ›´æ–°é¢‘ç‡
                            # if i - last_update >= update_interval:
                            #     print(f"Processed {i} records...", end='\r')
                            #     last_update = i
                        else:
                            update_counter += 1
                    
                    # æ›´æ–°å‰©ä½™çš„è¿›åº¦
                    if update_counter > 0:
                        pbar.update(update_counter)
                        # pbar.update(1)
                    # å¤„ç†å‰©ä½™æ‰¹æ¬¡
                    if batch:
                        self._fast_batch_process(batch, raw, multi, unique, firstID, multi2all)
                    
            except Exception as e:
                print(f"Error: {e}")
                continue
        
        duration = time.time() - start_time
        print(f"Completed: {total_count} records in {duration:.2f}s ({total_count/duration:.0f} rec/sec)")
        
        return counts_data, total_count
    
    def _fast_batch_process(self, batch, raw, multi, unique, firstID, multi2all):
        """å¿«é€Ÿæ‰¹é‡å¤„ç†"""
        for record in batch:
            ids = record.ref_names
            is_multi = record.is_multi
            
            # æœ€å°åŒ–å­—ç¬¦ä¸²æ“ä½œ
            first_id = ids[0]
            raw_id = first_id if len(ids) == 1 else ','.join(ids)
            
            raw[raw_id] += 1
            firstID[first_id] += 1
            
            if is_multi:
                multi[raw_id] += 1
                # ä½¿ç”¨é›†åˆæ“ä½œä¼˜åŒ–å¤šIDå¤„ç†
                for tid in ids:
                    multi2all[tid] += 1
            else:
                unique[raw_id] += 1




    def generate_isoform_level_counts(self, counts_data, total_count):
        """
        æ ¹æ®è§£æçš„è®¡æ•°æ•°æ®ç”Ÿæˆisoformæ°´å¹³çš„å„ç§è®¡æ•°
        """
        print("Generating isoform level counts...")
        start_time = time.time()
        
        # ç¬¬äºŒé˜¶æ®µï¼šé«˜çº§å¤šæ˜ å°„è®¡æ•°
        if counts_data['multi']:
            print("Starting advanced multi-mapping analysis...")
            self._process_advanced_multi_mapping(counts_data)
            print("Advanced multi-mapping analysis completed.")
        
        #ç¬¬ä¸‰é˜¶æ®µ:è®¡ç®—æ­£ç¡®çš„countsï¼Œåˆå¹¶rawå’Œmulti_emï¼Œä»¥åŠmulti_equal çš„counts
        print("Starting third stage: merging counts...")

        # åˆå§‹åŒ–åˆå¹¶è®¡æ•°å™¨
        counts_data['counts_em'] = Counter()
        counts_data['counts_eq'] = Counter()
        
        # 1. åˆå¹¶ unique å’Œ multi_EM è®¡æ•° (counts_em)
        # é¦–å…ˆæ·»åŠ æ‰€æœ‰uniqueè®¡æ•°
        for transcript, count in counts_data['unique'].items():
            counts_data['counts_em'][transcript] += count
        
        # ç„¶åæ·»åŠ multi_EMè®¡æ•°
        for transcript, count in counts_data['multi_EM'].items():
            counts_data['counts_em'][transcript] += count
        
        # 2. åˆå¹¶ unique å’Œ multi_equal è®¡æ•° (counts_eq)
        # é¦–å…ˆæ·»åŠ æ‰€æœ‰uniqueè®¡æ•°
        for transcript, count in counts_data['unique'].items():
            counts_data['counts_eq'][transcript] += count
        
        # ç„¶åæ·»åŠ multi_equalè®¡æ•°
        for transcript, count in counts_data['multi_equal'].items():
            counts_data['counts_eq'][transcript] += count
        
        # 3. éªŒè¯åˆå¹¶ç»“æœ
        total_em = sum(counts_data['counts_em'].values())
        total_eq = sum(counts_data['counts_eq'].values())
        total_unique = sum(counts_data['unique'].values())
        total_multi_em = sum(counts_data['multi_EM'].values())
        total_multi_eq = sum(counts_data['multi_equal'].values())
        
        print("åˆå¹¶éªŒè¯:")
        print(f"  - uniqueè®¡æ•°æ€»è®¡: {total_unique}")
        print(f"  - multi_EMè®¡æ•°æ€»è®¡: {total_multi_em}")
        print(f"  - multi_equalè®¡æ•°æ€»è®¡: {total_multi_eq}")
        print(f"  - counts_emæ€»è®¡: {total_em} (åº”ä¸º {total_unique + total_multi_em})")
        print(f"  - counts_eqæ€»è®¡: {total_eq} (åº”ä¸º {total_unique + total_multi_eq})")        
        
        # æ›´æ–°å®ä¾‹å˜é‡
        self.counts_data = counts_data
        self.summary_stats = {
            'total_reads': total_count,
            'unique_mapped': sum(counts_data['unique'].values()),
            'multi_mapped': sum(counts_data['multi'].values()),
            'raw': sum(counts_data['raw'].values()),
            'firstID': sum(counts_data['firstID'].values()),
            'multi_equal': sum(counts_data['multi_equal'].values()),
            'multi_EM': sum(counts_data['multi_EM'].values()),
            'multi_EM_cannot_allocate_tpm': sum(counts_data['multi_EM_cannot_allocate_tpm'].values()),
            'counts_em': total_em,
            'counts_eq': total_eq,
            'processing_time': time.time() - start_time
        }
        
        print(f"Count generation completed in {self.summary_stats['processing_time']:.2f} seconds")
        print("æœ€ç»ˆè®¡æ•°ç»Ÿè®¡:")
        print(f"  - counts_em: {len(counts_data['counts_em'])} ä¸ªè½¬å½•æœ¬, {total_em} æ¡reads")
        print(f"  - counts_eq: {len(counts_data['counts_eq'])} ä¸ªè½¬å½•æœ¬, {total_eq} æ¡reads")

    def _process_advanced_multi_mapping(self, counts_data):
        """å®Œæ•´çš„ä¿®å¤ç‰ˆï¼šå¤„ç†é«˜çº§å¤šæ˜ å°„è®¡æ•°
        multiéƒ¨åˆ†ï¼Œå’Œuniqueéƒ¨åˆ†æ˜¯å¦æœ‰é‡å ï¼Ÿ
         - æ²¡æœ‰ã€‚uniqueæ˜¯å•ç‹¬çš„readsï¼Œmultiéƒ¨åˆ†å¤„ç†å¾—åˆ°çš„readså¯ä»¥å’Œuniqueéƒ¨åˆ†åŠ å’Œï¼Œæ‰æ˜¯æœ€ç»ˆåº”è¯¥çš„readsã€‚
         - æ–°æƒ…å½¢
             - integred_2all  =  unique + multi2all
             - integred_equal  =  unique + multi_equal
             - integred_em  =  unique + multi_em    
        """
        print("å¼€å§‹é«˜çº§å¤šæ˜ å°„åˆ†æ...")
        
        if not counts_data['multi']:
            print("æ²¡æœ‰å¤šæ˜ å°„æ•°æ®ï¼Œè·³è¿‡é«˜çº§åˆ†æ")
            return
        
        # è·å–è½¬å½•æœ¬é•¿åº¦ä¿¡æ¯
        transcript_lengths = {}
        if self.annotation_df is not None:
            transcript_lengths = dict(zip(self.annotation_df['txname'], self.annotation_df['txLength']))
            print(f"åŠ è½½äº† {len(transcript_lengths)} ä¸ªè½¬å½•æœ¬çš„é•¿åº¦ä¿¡æ¯")
        
        # é€šè¿‡uniqueéƒ¨åˆ†è®¡ç®—TPMï¼Œå› æ­¤åªæœ‰å…·æœ‰unique readçš„è½¬å½•æœ¬æ‰è®¡å…¥ï¼Œ**ä¼šä¸¢æ‰æ²¡æœ‰unique readsçš„éƒ¨åˆ†/å®Œå…¨é‡å è½¬å½•æœ¬ã€‚
        tpm_values = self._calculate_tpm(counts_data['unique'], transcript_lengths)
        print(f"è®¡ç®—äº† {len(tpm_values)} ä¸ªè½¬å½•æœ¬çš„TPMå€¼")
        
        # åˆå§‹åŒ–è®¡æ•°å™¨
        multi_equal_counter = Counter()
        multi_em_counter = Counter()
        multi_em_cannot_allocate_tpm_counter  = Counter()
        
        #multiéƒ¨åˆ†çš„æ€»è®°å½•æ•°ç´¯è®¡
        processed_events = 0
        
        for transcript_ids_str, event_count in counts_data['multi'].items():
            transcript_ids = transcript_ids_str.split(',')
            
        # multi_equal: å¹³å‡åˆ†é…, ä¸è®ºæ˜¯å¦å…·æœ‰unique readsï¼Œåæœï¼Œæœ‰éƒ¨åˆ†åŸºå› åŸæœ¬æ²¡è¡¨è¾¾ï¼Œå¼ºè¡Œå®‰æ’ã€‚å‡é˜³æ€§ï¼ˆæ¯”ä¾‹ä¼°è®¡ï¼Ÿï¼‰
            equal_share_per_read = 1.0 / len(transcript_ids)
            for tid in transcript_ids:
                multi_equal_counter[tid] += event_count * equal_share_per_read
            
        # multi_EM: æŒ‰TPMæ¯”ä¾‹åˆ†é…ï¼Œåªæœ‰å…·æœ‰unique readsçš„æ‰å‚ä¸åˆ†é…ï¼›æ²¡æœ‰çš„æš‚æ—¶å¦å­˜ä¸€ä¸ªcolumnsï¼Œå¯è€ƒè™‘å¹³å‡åˆ†é…ï¼Œä½œä¸ºå‚è€ƒã€‚è¿™éƒ¨åˆ†å¯èƒ½æ˜¯åºåˆ—é«˜åº¦é‡å çš„åŸºå› ï¼Œä½†æ˜¯æ— æ³•åŒºåˆ†ï¼Œä¹Ÿä¸èƒ½å®Œå…¨è®¤ä¸ºåŸºå› ä¸è¡¨è¾¾ã€‚
            allocation = self._allocate_by_tpm(transcript_ids, tpm_values)
            if allocation:
                for tid, share_ratio in allocation.items():
                    multi_em_counter[tid] += event_count * share_ratio
            else: 
        #allocationä¸ºNoneçš„æƒ…å†µï¼Œå³æ— æ³•é€šè¿‡tpmåˆ†é…çš„æ— unique readsçš„éƒ¨åˆ†
                multi_em_cannot_allocate_tpm_counter[transcript_ids_str] = event_count
                
            processed_events += 1
            if processed_events % 10000 == 0:
                print(f"å·²å¤„ç† {processed_events} ä¸ªå¤šæ˜ å°„äº‹ä»¶")
        
        # æ›´æ–°è®¡æ•°å™¨
        counts_data['multi_equal'] = multi_equal_counter
        counts_data['multi_EM'] = multi_em_counter
        counts_data['multi_EM_cannot_allocate_tpm'] = multi_em_cannot_allocate_tpm_counter
        
        print( "é«˜çº§å¤šæ˜ å°„åˆ†æå®Œæˆï¼š")
        print(f"  - multi_equal: {len(multi_equal_counter)} ä¸ªè½¬å½•æœ¬")
        print(f"  - multi_EM: {len(multi_em_counter)} ä¸ªè½¬å½•æœ¬")

    def _calculate_tpm(self, unique_counts, transcript_lengths):
        '''
        """è®¡ç®—æ¯ä¸ªåŸºå› çš„TPMå€¼"""
        TPMæ˜¯ä¸€ç§å¸¸ç”¨çš„åŸºå› è¡¨è¾¾æ ‡å‡†åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæ¶ˆé™¤åŸºå› é•¿åº¦å’Œæµ‹åºæ·±åº¦çš„å½±å“ã€‚
        æ­£ç¡®çš„è®¡ç®—æ­¥éª¤åˆ†ä¸ºä¸¤æ­¥ï¼š
        - ç¬¬ä¸€æ­¥æ˜¯RPKæ ‡å‡†åŒ–ï¼Œç”¨åŸºå› çš„åŸå§‹readsæ•°é™¤ä»¥åŸºå› é•¿åº¦(ä»¥åƒç¢±åŸºä¸ºå•ä½)ï¼›
        - ç¬¬äºŒæ­¥æ˜¯æ€»å’Œæ ‡å‡†åŒ–ï¼Œå°†æ‰€æœ‰åŸºå› çš„RPKå€¼ç›¸åŠ ï¼Œç„¶åç”¨æ¯ä¸ªåŸºå› çš„RPKå€¼é™¤ä»¥è¿™ä¸ªæ€»å’Œå†ä¹˜ä»¥ä¸€ç™¾ä¸‡ã€‚
        '''
        if not unique_counts or not transcript_lengths:
            return {}
        
        # è®¡ç®—RPK (Reads Per Kilobase)
        rpk_values = {}
        total_rpk = 0
        
        for transcript, count in unique_counts.items():
            if transcript in transcript_lengths and transcript_lengths[transcript] > 0:
                length_kb = transcript_lengths[transcript] / 1000
                rpk = count / length_kb
                rpk_values[transcript] = rpk
                total_rpk += rpk   #è®¡ç®—æ€»rpk
        
        # è®¡ç®—TPM (Transcripts Per Million)
        tpm_values = {}
        if total_rpk > 0:
            scaling_factor = 1e6 / total_rpk
            for transcript, rpk in rpk_values.items():
                tpm_values[transcript] = rpk * scaling_factor
        
        return tpm_values

    def _allocate_by_tpm(self, transcript_ids, tpm_values):
        """æ ¹æ®unique è®¡ç®—çš„  TPMå€¼åˆ†é…å¤šæ˜ å°„reads"""
        allocation = {}
        
        # è¿‡æ»¤æ‰æ²¡æœ‰TPMå€¼çš„è½¬å½•æœ¬
        valid_transcripts = [tid for tid in transcript_ids if tid in tpm_values and tpm_values[tid] > 0]
        
        if not valid_transcripts:
            # å›é€€åˆ°å¹³å‡åˆ†é…ï¼Œï¼Œï¼Œè¿™ä¸ªæœ‰ç‚¹ä¸å¤ªåˆé€‚ï¼Œå¯ä»¥æ”¾åœ¨å¦ä¸€ä¸ªè¡¨æ ¼multi_EM_cannot_allocate_tpmé‡Œï¼Œæš‚æ—¶ä¸å‚ä¸åˆ†é…  20251111
            return None
            # share = 1.0 / len(transcript_ids)
            # return  {tid: share for tid in transcript_ids}
        
        # è®¡ç®—æ€»TPM
        total_tpm = sum(tpm_values[tid] for tid in valid_transcripts)
        
        # æŒ‰TPMæ¯”ä¾‹åˆ†é…
        for tid in valid_transcripts:
            allocation[tid] = tpm_values[tid] / total_tpm
        
        # å¤„ç†ä¸åœ¨valid_transcriptsä¸­çš„è½¬å½•æœ¬
        invalid_transcripts = [tid for tid in transcript_ids if tid not in valid_transcripts]
        if invalid_transcripts and total_tpm > 0:
            remaining_share = 1.0 - sum(allocation.values())
            if remaining_share > 0:
                share_per_invalid = remaining_share / len(invalid_transcripts)
                for tid in invalid_transcripts:
                    allocation[tid] = share_per_invalid
        
        return allocation

    def aggregate_gene_level_counts(self):
        """
        åŸºå› æ°´å¹³è®¡æ•°èšåˆ
        
        ä»…ä»…æ˜¯é’ˆå¯¹isoformä¸­çš„å„ç§ç±»å‹çš„ç»„åˆä¸­ï¼Œï¼Œåªåšè½¬æ¢ï¼Œå’ŒcountåŠ åˆ°ä¸€èµ·ï¼Œå…¶ä»–æ²¡äº†
            - å¤Ÿä¸å¤Ÿç”¨ï¼Ÿ 
            - è¡Œä¸è¡Œï¼Ÿ
            - åº”è¯¥åé¢æ€ä¹ˆåŠ ä¸€äº›æ‰å¯¹ï¼Ÿ
                - éœ€è¦åŠ åˆ°ä¸€èµ·çš„ï¼ŒåŠ åˆ°ä¸€èµ·
        """
        if self.annotation_df is None:
            print("Warning: Cannot aggregate gene level counts without annotation data")
            return {}, {}
        
        print("Aggregating gene level counts...")
        start_time = time.time()
        
        # åˆ›å»ºè½¬å½•æœ¬åˆ°åŸºå› çš„æ˜ å°„
        transcript_to_gene = dict(zip(self.annotation_df['txname'], self.annotation_df['geneName']))
        gene_level_counts_unique_genes = {}
        gene_level_counts_multi_genes = {}
        
        # åˆå§‹åŒ–æ‰€æœ‰åŸºå› è®¡æ•°ç±»å‹
        for count_type in self.counts_data.keys():
            gene_level_counts_unique_genes[count_type] = Counter()
            gene_level_counts_multi_genes[count_type] = Counter()
       
        # å•ç‹¬å¤„ç†å¤šåŸºå› ç»„åˆ
        # gene_level_counts_unique_genes['multi_genes'] = Counter()
        
        for count_type, counter in self.counts_data.items():
            gene_counter_unique = gene_level_counts_unique_genes[count_type]
            gene_counter_multi = gene_level_counts_multi_genes[count_type]
            
            for transcript_ids_str, event_count in counter.items():
                # å¤„ç†è½¬å½•æœ¬IDï¼ˆå¯èƒ½æ˜¯å•ä¸ªæˆ–å¤šä¸ªï¼‰
                if ',' not in transcript_ids_str:
                    # å•æ˜ å°„æƒ…å†µ
                    gene = transcript_to_gene.get(transcript_ids_str)
                    if gene:   #å› ä¸ºåªæœ‰ä¸€ä¸ªgene IDï¼Œæ‰€ä»¥ç›´æ¥åŠ countå³å¯
                        gene_counter_unique[gene] +=  event_count
                
                else:
                    # å¤šæ˜ å°„æƒ…å†µï¼šæ£€æŸ¥æ˜¯å¦æ˜ å°„åˆ°åŒä¸€ä¸ªåŸºå› 
                    transcript_ids = transcript_ids_str.split(',')
                    genes = set()   #å¦‚æœæ˜¯åŒä¸€ä¸ªåŸºå› ï¼Œåˆ™åªä¼šä¿ç•™ä¸€ä¸ªGENE ID
                    
                    for tid in transcript_ids:
                        gene = transcript_to_gene.get(tid)
                        if gene:
                            genes.add(gene)
                    
                    if len(genes) == 1:
                        # æ˜ å°„åˆ°åŒä¸€ä¸ªåŸºå› 
                        gene = list(genes)[0]
                        gene_counter_unique[gene] +=  event_count
                    elif len(genes) > 1:
                        # æ˜ å°„åˆ°å¤šä¸ªåŸºå› 
                        gene_key = ','.join(sorted(genes))
                        gene_counter_multi[gene_key] +=  event_count   #å®é™…è¿˜æ˜¯gene_level_counts_unique_genes['multi_genes']ï¼Œå› æ­¤ä¸ç”¨å•ç‹¬è¿”å›ï¼Œå·²ç»åŒ…æ‹¬
        
        processing_time = time.time() - start_time
        print(f"Gene level aggregation completed in {processing_time:.2f} seconds")
        
        #ä¸‹ä¸€é˜¶æ®µèšåˆï¼Œç”Ÿæˆæ­£ç¡®çš„counts
        # åˆå§‹åŒ–åˆå¹¶è®¡æ•°å™¨
        gene_level_counts_unique_genes['counts_em'] = Counter()
        gene_level_counts_unique_genes['counts_eq'] = Counter()
        #gene_level_counts_multi_genes
        
        # 1. åˆå¹¶ unique å’Œ multi_EM è®¡æ•° (counts_em)
        # é¦–å…ˆæ·»åŠ æ‰€æœ‰unique rawè®¡æ•°
        for transcript, count in gene_level_counts_unique_genes['raw'].items():
            gene_level_counts_unique_genes['counts_em'][transcript] += count
        
        # ç„¶åæ·»åŠ multi_EMè®¡æ•°
        for transcript, count in gene_level_counts_unique_genes['multi_EM'].items():
            gene_level_counts_unique_genes['counts_em'][transcript] += count
        
        # 2. åˆå¹¶ unique å’Œ multi_equal è®¡æ•° (counts_eq)
        # é¦–å…ˆæ·»åŠ æ‰€æœ‰unique rawè®¡æ•°
        for transcript, count in gene_level_counts_unique_genes['raw'].items():
            gene_level_counts_unique_genes['counts_eq'][transcript] += count
        
        # ç„¶åæ·»åŠ multi_equalè®¡æ•°
        for transcript, count in gene_level_counts_unique_genes['multi_equal'].items():
            gene_level_counts_unique_genes['counts_eq'][transcript] += count
        
        
        return gene_level_counts_unique_genes, gene_level_counts_multi_genes

   
    def _generate_isoform_level_files(self, base_name):
        """ç”Ÿæˆè½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶"""
        isoform_files = {}
        
        # ç”Ÿæˆåˆå¹¶çš„è½¬å½•æœ¬è®¡æ•°æ–‡ä»¶
        combined_df = pd.DataFrame(self.counts_data['firstID'].items(), 
                                 columns=['Transcript', 'firstID'])
        
        # åˆå¹¶æ‰€æœ‰è®¡æ•°ç±»å‹
        for count_type in ['raw', 'unique', 'multi', 'multi2all', 'multi_EM', 'multi_equal','counts_em','counts_eq']:
            if count_type in self.counts_data:
                temp_df = pd.DataFrame(self.counts_data[count_type].items(),
                                    columns=['Transcript', f'{count_type}_count'])
                combined_df = combined_df.merge(temp_df, on='Transcript', how='outer')
        
        # æ·»åŠ æ³¨é‡Šä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.annotation_df is not None:
            annotation_subset = self.annotation_df[['txname', 'geneName', 'txLength', 'cdsLength',]]
            combined_df = combined_df.merge(
                annotation_subset, 
                left_on='Transcript', 
                right_on='txname', 
                how='left'
            ).drop('txname', axis=1)
        
        combined_filename = self.output_dir / f'{base_name}_isoform_level.counts.csv'
        combined_df.to_csv(combined_filename, index=False, float_format='%.2f')
        isoform_files['isoform'] = combined_filename
        
        return isoform_files

   

    #20251111
    def _generate_gene_level_files(self, base_name):
        """ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ - æ ¹æ®æ–°çš„è¿”å›ç»“æ„ä¿®æ”¹"""
        if self.annotation_df is None:
            print("æ²¡æœ‰æ³¨é‡Šä¿¡æ¯ï¼Œè·³è¿‡åŸºå› æ°´å¹³æ–‡ä»¶ç”Ÿæˆ")
            return {} 
        # æ£€æŸ¥å®é™…çš„åˆ—å
        print(f"Annotation DataFrame åˆ—å: {list(self.annotation_df.columns)}")
        
        if not hasattr(self, 'gene_level_counts_unique_genes') or not self.gene_level_counts_unique_genes:
            print("Warning: No gene level counts available")
            return {}
        
        gene_files = {}
        
        # ç”Ÿæˆå•ä¸ªåŸºå› çš„è®¡æ•°æ–‡ä»¶ï¼ˆæ¥è‡ªgene_level_counts_unique_genesï¼‰
        single_gene_df = pd.DataFrame(self.gene_level_counts_unique_genes['firstID'].items(), 
                                     columns=['Gene', 'firstID_count'])
        
        # åˆå¹¶æ‰€æœ‰è®¡æ•°ç±»å‹ï¼ˆå•ä¸ªåŸºå› ï¼‰
        for count_type in ['raw', 'unique', 'multi', 'multi2all', 'multi_EM', 'multi_equal','counts_em','counts_eq']:
            if count_type in self.gene_level_counts_unique_genes:
                temp_df = pd.DataFrame(self.gene_level_counts_unique_genes[count_type].items(),
                                    columns=['Gene', f'{count_type}_count'])
                single_gene_df = single_gene_df.merge(temp_df, on='Gene', how='outer')
        
        # ç”Ÿæˆå¤šåŸºå› ç»„åˆçš„è®¡æ•°æ–‡ä»¶ï¼ˆæ¥è‡ªgene_level_counts_multi_genesï¼‰
        if hasattr(self, 'gene_level_counts_multi_genes') and self.gene_level_counts_multi_genes:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¤šåŸºå› è®¡æ•°æ•°æ®
            has_multi_data = False
            for count_type, counter in self.gene_level_counts_multi_genes.items():
                if counter:  # æ£€æŸ¥è®¡æ•°å™¨æ˜¯å¦éç©º
                    has_multi_data = True
                    break
            
            if has_multi_data:
                # ä½¿ç”¨firstIDä½œä¸ºåŸºç¡€ï¼ˆå¦‚æœæ²¡æœ‰firstIDï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„è®¡æ•°ç±»å‹ï¼‰
                base_count_type = None
                for count_type in ['firstID', 'raw', 'unique', 'multi']:
                    if count_type in self.gene_level_counts_multi_genes and self.gene_level_counts_multi_genes[count_type]:
                        base_count_type = count_type
                        break
                
                if base_count_type:
                    multi_genes_df = pd.DataFrame(self.gene_level_counts_multi_genes[base_count_type].items(),
                                                columns=['Gene_Combination', 'firstID_count'])
                    
                    # åˆå¹¶å…¶ä»–è®¡æ•°ç±»å‹ï¼ˆå¤šåŸºå› ç»„åˆï¼‰
                    for count_type in ['raw', 'unique', 'multi', 'multi2all', 'multi_EM', 'multi_equal']:
                        if count_type in self.gene_level_counts_multi_genes and self.gene_level_counts_multi_genes[count_type]:
                            temp_df = pd.DataFrame(self.gene_level_counts_multi_genes[count_type].items(),
                                                columns=['Gene_Combination', f'{count_type}_count'])
                            multi_genes_df = multi_genes_df.merge(temp_df, on='Gene_Combination', how='outer')
                else:
                    # å¦‚æœæ²¡æœ‰åŸºç¡€è®¡æ•°ç±»å‹ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„DataFrame
                    multi_genes_df = pd.DataFrame(columns=['Gene_Combination', 'firstID_count'])
            else:
                multi_genes_df = None
        else:
            multi_genes_df = None
        
        
        # # æ·»åŠ åŸºå› æ³¨é‡Šä¿¡æ¯å’Œè½¬å½•æœ¬ä¿¡æ¯
        # if self.annotation_df is not None:
        #     # è·å–åŸºå› åˆ°è½¬å½•æœ¬çš„æ˜ å°„
        #     gene_to_transcripts = defaultdict(list)
        #     for _, row in self.annotation_df.iterrows():
        #         # gene_to_transcripts[row['geneName']].append(row['txname'])
        #         gene_name = row.get('geneName', row.get('gene_name', ''))  # å¤„ç†ä¸åŒçš„åˆ—å
        #         tx_name = row.get('txname', row.get('transcript_id', ''))  # å¤„ç†ä¸åŒçš„åˆ—å
        #         if gene_name and tx_name:
        #             gene_to_transcripts[gene_name].append(tx_name)
            
        #     # ä¸ºå•ä¸ªåŸºå› æ–‡ä»¶æ·»åŠ è½¬å½•æœ¬ä¿¡æ¯
        #     single_gene_df['Transcripts'] = single_gene_df['Gene'].map(
        #         lambda x: ','.join(gene_to_transcripts.get(x, [])) if x in gene_to_transcripts else ''
        #     )
        #     single_gene_df['Transcript_Count'] = single_gene_df['Gene'].map(
        #         lambda x: len(gene_to_transcripts.get(x, []))
        #     )
            
        #     # ä¸ºå¤šåŸºå› ç»„åˆæ–‡ä»¶æ·»åŠ è½¬å½•æœ¬ä¿¡æ¯
        #     if multi_genes_df is not None and not multi_genes_df.empty:
        #         multi_genes_df['Transcripts'] = multi_genes_df['Gene_Combination'].map(
        #             lambda x: ','.join([','.join(gene_to_transcripts.get(g, [])) for g in x.split(',')])
        #         )
        #         multi_genes_df['Transcript_Count'] = multi_genes_df['Gene_Combination'].map(
        #             lambda x: sum(len(gene_to_transcripts.get(g, [])) for g in x.split(','))
        #         )
        
        # # æ·»åŠ å…¶ä»–åŸºå› æ³¨é‡Šä¿¡æ¯
        # if self.annotation_df is not None:
        #     gene_annotation = self.annotation_df[['geneName', 'symbol', 'txLength', 'cdsLength','genelongesttxLength','genelongestcdsLength']].drop_duplicates()
        #     # gene_annotation = annotation_df[['geneName', 'symbol', 'txLength', 'cdsLength']].drop_duplicates()
        #     gene_annotation = gene_annotation.groupby('geneName').agg({
        #         # 'symbol': 'first',
        #         'txLength': 'max',
        #         'cdsLength': 'max'
        #     }).reset_index()
            
        #     single_gene_df = single_gene_df.merge(
        #         gene_annotation, 
        #         left_on='Gene', 
        #         right_on='geneName', 
        #         how='left'
        #     ).drop('geneName', axis=1)
        
        # æ·»åŠ åŸºå› æ³¨é‡Šä¿¡æ¯å’Œè½¬å½•æœ¬ä¿¡æ¯
        if self.annotation_df is not None:
            # è·å–åŸºå› åˆ°è½¬å½•æœ¬çš„æ˜ å°„
            gene_to_transcripts = defaultdict(list)
            for _, row in self.annotation_df.iterrows():
                # å¤„ç†ä¸åŒçš„åˆ—åå¯èƒ½æ€§
                gene_name = row.get('geneName', row.get('gene_name', ''))
                tx_name = row.get('txname', row.get('transcript_id', ''))
                if gene_name and tx_name:
                    gene_to_transcripts[gene_name].append(tx_name)
            
            # ä¸ºå•ä¸ªåŸºå› æ–‡ä»¶æ·»åŠ è½¬å½•æœ¬ä¿¡æ¯
            single_gene_df['Transcripts'] = single_gene_df['Gene'].map(
                lambda x: ','.join(gene_to_transcripts.get(x, [])) if x in gene_to_transcripts else ''
            )
            single_gene_df['Transcript_Count'] = single_gene_df['Gene'].map(
                lambda x: len(gene_to_transcripts.get(x, []))
            )
            
            # ä¿®å¤ï¼šåŠ¨æ€æ£€æŸ¥å¹¶é€‰æ‹©å¯ç”¨çš„åˆ—
            available_columns = self.annotation_df.columns.tolist()
            
            # æ£€æŸ¥å¹¶é€‰æ‹©åŸºå› æ³¨é‡Šåˆ—
            gene_annotation_cols = ['geneName']
            symbol_cols = ['symbol', 'genename', 'gene_name']
            txlength_cols = ['genelongesttxLength', 'genelonesttxlength', 'txLength']
            cdslength_cols = ['genelongestcdsLength', 'genelongestcdslength', 'cdsLength']
            
            # é€‰æ‹©å®é™…å­˜åœ¨çš„åˆ—
            selected_cols = ['geneName']
            
            # æ·»åŠ symbolåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for col in symbol_cols:
                if col in available_columns:
                    selected_cols.append(col)
                    break
            
            # æ·»åŠ txLengthåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for col in txlength_cols:
                if col in available_columns:
                    selected_cols.append(col)
                    break
            
            # æ·»åŠ cdsLengthåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for col in cdslength_cols:
                if col in available_columns:
                    selected_cols.append(col)
                    break
            
            print(f"ä½¿ç”¨çš„åŸºå› æ³¨é‡Šåˆ—: {selected_cols}")
            
            # å»é‡å¹¶åˆå¹¶
            gene_annotation = self.annotation_df[selected_cols].drop_duplicates()
            
            # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
            rename_map = {}
            if 'genename' in gene_annotation.columns:
                rename_map['genename'] = 'symbol'
            if 'genelonesttxlength' in gene_annotation.columns:
                rename_map['genelonesttxlength'] = 'genelongesttxLength'
            if 'genelongestcdslength' in gene_annotation.columns:
                rename_map['genelongestcdslength'] = 'genelongestcdsLength'
            
            if rename_map:
                gene_annotation = gene_annotation.rename(columns=rename_map)
            
            # åˆå¹¶åˆ°ç»“æœæ•°æ®æ¡†
            single_gene_df = single_gene_df.merge(
                gene_annotation, 
                left_on='Gene', 
                right_on='geneName', 
                how='left'
            )
            
            # ç§»é™¤é‡å¤çš„geneNameåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'geneName' in single_gene_df.columns and 'Gene' in single_gene_df.columns:
                single_gene_df = single_gene_df.drop('geneName', axis=1)        
        
        
        # ä¿å­˜unique genesæ–‡ä»¶
        single_gene_filename = self.output_dir / f'{base_name}_gene_level.counts.csv'
        single_gene_df.to_csv(single_gene_filename, index=False, float_format='%.2f')
        gene_files['gene'] = single_gene_filename
        
        # ä¿å­˜multi genesç»„åˆæ–‡ä»¶ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
        if multi_genes_df is not None and not multi_genes_df.empty:
            multi_genes_filename = self.output_dir / f'{base_name}_multi_genes_level.counts.csv'
            multi_genes_df.to_csv(multi_genes_filename, index=False, float_format='%.2f')
            gene_files['multi_genes'] = multi_genes_filename
        
        return gene_files
  
    def _generate_multi_mapping_file(self, base_name):
        """ç”Ÿæˆå¤šæ˜ å°„ä¿¡æ¯æ–‡ä»¶"""
        if not self.multi_mapping_info:
            return None
        
        # åˆ›å»ºå¤šæ˜ å°„ä¿¡æ¯æ•°æ®æ¡†
        multi_data = []
        for transcript_ids, read_names in self.multi_mapping_info.items():
            multi_data.append({
                'transcript_ids': transcript_ids,
                'read_count': len(read_names),
                'read_names': ';'.join(read_names)  
            })
        
        multi_df = pd.DataFrame(multi_data)
        multi_filename = self.output_dir / f'{base_name}_multi_mapping_info.csv'
        multi_df.to_csv(multi_filename, index=False)
        
        return multi_filename

 
    def generate_count_files(self):
        """
        ç”Ÿæˆisoform å’Œ gene level è®¡æ•°æ–‡ä»¶
        
        """
        if self.output_filename:
            base_name = Path(self.output_filename).stem
        else:
            base_name = self.input_file.stem
        
        count_files = {}
        
        # ç”Ÿæˆè½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶
        if self.level in ['isoform', 'both']:
            try:
                isoform_files = self._generate_isoform_level_files(base_name)
                count_files.update(isoform_files)
                print(" isoform æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                print(f"è½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
        
        
        # ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶
        if self.annotation_df is not None and self.level in ['gene', 'both']:
            try:
                # # æ›´å¥å£®çš„æ¡ä»¶æ£€æŸ¥
                if (hasattr(self, 'gene_level_counts_unique_genes') and \
                    self.gene_level_counts_unique_genes):
     
                    gene_files = self._generate_gene_level_files(base_name)
                    count_files.update(gene_files)
                    print("åŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
                else:
                    print("æ²¡æœ‰åŸºå› æ°´å¹³è®¡æ•°æ•°æ®ï¼Œè·³è¿‡åŸºå› æ°´å¹³æ–‡ä»¶ç”Ÿæˆ")
            except Exception as e:
                print(f" åŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
        
        #     # # ç”Ÿæˆå¤šæ˜ å°„ä¿¡æ¯æ–‡ä»¶
        #     # multi_mapping_file = self._generate_multi_mapping_file(base_name)
        #     # if multi_mapping_file:
        #     #     count_files['multi_mapping'] = multi_mapping_file
        
        return count_files

    
    # def filter_by_minreads(self, minreads=None):
    #     """æ ¹æ®æœ€å°readsæ•°è¿‡æ»¤"""
    #     if minreads is None:
    #         minreads = self.minreads
        
    #     if minreads > 0:
    #         filtered_counts = {
    #             k: Counter({acc: count for acc, count in v.items() if count >= minreads})
    #             for k, v in self.counts_data.items()
    #         }
            
    #         base_name = self.input_file.stem
    #         for count_type, counter in filtered_counts.items():
    #             df = pd.DataFrame(counter.items(), columns=['Accession', 'count'])
    #             filename = self.output_dir / f'{base_name}_{count_type}_min{minreads}.csv'
    #             df.to_csv(filename, index=False)
            
    #         print(f"Filtered by minreads {minreads}, remaining genes: {len(filtered_counts['normal'])}")
    
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„è®¡æ•°æµç¨‹"""
        print("=" * 60)
        print("fansetools count - Starting processing")
        print("=" * 60)
        
        # 1. è§£æfanse3æ–‡ä»¶å¹¶ç›´æ¥è·å¾—è®¡æ•°
        counts_data, total_count = self.parse_fanse_file_optimized_final()
        # counts_data, total_count = self.parse_fanse_file()
        
        # 2. ç”Ÿæˆisoformæ°´å¹³è®¡æ•°
        self.generate_isoform_level_counts(counts_data, total_count)
        
        # 3. ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°
        if self.annotation_df is not None:
            # self.gene_counts = self.aggregate_gene_level_counts()
            self.gene_level_counts_unique_genes, self.gene_level_counts_multi_genes  = self.aggregate_gene_level_counts()
            if self.gene_level_counts_unique_genes:
                print(f"Gene level aggregation completed: {len(self.gene_level_counts_unique_genes)} unique-gene count types")
            if self.gene_level_counts_multi_genes:
                print(f"Gene level aggregation completed: {len(self.gene_level_counts_multi_genes)} multi-gene count types")
        else:
            print("No annotation provided, skipping gene level aggregation")
            # self.gene_counts = {}
            self.gene_level_counts_unique_genes = {}
            self.gene_level_counts_multi_genes  = {}
        # 4. ç”Ÿæˆè®¡æ•°æ–‡ä»¶
        count_files = self.generate_count_files()
        
        # 5. å¯é€‰è¿‡æ»¤readsæ•°ç›®ï¼Œæ­¤å¤„ä¸å»ºè®®
        # if self.minreads > 0:
        #     self.filter_by_minreads()
        
        # 6. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        self.generate_summary()
        
        print("fansetools count - Processing completed")
        print("=" * 60)
        
        return count_files
    

    def generate_summary(self):
        """ç”Ÿæˆå¤„ç†æ‘˜è¦"""
        summary_file = self.output_dir / f"{self.input_file.stem}_summary.txt"
        
        with open(summary_file, 'w') as f:
            f.write("fansetools count - Processing Summary\n")
            f.write("=" * 50 + "\n")
            f.write(f"Input file: {self.input_file}\n")
            f.write(f"Output directory: {self.output_dir}\n")
            f.write(f"Processing mode: {'Paired-end' if self.paired_end else 'Single-end'}\n")
            f.write(f"Level parameter: {self.level}\n")
            f.write(f"Annotation provided: {self.annotation_df is not None}\n")
            
            if self.annotation_df is not None:
                f.write(f"Annotation transcripts: {len(self.annotation_df)}\n")
                f.write(f"Annotation genes: {self.annotation_df['geneName'].nunique()}\n")
            
            f.write("\nStatistics:\n")
            for stat, value in self.summary_stats.items():
                f.write(f"{stat}: {value}\n")
            
            f.write(f"\nMulti-mapping statistics:\n")
            f.write(f"Multi-mapping events: {len(self.counts_data['multi'])}\n")
            if self.counts_data['multi']:
                total_multi_reads = sum(self.counts_data['multi'].values())
                avg_reads_per_event = total_multi_reads / len(self.counts_data['multi'])
                f.write(f"Total multi-mapped reads: {total_multi_reads}\n")
                f.write(f"Average reads per multi-mapping event: {avg_reads_per_event:.2f}\n")


def print_mini_fansetools():
    """
    æœ€å°çš„å¯è¯†åˆ«ç‰ˆæœ¬
    https://www.ascii-art-generator.org/
    """
    # mini_art = [
    #     '''
    #     #######                                #######                             
    #     #         ##   #    #  ####  ######       #     ####   ####  #       ####  
    #     #        #  #  ##   # #      #            #    #    # #    # #      #      
    #     #####   #    # # #  #  ####  #####        #    #    # #    # #       ####  
    #     #       ###### #  # #      # #            #    #    # #    # #           # 
    #     #       #    # #   ## #    # #            #    #    # #    # #      #    # 
    #     #       #    # #    #  ####  ######       #     ####   ####  ######  ####  
    #     '''                                                                        
    # ]
    
    mini_art =  ['''
     FANSeTools - Functional ANnotation SEquence Tools
     ''']
    
    for line in mini_art:
        print(line)
        
# def count_main(args):
#     """ä½¿ç”¨æ–°è·¯å¾„å¤„ç†å™¨çš„countä¸»å‡½æ•°"""
#     #æ‰“å°ä¸ªlogo
#     print_mini_fansetools()
    
#     processor = PathProcessor()
    
#     try:
#         # 1. è§£æè¾“å…¥è·¯å¾„
#         input_files = processor.parse_input_paths(args.input, ['.fanse','.fanse3', '.fanse3.gz', '.fanse.gz'])
#         # input_files = processor.parse_input_paths(input_file, ['.fanse','.fanse3', '.fanse3.gz', '.fanse.gz'])        
#         if not input_files:
#             print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
#             sys.exit(1)
            
#         # 2. åŠ è½½æ³¨é‡Šæ–‡ä»¶
#         annotation_df = None
#         if args.gxf:
#             annotation_df = load_annotation_data(args)
#             if annotation_df is None:
#                 print("é”™è¯¯: æ— æ³•åŠ è½½æ³¨é‡Šæ•°æ®")
#                 sys.exit(1)  
#         else:
#             print("è­¦å‘Š: æœªæä¾›æ³¨é‡Šæ–‡ä»¶ï¼Œå°†åªç”Ÿæˆisoformæ°´å¹³è®¡æ•°")
            
#         # 3. ç”Ÿæˆè¾“å‡ºæ˜ å°„
#         output_map = processor.generate_output_mapping(input_files, args.output, '.counts.csv')

#         # 4. æ–­ç‚¹ç»­ä¼ : æ£€æŸ¥å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶
#         skipped_files = 0
#         if args.resume:
#             print("å¯ç”¨æ–­ç‚¹ç»­ä¼ æ¨¡å¼ï¼Œæ£€æŸ¥å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶...")
#             files_to_process = {}
            
#             for input_file, output_file in output_map.items():
#                 # æ£€æŸ¥è¾“å‡ºç›®å½•ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åº”çš„ç»“æœæ–‡ä»¶
#                 output_dir = Path(output_file).parent
#                 input_stem = input_file.stem
                
#                 # æ ¹æ®levelå‚æ•°æ£€æŸ¥ç›¸åº”çš„è¾“å‡ºæ–‡ä»¶
#                 output_files_to_check = []
                
#                 if args.level in ['isoform', 'both']:
#                     output_files_to_check.append(output_dir / f"{input_stem}.counts_isoform_level.counts.csv")
                
#                 if args.level in ['gene', 'both']:
#                     output_files_to_check.append(output_dir / f"{input_stem}.counts_gene_level.counts.csv")
#                     output_files_to_check.append(output_dir / f"{input_stem}.counts_multi_genes_level.counts.csv")
                
#                     print(output_files_to_check)
#                 # æ£€æŸ¥æ‰€æœ‰ç›¸å…³æ–‡ä»¶æ˜¯å¦å­˜åœ¨
#                 all_files_exist = any(f.exists() for f in output_files_to_check)
#                 print(all_files_exist)
#                 if all_files_exist:
#                     print(f"  è·³è¿‡: {input_file.name} - è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨")
#                     skipped_files += 1
#                 else:
#                     files_to_process[input_file] = output_file
            
#             output_map = files_to_process
#             print(f"æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {skipped_files} ä¸ªå·²å¤„ç†çš„æ–‡ä»¶ï¼Œå‰©ä½™ {len(output_map)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
            
#             if not output_map:
#                 print("æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€ç»§ç»­è¿è¡Œï¼›é‡æ–°ç”Ÿæˆç»“æœè¯·å¦è®¾æ–‡ä»¶å¤¹ï¼Œæˆ–åˆ é™¤å·²æœ‰ç»“æœ")
#                 return
#         else:
#             print(f"æ‰¾åˆ° {len(input_files)} ä¸ªè¾“å…¥æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
            
        
        
        
#         # 5. éªŒè¯è·¯å¾„
#         validation_checks = []
#         for input_file in input_files:
#             validation_checks.append((input_file, "è¾“å…¥æ–‡ä»¶", {'must_exist': True, 'must_be_file': True}))
        
#         is_valid, errors = processor.validate_paths(*validation_checks)
#         if not is_valid:
#             print("è·¯å¾„éªŒè¯å¤±è´¥:")
#             for error in errors:
#                 print(f"  - {error}")
#             sys.exit(1)
        
#         # 4. æ‰¹é‡å¤„ç†æ–‡ä»¶
#         # print(f"æ‰¾åˆ° {len(output_map)} / len(input_files)ä¸ªè¾“å…¥æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")   #æœ‰å¤šå°‘ä¸ªè¾“å‡ºï¼Œå°±è‚¯å®šæœ‰å¤šå°‘ä¸ªè¾“å…¥ï¼Œæ²¡é”™çš„ï¼Œè¿™æ ·æ›´åˆç†
        
#         for i, (input_file, output_file) in enumerate(output_map.items(), 1):
#             print(f"\n[{ skipped_files + i }/{len(input_files)}] å¤„ç†: {input_file.name}")
#             print(f"  è¾“å‡º: {output_file}")
            
#             try:
#                 counter = FanseCounter(
#                     input_file=str(input_file),
#                     output_dir=str(output_file.parent),
#                     output_filename=output_file.name,                    
#                     # minreads=args.minreads,
#                     # rpkm=args.rpkm,
#                     gxf_file= args.gxf,
#                     level= args.level if annotation_df is not None else 'isoform',  # æ²¡æœ‰æ³¨é‡Šæ—¶å¼ºåˆ¶ä½¿ç”¨isoformæ°´å¹³
#                     paired_end= args.paired_end,
#                     annotation_df= annotation_df,  # ä¼ é€’æ³¨é‡Šæ•°æ®
#                 )
#                 count_files = counter.run()
#                 print(" å®Œæˆ")
#                 print(f"ç”Ÿæˆæ–‡ä»¶: {list(count_files.keys())}")
#             except Exception as e:
#                 print(f"å¤„ç†å¤±è´¥: {str(e)}")
        
#         print(f"\nå¤„ç†å®Œæˆ: æ€»å…± {len(input_files)} ä¸ªæ–‡ä»¶")
        
#     except Exception as e:
#         print(f"é”™è¯¯: {str(e)}")
#         sys.exit(1)


def load_annotation_data(args):
    """åŠ è½½æ³¨é‡Šæ•°æ®"""
    if not args.gxf:
        print("é”™è¯¯: éœ€è¦æä¾› --gxf å‚æ•°")
        return None
    
    print(f"\nLoading annotation from {args.gxf}")
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåçš„refflatæ–‡ä»¶
    refflat_file = os.path.splitext(args.gxf)[0] + ".genomic.refflat"
    
    if os.path.exists(refflat_file):
        print(f"Found existing refflat file: {refflat_file}")
        try:
            annotation_df = read_refflat_with_commented_header(refflat_file)
            print(f"Successfully loaded {len(annotation_df)} transcripts from existing refflat file")
            return annotation_df
        except Exception as e:
            print(f"Error loading refflat file: {e}")
            print("Converting GXF file instead...")
    
    print(f"No existing refflat file found at {refflat_file}")
    print("Converting GXF file to refflat format...")
    
    if args.annotation_output:
        # Generate both genomic and RNA coordinate files
        genomic_df, rna_df = convert_gxf_to_refflat(
            args.gxf, args.annotation_output, add_header=True
        )
        return genomic_df
    else:
        # Just load the data without saving
        genomic_df = load_annotation_to_dataframe(args.gxf)
        return genomic_df
    
    
# æ–¹æ³•1ï¼šå…ˆè¯»å–æ³¨é‡Šè¡Œè·å–åˆ—åï¼Œç„¶åè¯»å–æ•°æ®
def read_refflat_with_commented_header(file_path):
    """è¯»å–å¸¦æœ‰æ³¨é‡Šå¤´éƒ¨çš„refflatæ–‡ä»¶"""
    # é¦–å…ˆè¯»å–æ³¨é‡Šè¡Œè·å–åˆ—å
    with open(file_path, 'r') as f:
        header_line = None
        for line in f:
            if line.startswith('#'):
                header_line = line.strip()
                break
    
    if header_line:
        # æå–åˆ—åï¼ˆå»æ‰#å’Œç©ºæ ¼ï¼‰
        columns = header_line[1:].strip().split('\t')
        # è¯»å–æ•°æ®ï¼Œè·³è¿‡æ³¨é‡Šè¡Œ
        df = pd.read_csv(file_path, sep='\t', comment='#', header=None, names=columns)
    else:
        # å¦‚æœæ²¡æœ‰æ³¨é‡Šå¤´éƒ¨ï¼Œä½¿ç”¨é»˜è®¤åˆ—å
        default_columns = [
            "geneName", "txname", "chrom", "strand", "txStart", "txEnd",
            "cdsStart", "cdsEnd", "exonCount", "exonStarts", "exonEnds",
            "symbol", "g_biotype", "t_biotype", "description", "protein_id",
            "txLength", "cdsLength", "utr5Length", "utr3Length",
            "genelongesttxLength", "genelongestcdsLength", "geneEffectiveLength"
        ]
        df = pd.read_csv(file_path, sep='\t', header=None, names=default_columns)
    
    return df



def add_count_subparser(subparsers):
    """å‘½ä»¤è¡Œä¸»å‡½æ•°"""
    
    parser = subparsers.add_parser(
        'count',
        help='è¿è¡ŒFANSe to countï¼Œè¾“å‡ºreadcount',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
        ä½¿ç”¨ç¤ºä¾‹:
            é»˜è®¤isoform level
          å•ä¸ªæ–‡ä»¶å¤„ç†:
            fanse count -i sample.fanse3 -o results/ --gxf annotation.gtf
          
          æ‰¹é‡å¤„ç†ç›®å½•ä¸­æ‰€æœ‰fanse3æ–‡ä»¶:
            fanse count -i /data/*.fanse3 -o /output/ --gxf annotation.gtf
          
          åŒç«¯æµ‹åºæ•°æ®:
            fanse count -i R1.fanse3 -r R2.fanse3 -o results/ --gxf annotation.gtf
          
        **å¦‚éœ€è¦åŸºå› æ°´å¹³è®¡æ•°ï¼Œéœ€è¦è¾“å…¥gtf/gff/refflat/ç®€å•g-tå¯¹åº”æ–‡ä»¶ï¼Œ--gxféƒ½å¯ä»¥è§£æ
          åŸºå› æ°´å¹³è®¡æ•°:
            fanse count -i *.fanse3 -o results/ --gxf annotation.gtf --level gene
          
          åŒæ—¶è¾“å‡ºåŸºå› å’Œè½¬å½•æœ¬æ°´å¹³:
            fanse count -i *.fanse3 -o results/ --gxf annotation.gtf --level both
        
          å¤„ç†ä¸­æ–­åé‡æ–°è¿è¡Œï¼ˆè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶[è¾“å‡ºæ–‡ä»¶å¤¹ä¸­å­˜åœ¨å¯¹åº”ç»“æœæ–‡ä»¶ï¼Œéœ€é‡æ–°è¿è¡Œè¯·åˆ é™¤]ï¼‰
            fanse count -i *.fanse3 -o results/ --gxf annotation.gtf --resume
            
            # æŒ‡å®š4ä¸ªå¹¶è¡Œè¿›ç¨‹
            fanse count -i "*.fanse3" -o results --gxf annotation.gtf --p 4
            
            ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒå¹¶è¡Œå¤„ç†:
            fanse count -i *.fanse3 -o results --gxf annotation.gtf -p 0
                """
    )
    
    parser.add_argument('-i', '--input', required=True, 
                       help='Input fanse3 file,è¾“å…¥FANSe3æ–‡ä»¶/ç›®å½•/é€šé…ç¬¦ï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰')
    parser.add_argument('-r', '--paired-end', 
                       help='Paired-end fanse3 file (optional)')
    parser.add_argument('-o', '--output', required=False,
                       help='Output directory,è¾“å‡ºè·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰')
    
    # parser.add_argument('--minreads', type=int, default=0,
    #                     help='Minimum reads threshold for filtering')
    parser.add_argument('--rpkm', type=float, default=0,
                       help='RPKM threshold for filteringï¼Œå°šæœªå®Œæˆ')
    
    parser.add_argument('--gxf', required=False, help='Input GXF file (GTF or GFF3),if not provided, just give out isoform level readcounts')
    parser.add_argument('--annotation-output', help='Output refFlat file prefix (optional)')
    
    parser.add_argument('--level', choices=['gene', 'isoform', 'both'], default='gene',
                       help='Counting level')
    
    parser.add_argument('--resume', required=False, action='store_true', help='å¯ä»ä¸Šæ¬¡è¿è¡Œæ–­æ‰çš„åœ°æ–¹è‡ªåŠ¨å¼€å§‹ï¼Œè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰è¾“å…¥æ–‡ä»¶å¯¹åº”çš„ç»“æœæ–‡ä»¶ï¼Œæœ‰åˆ™è·³è¿‡')

    parser.add_argument('--processes', '-p', type=int, default=1,
                       help='å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°, 1=ä¸²è¡Œ)')

    # æ ¹æ®æ˜¯å¦å¹¶è¡Œé€‰æ‹©æ‰§è¡Œå‡½æ•°
    def count_main_wrapper(args):
        if getattr(args, 'processes', None) != 1:  # ä¸æ˜¯æ˜ç¡®è®¾ç½®ä¸º1
            return count_main_parallel(args)
        else:
            return count_main(args)  # åŸæœ‰çš„ä¸²è¡Œç‰ˆæœ¬
        
    # å…³é”®ä¿®å¤ï¼šè®¾ç½®å¤„ç†å‡½æ•°ï¼Œè€Œä¸æ˜¯ç›´æ¥è§£æå‚æ•°
    parser.set_defaults(func=count_main)


    

def main():
    """ä¸»å‡½æ•° - ç”¨äºç›´æ¥è¿è¡Œæ­¤è„šæœ¬"""
    parser = argparse.ArgumentParser(
        description='fansetools count - Process fanse3 files for read counting'
    )
    
    # æ·»åŠ å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    add_count_subparser(subparsers)
    
    args = parser.parse_args()
    
    if hasattr(args, 'func'):
        args.func(args)
    else:
        parser.print_help()




if __name__ == '__main__':
    main()


    def test_gene_level_counting():
        """æµ‹è¯•åŸºå› æ°´å¹³è®¡æ•°åŠŸèƒ½"""
        import tempfile
        import shutil
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        print(f"æµ‹è¯•ç›®å½•: {temp_dir}")
        
        try:
            # æµ‹è¯•æ–‡ä»¶è·¯å¾„
            fanse_file = r"\\fs2\D\DATA\Zhaojing\3.fanse3_result\old_s14\26.9311-Endosperm_RNC_R1_trimmed.fanse3"
            fanse_file = r'\\fs2\D\DATA\Zhaojing\3.fanse3_result\old_s14\16.9311-Root-RNC_R1_trimmed.fanse3'
            refflat_file = r'\\fs2\D\DATA\Zhaojing\202209æ•°æ®æ±‡ç¼´\0.Ref_seqs\20251024Oryza_sativa.IRGSP_9311.rna.refflat'
            # åˆ›å»ºæ¨¡æ‹Ÿçš„æ³¨é‡Šæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰

            # annotation_df = load_annotation_data(gtf_file )
            annotation_df = read_refflat_with_commented_header(refflat_file)
            
            # åˆ›å»ºè®¡æ•°å™¨å®ä¾‹
            counter = FanseCounter(
                input_file=fanse_file,
                output_dir=temp_dir,
                level='both',
                # minreads=0,
                gxf_file=None,
                annotation_df=annotation_df
            )
            
            print("å¼€å§‹è§£æfanseæ–‡ä»¶...")
            # counter.parse_fanse_file()
            # counter.generate_isoform_level_counts()
            counts_data, total_count = counter.parse_fanse_file()
            # counter.generate_isoform_level_counts(counts_data, total_count)  # ä¼ é€’å‚æ•°

            print(f"è§£æå®Œæˆï¼Œå…± {total_count} æ¡è®°å½•")
            print(f"è®¡æ•°æ•°æ®åŒ…å« {len(counts_data)} ç§è®¡æ•°ç±»å‹")
            
            # # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            # for count_type, counter_obj in counts_data.items():
            #     if counter_obj:  # åªæ˜¾ç¤ºéç©ºçš„è®¡æ•°å™¨
            #         print(f"{count_type}: {len(counter_obj)} ä¸ªè½¬å½•æœ¬")
            #         # æ˜¾ç¤ºå‰5ä¸ªæœ€é«˜è®¡æ•°çš„è½¬å½•æœ¬
            #         top5 = counter_obj.most_common(5)
            #         print(f"  å‰5ä¸ªè½¬å½•æœ¬: {top5}")
            
            print("\nå¼€å§‹ç”Ÿæˆisoformæ°´å¹³è®¡æ•°...")
            # æ­£ç¡®è°ƒç”¨ï¼šä¼ é€’å‚æ•°
            counter.generate_isoform_level_counts(counts_data, total_count)
            
            print("å¼€å§‹åŸºå› æ°´å¹³èšåˆ...")
            gene_level_counts_unique_genes, gene_level_counts_multi_genes  = counter.aggregate_gene_level_counts()
            
            # if gene_counts_unique_genes:
            #     print("\nåŸºå› æ°´å¹³è®¡æ•°ç»Ÿè®¡:")
            #     for count_type, gene_counter in gene_counts_unique_genes.items():
            #         if gene_counter:  # åªæ˜¾ç¤ºéç©ºçš„è®¡æ•°å™¨
            #             print(f"{count_type}: {len(gene_counter)} ä¸ªåŸºå› ")
            #             top5_genes = gene_counter.most_common(5)
            #             print(f"  å‰5ä¸ªåŸºå› : {top5_genes}")
            
            print("\nç”Ÿæˆè®¡æ•°æ–‡ä»¶...")
            count_files = counter.generate_count_files()
            print(f"ç”Ÿæˆçš„æ–‡ä»¶: {list(count_files.keys())}")





            
            print("\nè§£æç»Ÿè®¡:")
            print(f"æ€»readsæ•°: {counter.summary_stats['total_reads']}")
            print(f"å”¯ä¸€æ˜ å°„reads: {counter.summary_stats['unique_mapped']}")
            print(f"å¤šæ˜ å°„reads: {counter.summary_stats['multi_mapped']} PEI25k ")
            
            print("\nè½¬å½•æœ¬æ°´å¹³è®¡æ•°ç»Ÿè®¡:")
            for count_type, counter_data in counter.counts_data.items():
                print(f"{count_type}: {len(counter_data)} ç»„è½¬å½•æœ¬ID")
            #     if len(counter_data) > 0:
            #         top5 = counter_data.most_common(5)
            #         print(f"  å‰5ä¸ª: {top5}")
            
            print("\nè¿›è¡ŒåŸºå› æ°´å¹³èšåˆ...")
            counter.annotation_df = annotation_df
            gene_counts = counter.aggregate_gene_level_counts()
            
            if gene_counts:
                print("\nåŸºå› æ°´å¹³è®¡æ•°ç»Ÿè®¡:")
                for count_type, gene_counter in gene_counts.items():
                    print(f"{count_type}: {len(gene_counter)} ä¸ªåŸºå› ")
                    # if len(gene_counter) > 0:
                    #     top5 = gene_counter.most_common(5)
                    #     print(f"  å‰5ä¸ªåŸºå› : {top5}")
            
            print("\nç”Ÿæˆè®¡æ•°æ–‡ä»¶...")
            count_files = counter.generate_count_files()
            print(f"ç”Ÿæˆçš„æ–‡ä»¶: {list(count_files.keys())}")
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            for file_type, file_path in count_files.items():
                if file_path.exists():
                    df = pd.read_csv(file_path)
                    print(f"\n{file_type} æ–‡ä»¶ä¿¡æ¯:")
                    print(f"  è¡Œæ•°: {len(df)}")
                    print(f"  åˆ—æ•°: {len(df.columns)}")
                    if len(df) > 0:
                        print(f"  å‰3è¡Œ:")
                        print(df.head(3))
            
            # æ£€æŸ¥å¤šæ˜ å°„ä¿¡æ¯
            if counter.multi_mapping_info:
                print(f"\nå¤šæ˜ å°„äº‹ä»¶æ•°é‡: {len(counter.multi_mapping_info)}")
                multi_events = list(counter.multi_mapping_info.items())[:3]
                for transcript_ids, read_names in multi_events:
                    print(f"  è½¬å½•æœ¬: {transcript_ids}, readsæ•°: {len(read_names)}")
            
            print("\næµ‹è¯•å®Œæˆ!")
            
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
        
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_dir, ignore_errors=True)
            print(f"\næ¸…ç†æµ‹è¯•ç›®å½•: {temp_dir}")
    
    # def debug_gene_aggregation():
    #     """è°ƒè¯•åŸºå› èšåˆåŠŸèƒ½"""
    #     # åˆ›å»ºæµ‹è¯•æ•°æ®
    #     test_counts = {
    #         'firstID': Counter({
    #             'transcript1': 100,
    #             'transcript2': 50,
    #             'transcript3': 75,
    #             'transcript4': 25
    #         }),
    #         'multi': Counter({
    #             'transcript1,transcript2': 30,
    #             'transcript3,transcript4': 20
    #         })
    #     }
        
    #     test_annotation = pd.DataFrame({
    #         'txname': ['transcript1', 'transcript2', 'transcript3', 'transcript4'],
    #         'geneName': ['geneA', 'geneA', 'geneB', 'geneB']
    #     })
        
    #     # æ‰‹åŠ¨æµ‹è¯•èšåˆé€»è¾‘
    #     transcript_to_gene = dict(zip(test_annotation['txname'], test_annotation['geneName']))
    #     print("è½¬å½•æœ¬åˆ°åŸºå› çš„æ˜ å°„:", transcript_to_gene)
        
    #     for count_type, counter in test_counts.items():
    #         print(f"\nå¤„ç† {count_type}:")
    #         gene_counter = Counter()
            
    #         for transcript_ids_str, count in counter.items():
    #             print(f"  å¤„ç† '{transcript_ids_str}': è®¡æ•°={count}")
                
    #             if ',' in transcript_ids_str:
    #                 transcript_ids = transcript_ids_str.split(',')
    #                 print(f"    å¤šæ˜ å°„è½¬å½•æœ¬: {transcript_ids}")
                    
    #                 gene_counts = {}
    #                 for tid in transcript_ids:
    #                     gene = transcript_to_gene.get(tid)
    #                     if gene:
    #                         gene_counts[gene] = gene_counts.get(gene, 0) + 1
                    
    #                 print(f"    åŸºå› åˆ†å¸ƒ: {gene_counts}")
                    
    #                 if gene_counts:
    #                     for gene, gene_count in gene_counts.items():
    #                         allocation = count * (gene_count / len(transcript_ids))
    #                         gene_counter[gene] += allocation
    #                         print(f"    åˆ†é…ç»™åŸºå›  {gene}: {allocation}")
    #             else:
    #                 gene = transcript_to_gene.get(transcript_ids_str)
    #                 if gene:
    #                     gene_counter[gene] += count
    #                     print(f"    å•æ˜ å°„: åŸºå›  {gene} å¢åŠ  {count}")
            
    #         print(f"  æœ€ç»ˆåŸºå› è®¡æ•°: {dict(gene_counter)}")
    
    # if __name__ == '__main__':
    #     # è¿è¡Œæµ‹è¯•
    #     print("=" * 60)
    #     print("å¼€å§‹æµ‹è¯•åŸºå› æ°´å¹³è®¡æ•°åŠŸèƒ½")
    #     print("=" * 60)
        
    #     # å…ˆè¿è¡Œè°ƒè¯•
    #     debug_gene_aggregation()
        
    #     print("\n" + "=" * 60)
    #     print("å¼€å§‹å®Œæ•´æµ‹è¯•")
    #     print("=" * 60)
        
    #     # è¿è¡Œå®Œæ•´æµ‹è¯•
    #     test_gene_level_counting()
