#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fansetools count ç»„ä»¶ - ç”¨äºå¤„ç†fanse3æ–‡ä»¶çš„readè®¡æ•°

-count gene level
-count transcript level
-count exon level
-count cds level
-count 5utr
-count 3utr
---å¦‚æœæƒ³å®ç°è¿™ä¸ªï¼Œå¯èƒ½å¾—è½¬æ¢åæ ‡ï¼Œå°†åŸºå› ç»„åæ ‡è½¬æ¢ä¸ºè½¬å½•ç»„åæ ‡ï¼Œrefflatæ–‡ä»¶ä¸­å¯¹åº”çš„åæ ‡éƒ½è¿›è¡Œæ›´æ”¹ï¼Œå…¨éƒ¨éƒ½å‡å»ç¬¬ä¸€ä¸ªstartæ¥è½¬æ¢ã€‚
é•¿åº¦éœ€è¦æœ‰å¤šä¸ªï¼Œ
"""

import os
import sys
import argparse
import pandas as pd
import glob
from collections import Counter, defaultdict
from tqdm import tqdm
import time
from pathlib import Path
# import defaultdict

# å¯¼å…¥æ–°çš„è·¯å¾„å¤„ç†å™¨
from fansetools.utils.path_utils import PathProcessor
# å¯¼å…¥æ–°çš„fanse_parser
from fansetools.parser import fanse_parser, FANSeRecord, fanse_parser_high_performance
from fansetools.gxf2refflat_plus import convert_gxf_to_refflat, load_annotation_to_dataframe


# åœ¨æ‚¨çš„FanseCounterç±»ä¸­æ·»åŠ å¹¶è¡Œå¤„ç†æ–¹æ³•
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm.contrib.concurrent import process_map

# %% ParallelFanseCounter


class ParallelFanseCounter:
    """å¹¶è¡Œå¤„ç†å¤šä¸ªfanse3æ–‡ä»¶çš„è®¡æ•°å™¨"""

    def __init__(self, max_workers=None):
        self.max_workers = max_workers or min(mp.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
        print(f"åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨: {self.max_workers} ä¸ªè¿›ç¨‹")

    def process_files_parallel(self, file_list, output_base_dir, gxf_file=None, level='gene', paired_end=None, annotation_df=None):
        """å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶ - ä¿®å¤ç‰ˆæœ¬"""
        print(f" å¼€å§‹å¹¶è¡Œå¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªè¿›ç¨‹")

        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        tasks = []
        for input_file in file_list:
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
            file_stem = input_file.stem
            output_dir = Path(output_base_dir) / file_stem
            output_dir.mkdir(parents=True, exist_ok=True)

            task = {
                'input_file': str(input_file),
                'output_dir': str(output_dir),
                'gxf_file': gxf_file,
                'level': level,
                'paired_end': paired_end,
                'file_stem': file_stem  # ç”¨äºè¿›åº¦æ˜¾ç¤º
            }
            tasks.append(task)

        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        results = []
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {}
            for task in tasks:
                future = executor.submit(
                    self._process_single_file, task, annotation_df)
                future_to_task[future] = task

            # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»ä½“è¿›åº¦
            with tqdm(total=len(tasks), desc="æ€»ä½“è¿›åº¦", position=0) as pbar:
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    try:
                        result = future.result()
                        results.append((task['input_file'], True, result))
                        pbar.set_description(f" å®Œæˆ: {task['file_stem']}")
                    except Exception as e:
                        results.append((task['input_file'], False, str(e)))
                        pbar.set_description(f" å¤±è´¥: {task['file_stem']}")
                    finally:
                        pbar.update(1)

        return results

    def _process_single_file(self, task, annotation_df=None):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå·¥ä½œè¿›ç¨‹å‡½æ•°ï¼‰"""
        try:
            # åœ¨å·¥ä½œè¿›ç¨‹ä¸­é‡æ–°åŠ è½½æ³¨é‡Šæ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if task['gxf_file'] and annotation_df is None:
                # è¿™é‡Œå¯ä»¥æ·»åŠ åœ¨å·¥ä½œè¿›ç¨‹ä¸­åŠ è½½æ³¨é‡Šçš„é€»è¾‘
                pass

            counter = FanseCounter(
                input_file=task['input_file'],
                output_dir=task['output_dir'],
                gxf_file=task['gxf_file'],
                level=task['level'],
                paired_end=task['paired_end'],
                annotation_df=annotation_df  # ä¼ é€’å·²åŠ è½½çš„æ³¨é‡Šæ•°æ®
            )

            # è¿è¡Œè®¡æ•°å¤„ç†
            result = counter.run()
            return f"æˆåŠŸå¤„ç† {task['file_stem']}"

        except Exception as e:
            raise Exception(f"å¤„ç†æ–‡ä»¶ {task['input_file']} å¤±è´¥: {str(e)}")


def count_main_parallel(args):
    """æ”¯æŒå¹¶è¡Œçš„ä¸»å‡½æ•°"""
    print_mini_fansetools()
    processor = PathProcessor()

    try:
        # 1. è§£æè¾“å…¥æ–‡ä»¶
        input_files = processor.parse_input_paths(
            args.input, ['.fanse', '.fanse3', '.fanse3.gz', '.fanse.gz'])
        if not input_files:
            print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
            return

        print(f"æ‰¾åˆ° {len(input_files)} ä¸ªè¾“å…¥æ–‡ä»¶")

        # 2. åŠ è½½æ³¨é‡Šæ–‡ä»¶ï¼ˆä¸»è¿›ç¨‹åŠ è½½ï¼Œç„¶åä¼ é€’ç»™å·¥ä½œè¿›ç¨‹ï¼‰
        annotation_df = None
        if args.gxf:
            annotation_df = load_annotation_data(args)
            if annotation_df is None:
                print("é”™è¯¯: æ— æ³•åŠ è½½æ³¨é‡Šæ•°æ®")
                return
            print(f"å·²åŠ è½½æ³¨é‡Šæ•°æ®: {len(annotation_df)} ä¸ªè½¬å½•æœ¬")
        else:
            print("è­¦å‘Š: æœªæä¾›æ³¨é‡Šæ–‡ä»¶ï¼Œå°†åªç”Ÿæˆisoformæ°´å¹³è®¡æ•°")

        # 3. è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = Path(
            args.output) if args.output else Path.cwd() / "fansetools_results"
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")

        # 4. æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
        files_to_process = []
        skipped_files = 0

        for input_file in input_files:
            file_stem = input_file.stem
            individual_output_dir = output_dir / file_stem

            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            output_files_to_check = []
            if args.level in ['isoform', 'both']:
                output_files_to_check.append(
                    individual_output_dir / f"{file_stem}_isoform_level.counts.csv")
            if args.level in ['gene', 'both'] and args.gxf:
                output_files_to_check.append(
                    individual_output_dir / f"{file_stem}_gene_level.counts.csv")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            all_files_exist = all(f.exists() for f in output_files_to_check)

            if args.resume and all_files_exist:
                print(f"  è·³è¿‡: {input_file.name} - è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨")
                skipped_files += 1
            else:
                files_to_process.append(input_file)

        if not files_to_process:
            print("æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†å®Œæˆ")
            return

        print(
            f"æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {skipped_files} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(files_to_process)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")

        # 5. å¹¶è¡Œå¤„ç†
        max_workers = args.processes if hasattr(
            args, 'processes') and args.processes > 1 else min(mp.cpu_count(), len(files_to_process))

        if max_workers == 1:
            print("ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼")
            return count_main_serial(args)  # å›é€€åˆ°ä¸²è¡Œå¤„ç†

        parallel_counter = ParallelFanseCounter(max_workers=max_workers)

        print("ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†...")
        print("=" * 60)

        start_time = time.time()
        results = parallel_counter.process_files_parallel(
            file_list=files_to_process,
            output_base_dir=output_dir,
            gxf_file=args.gxf,
            level=args.level,
            paired_end=args.paired_end,
            annotation_df=annotation_df
        )

        duration = time.time() - start_time

        # 6. è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print(" å¤„ç†ç»“æœæ‘˜è¦")
        print("=" * 60)

        success_count = sum(1 for _, success, _ in results if success)
        failed_count = len(results) - success_count

        print(f" æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        print(f" å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        print(f" æ€»è€—æ—¶: {duration:.2f} ç§’")

        if failed_count > 0:
            print("\nå¤±è´¥è¯¦æƒ…:")
            for input_file, success, result in results:
                if not success:
                    print(f"  - {Path(input_file).name}: {result}")

        print(f"\n å¤„ç†å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")

    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


def count_main_serial(args):
    """ä¸²è¡Œå¤„ç†ç‰ˆæœ¬ï¼ˆåŸæœ‰çš„count_mainå‡½æ•°ï¼‰"""
    print("ä½¿ç”¨å•ä»»åŠ¡å¤„ç†æ¨¡å¼...")
    processor = PathProcessor()

    try:
        # åŸæœ‰çš„ä¸²è¡Œå¤„ç†é€»è¾‘...
        input_files = processor.parse_input_paths(
            args.input, ['.fanse', '.fanse3', '.fanse3.gz', '.fanse.gz'])
        if not input_files:
            print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
            return

        # åŠ è½½æ³¨é‡Šæ–‡ä»¶
        annotation_df = None
        if args.gxf:
            annotation_df = load_annotation_data(args)
            if annotation_df is None:
                print("é”™è¯¯: æ— æ³•åŠ è½½æ³¨é‡Šæ•°æ®")
                return
        else:
            print("è­¦å‘Š: æœªæä¾›æ³¨é‡Šæ–‡ä»¶ï¼Œå°†åªç”Ÿæˆisoformæ°´å¹³è®¡æ•°")

        # ç”Ÿæˆè¾“å‡ºæ˜ å°„
        output_map = processor.generate_output_mapping(
            input_files, args.output, '.counts.csv')

        # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
        skipped_files = 0
        if args.resume:
            print("å¯ç”¨æ–­ç‚¹ç»­ä¼ æ¨¡å¼ï¼Œæ£€æŸ¥å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶...")
            files_to_process = {}

            for input_file, output_file in output_map.items():
                output_dir = Path(output_file).parent
                input_stem = input_file.stem

                output_files_to_check = []
                if args.level in ['isoform', 'both']:
                    output_files_to_check.append(
                        output_dir / f"{input_stem}_isoform_level.counts.csv")
                if args.level in ['gene', 'both']:
                    output_files_to_check.append(
                        output_dir / f"{input_stem}_gene_level.counts.csv")
                    output_files_to_check.append(
                        output_dir / f"{input_stem}_multi_genes_level.counts.csv")

                all_files_exist = all(f.exists()
                                      for f in output_files_to_check)
                if all_files_exist:
                    print(f"  è·³è¿‡: {input_file.name} - è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨")
                    skipped_files += 1
                else:
                    files_to_process[input_file] = output_file

            output_map = files_to_process
            print(f"æ–­ç‚¹ç»­ä¼ : è·³è¿‡ {skipped_files} ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™ {len(output_map)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")

            if not output_map:
                print("æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†å®Œæˆ")
                return

        # ä¸²è¡Œå¤„ç†æ¯ä¸ªæ–‡ä»¶
        for i, (input_file, output_file) in enumerate(output_map.items(), 1):
            print(
                f"\n[{i + skipped_files}/{len(input_files)}] å¤„ç†: {input_file.name}")
            print(f"  è¾“å‡º: {output_file}")

            try:
                counter = FanseCounter(
                    input_file=str(input_file),
                    output_dir=str(output_file.parent),
                    output_filename=output_file.name,
                    gxf_file=args.gxf,
                    level=args.level if annotation_df is not None else 'isoform',
                    paired_end=args.paired_end,
                    annotation_df=annotation_df,
                )
                count_files = counter.run()
                print(" å®Œæˆ")
            except Exception as e:
                print(f" å¤„ç†å¤±è´¥: {str(e)}")

        print(f"\nå¤„ç†å®Œæˆ: æ€»å…± {len(input_files)} ä¸ªæ–‡ä»¶")

    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")


def count_main(args):
    """ä¸»å…¥å£å‡½æ•°ï¼Œæ ¹æ®å‚æ•°é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œ"""
    if hasattr(args, 'processes') and args.processes != 1:
        return count_main_parallel(args)
    else:
        return count_main_serial(args)


class FanseCounter:
    """fanse3æ–‡ä»¶è®¡æ•°å¤„ç†å™¨"""

    def __init__(self, input_file, output_dir, level='isoform',
                 # minreads=0,
                 rpkm=0,
                 gxf_file=None,
                 paired_end=None,
                 output_filename=None,
                 annotation_df=None):

        # æ·»åŠ è®¡æ•°ç±»å‹å‰ç¼€
        self.isoform_prefix = 'isoform_'
        self.gene_prefix = 'gene_'

        self.input_file = Path(input_file)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.level = level
        # self.minreads = minreads
        # self.rpkm = rpkm
        self.gxf_file = gxf_file
        self.paired_end = paired_end
        self.output_filename = output_filename  # æ–°å¢ï¼šæ”¯æŒè‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶å
        self.annotation_df = annotation_df  # æ–°å¢ï¼šæ³¨é‡Šæ•°æ®æ¡†

        # # å­˜å‚¨è®¡æ•°ç»“æœ
        # self.counts_data = {}
        # self.summary_stats = {}
        # self.multi_mapping_info = defaultdict(list)  # å­˜å‚¨å¤šæ˜ å°„ä¿¡æ¯
        # å­˜å‚¨è®¡æ•°ç»“æœ
        self.counts_data = {
            # isoformæ°´å¹³è®¡æ•°
            f'{self.isoform_prefix}raw': Counter(),
            f'{self.isoform_prefix}unique': Counter(),
            f'{self.isoform_prefix}multi': Counter(),
            f'{self.isoform_prefix}firstID': Counter(),
            f'{self.isoform_prefix}multi2all': Counter(),
            f'{self.isoform_prefix}multi_equal': Counter(),
            f'{self.isoform_prefix}multi_EM': Counter(),
            f'{self.isoform_prefix}Final_em': Counter(),
            f'{self.isoform_prefix}Final_eq': Counter(),

            # geneæ°´å¹³è®¡æ•°
            f'{self.gene_prefix}unique': Counter(),
            f'{self.gene_prefix}multi_equal': Counter(),
            f'{self.gene_prefix}multi_EM': Counter(),
            f'{self.gene_prefix}Final_em': Counter(),
            f'{self.gene_prefix}Final_eq': Counter(),
        }
        self.summary_stats = {}
        self.multi_mapping_info = defaultdict(list)

    def judge_sequence_mode(self):
        """åˆ¤æ–­æµ‹åºæ¨¡å¼ï¼ˆå•ç«¯/åŒç«¯ï¼‰"""
        if self.paired_end and os.path.isfile(self.paired_end):
            print('Pair-End mode detected.')
            return True
        else:
            print('Single-End mode detected.')
            return False


# %% parser

    # def parse_fanse_file(self):  # å±€éƒ¨ä¼˜åŒ–å˜é‡åçš„ï¼Œæµ‹è¯•20251114
    #     """
    #     ä¸“é—¨è´Ÿè´£è§£æfanse3æ–‡ä»¶ï¼Œç›´æ¥è¿›è¡ŒåŸºæœ¬è®¡æ•°
    #     """
    #     # é€‰æ‹©ä¼˜åŒ–ç‰ˆæœ¬
    #     if self.input_file.stat().st_size > 1024 * 1024 * 1024:  # å¤§äº1024 MB
    #         fanse_parser = fanse_parser_high_performance
    #     else:
    #         fanse_parser = fanse_parser

    #     print(f'Parsing {self.input_file.name}ï¼Œé¢„å®šä¹‰åŠ é€Ÿç‰ˆ')
    #     start_time = time.time()

    #     # åˆå§‹åŒ–æ‰€æœ‰è®¡æ•°å™¨ - ä½¿ç”¨å¸¦å‰ç¼€çš„é”®
    #     counts_data = {
    #         f'{self.isoform_prefix}raw': Counter(),
    #         f'{self.isoform_prefix}multi': Counter(),
    #         f'{self.isoform_prefix}unique': Counter(),
    #         f'{self.isoform_prefix}firstID': Counter(),
    #         f'{self.isoform_prefix}multi2all': Counter(),
    #         f'{self.isoform_prefix}multi_equal': Counter(),
    #         f'{self.isoform_prefix}multi_EM': Counter(),
    #         f'{self.isoform_prefix}multi_EM_cannot_allocate_tpm': Counter(),
    #         f'{self.isoform_prefix}Final_em': Counter(),
    #         f'{self.isoform_prefix}Final_eq': Counter(),
    #     }
    #     # ä½¿ç”¨å±€éƒ¨å˜é‡åŠ é€Ÿ
    #     raw, multi, unique, firstID, multi2all = (
    #         counts_data[f'{self.isoform_prefix}raw'],
    #         counts_data[f'{self.isoform_prefix}multi'],
    #         counts_data[f'{self.isoform_prefix}unique'],
    #         counts_data[f'{self.isoform_prefix}firstID'],
    #         counts_data[f'{self.isoform_prefix}multi2all']
    #     )

    #     total_count = 0

    #     files_to_process = [self.input_file]
    #     if self.paired_end:
    #         files_to_process.append(Path(self.paired_end))

    #     for fanse_file in files_to_process:
    #         if not fanse_file.exists():
    #             continue

    #         try:
    #             sample_size = 100000  # é‡‡æ ·æ•°ç›®ï¼Œç”¨æ¥ä¼°ç®—æ€»readsæ•°
    #             estimated_records = self.calculate_file_record_estimate(
    #                 fanse_file, sample_size)

    #             with tqdm(total=estimated_records, unit='reads', mininterval=5, unit_scale=True) as pbar:
    #                 for record in fanse_parser(str(fanse_file)):
    #                     if record.ref_names:
    #                         transcript_ids = record.ref_names
    #                         is_multi = record.is_multi

    #                         # ç›´æ¥æ›´æ–°è®¡æ•°å™¨ - ä½¿ç”¨é¢„å…ˆå®šä¹‰çš„å¸¦å‰ç¼€çš„é”®åŠ é€Ÿ
    #                         raw_id = transcript_ids[0] if len(
    #                             transcript_ids) == 1 else ','.join(transcript_ids)
    #                         raw[raw_id] += 1
    #                         firstID[transcript_ids[0]] += 1

    #                         if is_multi:
    #                             multi[raw_id] += 1
    #                             for tid in transcript_ids:
    #                                 multi2all[tid] += 1
    #                         else:
    #                             unique[raw_id] += 1

    #                         total_count += 1
    #                         pbar.update(1)

    #         except Exception as e:
    #             print(f"Error parsing file {fanse_file}: {str(e)}")
    #             continue

    #     parsing_time = time.time() - start_time
    #     print(
    #         f"Parsing completed in {parsing_time:.2f} seconds, {total_count} records")

    #     return counts_data, total_count

    def parse_fanse_file_optimized_final(self, position=0):
        """ç»¼åˆä¼˜åŒ–ç‰ˆæœ¬"""

        # # æ·»åŠ å¹¶è¡Œæ£€æŸ¥ï¼ˆæ–°å¢ä»£ç ï¼‰
        # if hasattr(self, 'num_processes') and self.num_processes > 1:
        #     file_size_mb = self.input_file.stat().st_size / (1024 * 1024)
        #     if file_size_mb > 500:  # å¤§äº500MBä½¿ç”¨å¹¶è¡Œ
        #         return self._parse_parallel(self.num_processes)

        # é€‰æ‹©ä¼˜åŒ–ç‰ˆæœ¬
        if self.input_file.stat().st_size > 1024 * 1024 * 1024:  # å¤§äº1024 MB
            fanse_parser_selected = fanse_parser_high_performance
        else:
            fanse_parser_selected = fanse_parser

        print(f'Parsing {self.input_file.name}')
        start_time = time.time()

        # é¢„åˆå§‹åŒ–æ•°æ®ç»“æ„
        counts_data = {
            f'{self.isoform_prefix}raw': Counter(),
            f'{self.isoform_prefix}multi': Counter(),
            f'{self.isoform_prefix}unique': Counter(),
            f'{self.isoform_prefix}firstID': Counter(),
            f'{self.isoform_prefix}multi2all': Counter(),
            f'{self.isoform_prefix}multi_equal': Counter(),
            f'{self.isoform_prefix}multi_EM': Counter(),
            f'{self.isoform_prefix}multi_EM_cannot_allocate_tpm': Counter(),
            f'{self.isoform_prefix}Final_em': Counter(),
            f'{self.isoform_prefix}Final_eq': Counter(),
        }

        total_count = 0
        batch_size = 600000
        # update_interval = 10000

        # ä½¿ç”¨å±€éƒ¨å˜é‡åŠ é€Ÿ
        raw, multi, unique, firstID, multi2all = (
            counts_data[f'{self.isoform_prefix}raw'],
            counts_data[f'{self.isoform_prefix}multi'],
            counts_data[f'{self.isoform_prefix}unique'],
            counts_data[f'{self.isoform_prefix}firstID'],
            counts_data[f'{self.isoform_prefix}multi2all']
        )

        for position, fanse_file in enumerate([self.input_file] + ([Path(self.paired_end)] if self.paired_end else [])):
            if not fanse_file.exists():
                continue

            try:
                batch = []
                # last_update = 0

                # file_size = fanse_file.stat().st_size
                # estimated_records = max(1, file_size // 527)
                # æ™ºèƒ½ä¼°ç®—è®°å½•æ•°
                sample_size = 100000  # é‡‡æ ·æ•°ç›®ï¼Œç”¨æ¥ä¼°ç®—æ€»readsæ•°
                estimated_records = self.calculate_file_record_estimate(
                    fanse_file, sample_size)

                with tqdm(total=estimated_records, unit='reads', mininterval=5, unit_scale=True, position=position, leave=False) as pbar:
                    # è¿›åº¦æ¡æ›´æ–°é¢‘ç‡æ§åˆ¶
                    update_interval = 1000
                    update_counter = 0

                    for i, record in enumerate(fanse_parser_selected(str(fanse_file))):
                        if record.ref_names:
                            total_count += 1

                            # æ‰¹é‡å¤„ç†
                            batch.append(record)
                            if len(batch) >= batch_size:
                                self._fast_batch_process(
                                    batch, raw, multi, unique, firstID, multi2all)
                                batch = []

                            # æ™ºèƒ½æ›´æ–°
                            update_counter += 1
                            if update_counter >= update_interval:
                                pbar.update(update_counter)
                                update_counter = 0
                            # å‡å°‘è¿›åº¦æ›´æ–°é¢‘ç‡
                            # if i - last_update >= update_interval:
                            #     print(f"Processed {i} records...", end='\r')
                            #     last_update = i
                        else:
                            update_counter += 1

                    # æ›´æ–°å‰©ä½™çš„è¿›åº¦
                    if update_counter > 0:
                        pbar.update(update_counter)
                        # pbar.update(1)
                    # å¤„ç†å‰©ä½™æ‰¹æ¬¡
                    if batch:
                        self._fast_batch_process(
                            batch, raw, multi, unique, firstID, multi2all)

            except Exception as e:
                print(f"Error: {e}")
                continue

        duration = time.time() - start_time
        print(
            f" Completed: {total_count} records in {duration:.2f}s ({total_count/duration:.0f} rec/sec)")

        return counts_data, total_count

    def _fast_batch_process(self, batch, raw, multi, unique, firstID, multi2all):
        """å¿«é€Ÿæ‰¹é‡å¤„ç†"""
        for record in batch:
            ids = record.ref_names
            is_multi = record.is_multi

            # æœ€å°åŒ–å­—ç¬¦ä¸²æ“ä½œ
            first_id = ids[0]
            raw_id = first_id if len(ids) == 1 else ','.join(ids)

            raw[raw_id] += 1
            firstID[first_id] += 1

            if is_multi:
                multi[raw_id] += 1
                # ä½¿ç”¨é›†åˆæ“ä½œä¼˜åŒ–å¤šIDå¤„ç†
                for tid in ids:
                    multi2all[tid] += 1
            else:
                unique[raw_id] += 1

    def calculate_average_record_size(self, file_path, sample_size=100000):
        """
        é€šè¿‡é‡‡æ ·è®¡ç®—fanse3æ–‡ä»¶çš„å¹³å‡è®°å½•å¤§å°

        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·è®°å½•æ•°ï¼ˆé»˜è®¤10000æ¡ï¼‰

        è¿”å›:
            å¹³å‡æ¯æ¡è®°å½•çš„å­—èŠ‚æ•°
        """
        print(f"é‡‡æ ·è®¡ç®—å¹³å‡è®°å½•å¤§å°ï¼Œé‡‡æ ·æ•°: {sample_size}")

        try:
            total_bytes = 0
            record_count = 0

            # ä½¿ç”¨fanse_parserè¿›è¡Œé‡‡æ ·
            for i, record in enumerate(fanse_parser(str(file_path))):
                if i >= sample_size:
                    break

                # ä¼°ç®—å½“å‰è®°å½•çš„å¤§å°ï¼ˆåŸºäºè®°å½•å†…å®¹çš„å­—ç¬¦ä¸²é•¿åº¦ï¼‰
                record_size = len(str(record))  # åŸºæœ¬ä¼°ç®—
                total_bytes += record_size
                record_count += 1

            if record_count > 0:
                avg_size = total_bytes / record_count
                print(f"é‡‡æ ·å®Œæˆ: {record_count} æ¡è®°å½•ï¼Œå¹³å‡å¤§å°: {avg_size:.1f} å­—èŠ‚")
                return avg_size
            else:
                print("è­¦å‘Š: æ— æ³•é‡‡æ ·è®°å½•ï¼Œä½¿ç”¨é»˜è®¤å€¼527")
                return 527

        except Exception as e:
            print(f"é‡‡æ ·å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼527")
            return 527

    def calculate_file_record_estimate(self, file_path, sample_size=100000):
        """
        ç»¼åˆä¼°ç®—æ–‡ä»¶ä¸­çš„è®°å½•æ•°é‡

        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·å¤§å°

        è¿”å›:
            ä¼°è®¡çš„è®°å½•æ•°é‡
        """
        if not file_path.exists():
            return 0

        # è·å–æ–‡ä»¶å¤§å°
        file_size = file_path.stat().st_size

        # å¦‚æœæ˜¯å°æ–‡ä»¶ï¼Œç›´æ¥è§£æè®¡æ•°
        if file_size < 100 * 1024 * 1024:  # å°äº10MBçš„æ–‡ä»¶
            print("å°æ–‡ä»¶ï¼Œç›´æ¥è®¡æ•°...")
            try:
                record_count = sum(
                    1 for _ in fanse_parser(str(file_path)))
                print(f"ç›´æ¥è®¡æ•°å®Œæˆ: {record_count} æ¡è®°å½•")
                return record_count
            except:
                pass

        # å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨é‡‡æ ·ä¼°ç®—
        avg_size = self.calculate_average_record_size(
            file_path, sample_size)*0.8  # ç»éªŒå‡å»50å­—èŠ‚ï¼Œäººä¸ºå¢å¤§ä¸€ç‚¹ä¼°ç®—çš„readsæ€»æ•°ï¼Œåè€Œæ¯”è¾ƒç¬¦åˆå®é™…
        estimated_records = max(1, int(file_size / avg_size))

        print(f"æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        print(f"å¹³å‡è®°å½•å¤§å°: {avg_size:.1f} å­—èŠ‚")
        print(f"ä¼°è®¡Fanseè®°å½•æ•°: {estimated_records} æ¡")

        return estimated_records


# %% generate counts

    def _rescue_multi_mappings_by_tpm(self, counts_data, prefix=None, length_dict=None, annotation_df=None):
        """
        é€šç”¨å¤šæ˜ å°„å¤„ç† - 
        1.æ”¯æŒisoformå’Œgene levelå‰ç¼€
        2.æ”¯æŒè®°å½•multi mapped readsçš„åˆ†é…æ¯”ä¾‹
        å‚æ•°:
            counts_data: è®¡æ•°æ•°æ®å­—å…¸
            prefix: å‰ç¼€ç±»å‹ ('isoform_' æˆ– 'gene_')ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹çš„isoform_prefix
            length_dict: IDåˆ°é•¿åº¦çš„æ˜ å°„å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä»annotation_dfè·å–
            annotation_df: æ³¨é‡Šæ•°æ®æ¡†ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹çš„annotation_df
        è¿”å›:
            åˆ†é…æ¯”ä¾‹å­—å…¸ï¼Œæ ¼å¼: {å¤šæ˜ å°„ID: {åˆ†é…ID1: æ¯”ä¾‹1, åˆ†é…ID2: æ¯”ä¾‹2, ...}}
        """
        if prefix is None:
            prefix = self.isoform_prefix  # é»˜è®¤ä½¿ç”¨isoformå‰ç¼€

        print(f"å¼€å§‹é«˜çº§å¤šæ˜ å°„åˆ†æ (å‰ç¼€: {prefix})...")

        # æ£€æŸ¥æ˜¯å¦æœ‰multiæ•°æ®
        multi_key = f'{prefix}multi'
        if multi_key not in counts_data or not counts_data[multi_key]:
            print(f"æ²¡æœ‰{prefix}å¤šæ˜ å°„æ•°æ®ï¼Œè·³è¿‡é«˜çº§åˆ†æ")
            return

        # è·å–é•¿åº¦ä¿¡æ¯
        if length_dict is None:
            length_dict = {}
            current_annotation_df = annotation_df if annotation_df is not None else self.annotation_df

            if current_annotation_df is not None:
                # æ ¹æ®å‰ç¼€é€‰æ‹©ä¸åŒçš„åˆ—åæ˜ å°„
                if prefix == self.isoform_prefix:
                    # isoformæ°´å¹³ï¼šä½¿ç”¨è½¬å½•æœ¬é•¿åº¦
                    id_col = 'txname' if 'txname' in current_annotation_df.columns else 'transcript_id'
                    length_col = 'txLength' if 'txLength' in current_annotation_df.columns else 'length'
                else:
                    # geneæ°´å¹³ï¼šä½¿ç”¨åŸºå› é•¿åº¦
                    id_col = 'geneName' if 'geneName' in current_annotation_df.columns else 'gene_id'
                    # å¯¹äºåŸºå› æ°´å¹³ï¼Œä½¿ç”¨æœ€é•¿è½¬å½•æœ¬é•¿åº¦
                    length_col = 'genelongesttxLength' if 'genelongesttxLength' in current_annotation_df.columns else 'txLength'

                if id_col in current_annotation_df.columns and length_col in current_annotation_df.columns:
                    if prefix == self.gene_prefix:
                        # å¯¹äºåŸºå› æ°´å¹³ï¼Œéœ€è¦è®¡ç®—æ¯ä¸ªåŸºå› çš„æœ€é•¿è½¬å½•æœ¬é•¿åº¦
                        gene_lengths = current_annotation_df.groupby(id_col)[
                            length_col].max()
                        length_dict = gene_lengths.to_dict()
                    else:
                        # isoformæ°´å¹³ç›´æ¥æ˜ å°„
                        length_dict = dict(
                            zip(current_annotation_df[id_col], current_annotation_df[length_col]))

                    print(f"åŠ è½½äº† {len(length_dict)} ä¸ª{prefix}IDçš„é•¿åº¦ä¿¡æ¯")

        # é€šè¿‡uniqueéƒ¨åˆ†è®¡ç®—TPM
        unique_key = f'{prefix}unique'
        tpm_values = self._calculate_tpm(
            counts_data.get(unique_key, Counter()), length_dict)
        print(f"è®¡ç®—äº† {len(tpm_values)} ä¸ªå…·æœ‰unique reads {prefix}IDçš„TPMå€¼")

        # åˆå§‹åŒ–è®¡æ•°å™¨
        multi_equal_counter = Counter()
        multi_em_counter = Counter()
        multi_em_cannot_allocate_tpm_counter = Counter()

        allocation_ratios = {}

        processed_events = 0
        total_events = len(counts_data[multi_key])

        print(f"å¼€å§‹å¤„ç† {total_events} ä¸ª{prefix}å¤šæ˜ å°„äº‹ä»¶...")

        for ids_str, event_count in counts_data[multi_key].items():
            try:
                # åˆ†å‰²IDï¼ˆå¯èƒ½æ˜¯è½¬å½•æœ¬IDæˆ–åŸºå› IDï¼‰
                ids = ids_str.split(',')

                # multi_equal: å¹³å‡åˆ†é…
                equal_share_per_read = 1.0 / len(ids)
                for id_val in ids:
                    multi_equal_counter[id_val] += event_count * \
                        equal_share_per_read
                equal_allocation = {id_val: equal_share_per_read for id_val in ids}
                allocation_ratios[ids_str] = {}
                allocation_ratios[ids_str]['equal'] = equal_allocation
                allocation_ratios[ids_str]['multi2all'] = {id_val: 1.0 for id_val in ids}

                # multi_EM: æŒ‰TPMæ¯”ä¾‹åˆ†é…
                allocation = self._allocate_multi_reads_by_tpm_rescued(
                    ids, tpm_values)
                if allocation:
                    for id_val, share_ratio in allocation.items():
                        multi_em_counter[id_val] += event_count * share_ratio
                    allocation_ratios[ids_str]['EM'] = allocation
                else:
                    # æ— æ³•åˆ†é…çš„æƒ…å†µ
                    multi_em_cannot_allocate_tpm_counter[ids_str] += event_count
                    allocation_ratios[ids_str]['EM'] = None

                processed_events += 1
                if processed_events % 10000 == 0:
                    print(
                        f"å·²å¤„ç† {processed_events}/{total_events} ä¸ª{prefix}å¤šæ˜ å°„äº‹ä»¶")

            except Exception as e:
                print(f"å¤„ç†{prefix}å¤šæ˜ å°„äº‹ä»¶ {ids_str} æ—¶å‡ºé”™: {str(e)}")
                continue

        # æ›´æ–°è®¡æ•°å™¨
        counts_data[f'{prefix}multi_equal'] = multi_equal_counter
        counts_data[f'{prefix}multi_EM'] = multi_em_counter
        counts_data[f'{prefix}multi_EM_cannot_allocate_tpm'] = multi_em_cannot_allocate_tpm_counter

        counts_data[f'{prefix}allocation_ratios'] = allocation_ratios

        print(f"{prefix}é«˜çº§å¤šæ˜ å°„åˆ†æå®Œæˆï¼š")
        print(f"  - {prefix}multi_equal: {len(multi_equal_counter)} ä¸ªID")
        print(f"  - {prefix}multi_EM: {len(multi_em_counter)} ä¸ªID")
        print(f"  - æ— æ³•åˆ†é…TPMçš„äº‹ä»¶: {len(multi_em_cannot_allocate_tpm_counter)} ä¸ª")
        print(f"  - è®°å½•åˆ†é…æ¯”ä¾‹: {len(allocation_ratios)} ä¸ªäº‹ä»¶")

        return allocation_ratios

    def _rescue_multi_mappings_by_tpm_isoform(self, counts_data):
        """isoformæ°´å¹³çš„å¤šæ˜ å°„å¤„ç†ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self._rescue_multi_mappings_by_tpm(counts_data, prefix=self.isoform_prefix)

    def _rescue_multi_mappings_by_tpm_gene(self, counts_data):
        """geneæ°´å¹³çš„å¤šæ˜ å°„å¤„ç†"""
        return self._rescue_multi_mappings_by_tpm(counts_data, prefix=self.gene_prefix)

    def _calculate_tpm(self, unique_counts, transcript_lengths):
        '''
        """è®¡ç®—æ¯ä¸ªåŸºå› çš„TPMå€¼"""
        TPMæ˜¯ä¸€ç§å¸¸ç”¨çš„åŸºå› è¡¨è¾¾æ ‡å‡†åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæ¶ˆé™¤åŸºå› é•¿åº¦å’Œæµ‹åºæ·±åº¦çš„å½±å“ã€‚
        æ­£ç¡®çš„è®¡ç®—æ­¥éª¤åˆ†ä¸ºä¸¤æ­¥ï¼š
        - ç¬¬ä¸€æ­¥æ˜¯RPKæ ‡å‡†åŒ–ï¼Œç”¨åŸºå› çš„åŸå§‹readsæ•°é™¤ä»¥åŸºå› é•¿åº¦(ä»¥åƒç¢±åŸºä¸ºå•ä½)ï¼›
        - ç¬¬äºŒæ­¥æ˜¯æ€»å’Œæ ‡å‡†åŒ–ï¼Œå°†æ‰€æœ‰åŸºå› çš„RPKå€¼ç›¸åŠ ï¼Œç„¶åç”¨æ¯ä¸ªåŸºå› çš„RPKå€¼é™¤ä»¥è¿™ä¸ªæ€»å’Œå†ä¹˜ä»¥ä¸€ç™¾ä¸‡ã€‚
        '''
        if not unique_counts or not transcript_lengths:
            return {}

        # è®¡ç®—RPK (Reads Per Kilobase)
        rpk_values = {}
        total_rpk = 0

        for transcript, count in unique_counts.items():
            if transcript in transcript_lengths and transcript_lengths[transcript] > 0:
                length_kb = transcript_lengths[transcript] / 1000
                rpk = count / length_kb
                rpk_values[transcript] = rpk
                total_rpk += rpk  # è®¡ç®—æ€»rpk

        # è®¡ç®—TPM (Transcripts Per Million)
        tpm_values = {}
        if total_rpk > 0:
            scaling_factor = 1e6 / total_rpk
            for transcript, rpk in rpk_values.items():
                tpm_values[transcript] = rpk * scaling_factor

        return tpm_values

    def _allocate_multi_reads_by_tpm_rescued(self, transcript_ids, tpm_values):
        """æ ¹æ®unique è®¡ç®—çš„  TPMå€¼åˆ†é…å¤šæ˜ å°„reads"""
        allocation = {}

        # è¿‡æ»¤æ‰æ²¡æœ‰TPMå€¼çš„è½¬å½•æœ¬
        valid_transcripts = [
            tid for tid in transcript_ids if tid in tpm_values and tpm_values[tid] > 0]

        if not valid_transcripts:
            # å›é€€åˆ°å¹³å‡åˆ†é…ï¼Œï¼Œï¼Œè¿™ä¸ªæœ‰ç‚¹ä¸å¤ªåˆé€‚ï¼Œå¯ä»¥æ”¾åœ¨å¦ä¸€ä¸ªè¡¨æ ¼multi_EM_cannot_allocate_tpmé‡Œï¼Œæš‚æ—¶ä¸å‚ä¸åˆ†é…  20251111
            return None
            # share = 1.0 / len(transcript_ids)
            # return  {tid: share for tid in transcript_ids}

        # è®¡ç®—æ€»TPM
        total_tpm = sum(tpm_values[tid] for tid in valid_transcripts)

        # æŒ‰TPMæ¯”ä¾‹åˆ†é…total_tpm
        for tid in valid_transcripts:
            allocation[tid] = tpm_values[tid] / total_tpm

        # å¤„ç†ä¸åœ¨valid_transcriptsä¸­çš„è½¬å½•æœ¬,å…¶å®ä»€ä¹ˆéƒ½ä¸åšæœ€å¥½äº†ï¼Œå…ˆæ³¨é‡Šæ‰20251125
        # invalid_transcripts = [
        #     tid for tid in transcript_ids if tid not in valid_transcripts]
        # if invalid_transcripts and total_tpm > 0:
        #     remaining_share = 1.0 - sum(allocation.values())
        #     if remaining_share > 0:
        #         share_per_invalid = remaining_share / len(invalid_transcripts)
        #         for tid in invalid_transcripts:
        #             allocation[tid] = share_per_invalid

        return allocation

    def generate_isoform_level_counts(self, counts_data, total_count):
        """
        æ ¹æ®è§£æçš„è®¡æ•°æ•°æ®ç”Ÿæˆisoformæ°´å¹³çš„å„ç§è®¡æ•°
        """
        print("Generating isoform level counts...")
        start_time = time.time()

        # ç¬¬äºŒé˜¶æ®µï¼šé«˜çº§å¤šæ˜ å°„è®¡æ•°rescue multi mapped reads
        # ä¿®å¤ï¼šä½¿ç”¨å¸¦å‰ç¼€çš„é”®
        if counts_data[f'{self.isoform_prefix}multi']:
            print("Starting advanced multi-mapping analysis...")
            # self._rescue_multi_mappings_by_tpm(counts_data)
            self._rescue_multi_mappings_by_tpm(
                counts_data, prefix=self.isoform_prefix)
            print("Advanced multi-mapping analysis completed.")

        # ç¬¬ä¸‰é˜¶æ®µ:è®¡ç®—æ­£ç¡®çš„countsï¼Œåˆå¹¶rawå’Œmulti_emï¼Œä»¥åŠmulti_equal çš„counts
        print("Starting third stage: merging counts...")

        # åˆå§‹åŒ–åˆå¹¶è®¡æ•°å™¨
        counts_data[f'{self.isoform_prefix}Final_em'] = Counter()
        counts_data[f'{self.isoform_prefix}Final_eq'] = Counter()

        # 1. åˆå¹¶ unique å’Œ multi_EM è®¡æ•° (Final_em)
        for transcript, count in counts_data[f'{self.isoform_prefix}unique'].items():
            counts_data[f'{self.isoform_prefix}Final_em'][transcript] += count

        for transcript, count in counts_data[f'{self.isoform_prefix}multi_EM'].items():
            counts_data[f'{self.isoform_prefix}Final_em'][transcript] += count

        # 2. åˆå¹¶ unique å’Œ multi_equal è®¡æ•° (Final_eq)
        for transcript, count in counts_data[f'{self.isoform_prefix}unique'].items():
            counts_data[f'{self.isoform_prefix}Final_eq'][transcript] += count

        for transcript, count in counts_data[f'{self.isoform_prefix}multi_equal'].items():
            counts_data[f'{self.isoform_prefix}Final_eq'][transcript] += count

        # éªŒè¯åˆå¹¶ç»“æœ
        total_em = sum(counts_data[f'{self.isoform_prefix}Final_em'].values())
        total_eq = sum(counts_data[f'{self.isoform_prefix}Final_eq'].values())
        total_unique = sum(
            counts_data[f'{self.isoform_prefix}unique'].values())
        total_multi_em = sum(
            counts_data[f'{self.isoform_prefix}multi_EM'].values())
        total_multi_eq = sum(
            counts_data[f'{self.isoform_prefix}multi_equal'].values())

        print("åˆå¹¶éªŒè¯:")
        print(f"  - uniqueè®¡æ•°æ€»è®¡: {total_unique}")
        print(f"  - multi_EMè®¡æ•°æ€»è®¡: {round(total_multi_em)}")
        print(f"  - multi_equalè®¡æ•°æ€»è®¡: {round(total_multi_eq)}")
        print(f"  - Final_emæ€»è®¡: {round(total_em)} ")
        print(f"  - Final_eqæ€»è®¡: {round(total_eq)} ")

        # æ›´æ–°å®ä¾‹å˜é‡
        self.counts_data = counts_data
        self.summary_stats = {
            'total_reads': total_count,
            'unique_mapped': sum(counts_data[f'{self.isoform_prefix}unique'].values()),
            'multi_mapped': sum(counts_data[f'{self.isoform_prefix}multi'].values()),
            'raw': sum(counts_data[f'{self.isoform_prefix}raw'].values()),
            'firstID': sum(counts_data[f'{self.isoform_prefix}firstID'].values()),
            'multi_equal': sum(counts_data[f'{self.isoform_prefix}multi_equal'].values()),
            'multi_EM': sum(counts_data[f'{self.isoform_prefix}multi_EM'].values()),
            'multi_EM_cannot_allocate_tpm': sum(counts_data[f'{self.isoform_prefix}multi_EM_cannot_allocate_tpm'].values()),
            'Final_em': total_em,
            'Final_eq': total_eq,
            'processing_time': time.time() - start_time
        }

        print(
            f"Count generation completed in {self.summary_stats['processing_time']:.2f} seconds")
        print("æœ€ç»ˆè®¡æ•°ç»Ÿè®¡:")
        print(
            f"  - Final_em: {len(counts_data[f'{self.isoform_prefix}Final_em'])} ä¸ªè½¬å½•æœ¬, {round(total_em)} æ¡reads")
        print(
            f"  - Final_eq: {len(counts_data[f'{self.isoform_prefix}Final_eq'])} ä¸ªè½¬å½•æœ¬, {round(total_eq)} æ¡reads")

    def aggregate_gene_level_counts(self):
        """
        åŸºå› æ°´å¹³è®¡æ•°èšåˆ
        """
        if self.annotation_df is None:
            print("Warning: Cannot aggregate gene level counts without annotation data")
            return {}, {}

        print("Aggregating gene level counts...")
        start_time = time.time()

        # åˆ›å»ºè½¬å½•æœ¬åˆ°åŸºå› çš„æ˜ å°„åˆ—è¡¨
        transcript_to_gene = dict(
            zip(self.annotation_df['txname'], self.annotation_df['geneName']))

        # åˆå§‹åŒ–åŸºå› æ°´å¹³è®¡æ•°å™¨
        gene_level_counts_unique_genes = {}
        gene_level_counts_multi_genes = {}

        # åˆå§‹åŒ–æ‰€æœ‰åŸºå› è®¡æ•°ç±»å‹
        for count_type in self.counts_data.keys():
            if count_type.startswith(self.isoform_prefix):
                # å°†isoformæ›¿æ¢ä¸ºgeneçš„è®¡æ•°ç±»å‹
                base_type = count_type.replace(self.isoform_prefix, '')
                gene_level_counts_unique_genes[f'{self.gene_prefix}{base_type}'] = Counter(
                )
                gene_level_counts_multi_genes[f'{self.gene_prefix}{base_type}'] = Counter(
                )

        # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—åŸºå› æ°´å¹³çš„unique readsè®¡æ•°å’Œmultiè®¡æ•°
        # gene_unique_counts = Counter()
        for count_type, counter in self.counts_data.items():
            if not count_type.startswith('isoform_allocation_ratios'):
                pass
            else:
                continue

            # è·å–å¯¹åº”çš„åŸºå› æ°´å¹³è®¡æ•°å™¨ï¼Œå„ç§typeéƒ½æå‰åœ¨è¿™é‡Œè®¾ç½®å¥½äº†
            gene_counter_unique = gene_level_counts_unique_genes.get(
                count_type.replace(self.isoform_prefix,
                                   self.gene_prefix), Counter()
            )
            gene_counter_multi = gene_level_counts_multi_genes.get(
                count_type.replace(self.isoform_prefix,
                                   self.gene_prefix), Counter()
            )

            # å¼€å§‹ä¸åŒçš„typeæŒ¨ä¸ªæä¸€éï¼Œåˆ†åˆ«ä¿å­˜åœ¨ä¸¤ç§åˆ—è¡¨ä¸­ï¼Œuinqueå’Œmulti
            for transcript_ids_str, event_count in counter.items():
                # å¤„ç†è½¬å½•æœ¬IDï¼ˆå¯èƒ½æ˜¯å•ä¸ªæˆ–å¤šä¸ªï¼‰
                if ',' not in transcript_ids_str:
                    # å•æ˜ å°„æƒ…å†µï¼Œè·å–ä¸€ä¸ªgeneå•åï¼ŒcountåŠ åˆ°ä¸€èµ·
                    gene = transcript_to_gene.get(transcript_ids_str)
                    if gene:
                        gene_counter_unique[gene] += event_count
                        # if count_type == f'{self.isoform_prefix}unique':
                        #     gene_unique_counts[gene] += event_count
                else:
                    # å¤šæ˜ å°„æƒ…å†µï¼šå…ˆæ£€æŸ¥æ˜¯å¦æ˜ å°„åˆ°åŒä¸€ä¸ªåŸºå› 
                    transcript_ids = transcript_ids_str.split(',')
                    genes = set()

                    for tid in transcript_ids:
                        gene = transcript_to_gene.get(tid)
                        if gene:
                            genes.add(gene)
                    # æ˜¯åŒä¸€ä¸ªåŸºå› æ¥çš„
                    if len(genes) == 1:
                        # æ˜ å°„åˆ°åŒä¸€ä¸ªåŸºå› ï¼Œå­˜åˆ°uniqueåˆ—è¡¨ï¼Œå¿…é¡»æ˜¯+=æ‰å¯ä»¥
                        gene = list(genes)[0]
                        # è¿™æ¡ï¼ŒåŸæœ¬çš„multiï¼Œæš‚æ—¶å…ˆæ³¨é‡Šæ‰ --20251124
                        gene_counter_unique[gene] += event_count
                        # ç›´æ¥å­˜åˆ°uniqueçš„æ¡ç›®ä¸­ï¼Œè¿™æœ¬æ˜¯isoformä¸­çš„multiï¼Œç°åœ¨geneä¸­æ˜¯uniqueäº†
                        gene_level_counts_unique_genes[f'{self.gene_prefix}unique'][gene] += event_count
                    # ä¸æ˜¯åŒä¸€ä¸ªåŸºå› æ¥çš„ï¼Œå­˜åˆ°multi
                    elif len(genes) > 1:
                        # æ˜ å°„åˆ°å¤šä¸ªåŸºå› 
                        gene_key = ','.join(sorted(genes))
                        gene_counter_multi[gene_key] += event_count

        print(f"åŸºå› æ°´å¹³unique readsè®¡æ•°å®Œæˆ: {len(gene_counter_unique)} ä¸ªåŸºå› ")

        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨åŸºå› æ°´å¹³çš„unique readsè®¡ç®—TPM,æš‚æ—¶ç”¨æœ€é•¿è½¬å½•æœ¬é•¿åº¦
        # 1. æ£€æŸ¥uniqueå’Œgeneæ°´å¹³çš„ä¸œè¥¿éƒ½å­˜åœ¨å“
        if hasattr(self, 'gene_level_counts_unique_genes') and self.gene_level_counts_unique_genes:
            # æ£€æŸ¥æ˜¯å¦æœ‰geneæ°´å¹³çš„multiæ•°æ®
            gene_multi_key = f'{self.gene_prefix}multi'
            if gene_multi_key in self.gene_level_counts_multi_genes and self.gene_level_counts_multi_genes[gene_multi_key]:
                print("Starting advanced multi-mapping analysis for gene level...")

                # ç”¨æœ‰unique readsçš„genesä¸ºgeneæ°´å¹³çš„åŸºå› ä»¬æ„å»ºé•¿åº¦å­—å…¸ï¼ˆè¿™é‡Œç›®å‰é‡‡ç”¨åŸºå› æœ€é•¿è½¬å½•æœ¬é•¿åº¦ï¼Œè¿˜å¯ä»¥é‡‡ç”¨å…¶ä»–ç§ç±»é•¿åº¦æ›¿ä»£ï¼‰
                gene_lengths = {}
                for gene_name in set(self.gene_level_counts_unique_genes[f'{self.gene_prefix}unique'].keys()):
                    gene_transcripts = self.annotation_df[self.annotation_df['geneName'] == gene_name]
                    if not gene_transcripts.empty:
                        max_length = gene_transcripts['txLength'].max()
                        gene_lengths[gene_name] = max_length

                # ä½¿ç”¨é€šç”¨æ–¹æ³•å¤„ç†self.gene_level_counts_multi_genesåˆ—è¡¨é‡Œé¢æ°´å¹³çš„geneæ°´å¹³çš„å¤šæ˜ å°„å¹¶åˆ†é…
                self._rescue_multi_mappings_by_tpm(
                    counts_data=self.gene_level_counts_multi_genes,
                    prefix=self.gene_prefix,
                    length_dict=gene_lengths,
                    annotation_df=self.annotation_df
                )
                print("Gene level advanced multi-mapping analysis completed.")

        # ç¬¬å››æ­¥ï¼šåˆå¹¶è®¡æ•°
        gene_level_counts_unique_genes[f'{self.gene_prefix}Final_em'] = Counter(
        )
        gene_level_counts_unique_genes[f'{self.gene_prefix}Final_eq'] = Counter(
        )

        # 1. åˆå¹¶ unique å’Œ multi_EM è®¡æ•° (æœ€ç»ˆçš„Final_em)
        #Final_em = gene_unique + gene_EM
        for gene, count in gene_level_counts_unique_genes[f'{self.gene_prefix}unique'].items():
            gene_level_counts_unique_genes[f'{self.gene_prefix}Final_em'][gene] += count
        # for gene, count in gene_level_counts_unique_genes[f'{self.gene_prefix}multi'].items():
        #     gene_level_counts_unique_genes[f'{self.gene_prefix}Final_em'][gene] += count
        for gene, count in gene_level_counts_unique_genes[f'{self.gene_prefix}multi_EM'].items():
            gene_level_counts_unique_genes[f'{self.gene_prefix}Final_em'][gene] += count

        # 2. åˆå¹¶ unique å’Œ multi_equal è®¡æ•° (Final_eq)
        #Final_eq = gene_unique+gene_multi_equal
        for gene, count in gene_level_counts_unique_genes[f'{self.gene_prefix}unique'].items():
            gene_level_counts_unique_genes[f'{self.gene_prefix}Final_eq'][gene] += count
        for gene, count in gene_level_counts_unique_genes[f'{self.gene_prefix}multi_equal'].items():
            gene_level_counts_unique_genes[f'{self.gene_prefix}Final_eq'][gene] += count

        processing_time = time.time() - start_time
        print(f"åŸºå› æ°´å¹³èšåˆå®Œæˆï¼Œè€—æ—¶ {processing_time:.2f} ç§’")

        return gene_level_counts_unique_genes, gene_level_counts_multi_genes


# %% run and generate files

    def _generate_isoform_level_files(self, base_name):
        """ç”Ÿæˆè½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶ - ä¿®å¤ç‰ˆ"""
        isoform_files = {}

        try:
            # æ”¶é›†æ‰€æœ‰è½¬å½•æœ¬æ°´å¹³è®¡æ•°ç±»å‹
            isoform_count_types = []
            for count_type in self.counts_data.keys():
                if count_type.startswith(self.isoform_prefix):
                    base_type = count_type.replace(self.isoform_prefix, '')
                    isoform_count_types.append(base_type)

            print(f"æ‰¾åˆ°è½¬å½•æœ¬æ°´å¹³è®¡æ•°ç±»å‹: {isoform_count_types}")

            if not isoform_count_types:
                print("æ²¡æœ‰è½¬å½•æœ¬æ°´å¹³è®¡æ•°æ•°æ®")
                return {}

            # ä½¿ç”¨firstIDä½œä¸ºåŸºç¡€æ•°æ®æ¡†
            firstID_type = f'{self.isoform_prefix}firstID'
            if firstID_type not in self.counts_data or not self.counts_data[firstID_type]:
                print("æ²¡æœ‰firstIDè®¡æ•°æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆè½¬å½•æœ¬æ°´å¹³æ–‡ä»¶")
                return {}

            combined_df = pd.DataFrame(self.counts_data[firstID_type].items(),
                                       columns=['Transcript', 'firstID_count'])

            # åˆå¹¶æ‰€æœ‰è®¡æ•°ç±»å‹
            for count_type in isoform_count_types:
                if count_type == 'firstID':  # å·²ç»ä½œä¸ºåŸºç¡€ï¼Œè·³è¿‡
                    continue

                full_type = f'{self.isoform_prefix}{count_type}'
                if full_type in self.counts_data and self.counts_data[full_type]:
                    temp_df = pd.DataFrame(self.counts_data[full_type].items(),
                                           columns=['Transcript', f'{count_type}_count'])
                    combined_df = combined_df.merge(
                        temp_df, on='Transcript', how='outer')

            # æ·»åŠ æ³¨é‡Šä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.annotation_df is not None:
                # æ£€æŸ¥å¯ç”¨çš„æ³¨é‡Šåˆ—
                available_columns = self.annotation_df.columns.tolist()
                annotation_cols = ['txname', 'geneName']

                # æ·»åŠ å…¶ä»–å¯èƒ½æœ‰ç”¨çš„åˆ—
                optional_cols = ['txLength',
                                 'cdsLength', 'symbol', 'description']
                for col in optional_cols:
                    if col in available_columns:
                        annotation_cols.append(col)

                annotation_subset = self.annotation_df[annotation_cols]
                combined_df = combined_df.merge(
                    annotation_subset,
                    left_on='Transcript',
                    right_on='txname',
                    how='left'
                )

                # ç§»é™¤é‡å¤çš„txnameåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if 'txname' in combined_df.columns and 'Transcript' in combined_df.columns:
                    combined_df = combined_df.drop('txname', axis=1)

            # å¡«å……NaNå€¼ä¸º0
            count_columns = [
                col for col in combined_df.columns if col.endswith('_count')]
            combined_df[count_columns] = combined_df[count_columns].fillna(0)

            # ä¿å­˜æ–‡ä»¶
            combined_filename = self.output_dir / \
                f'{base_name}_isoform_level.counts.csv'
            combined_df.to_csv(combined_filename,
                               index=False, float_format='%.2f')
            isoform_files['isoform'] = combined_filename

            ratios_key = f'{self.isoform_prefix}allocation_ratios'
            if ratios_key in self.counts_data and self.counts_data[ratios_key]:
                rows = []
                for ids_str, maps in self.counts_data[ratios_key].items():
                    ids = ids_str.split(',')
                    em_map = maps.get('EM')
                    equal_map = maps.get('equal')
                    other_map = maps.get('multi2all')
                    em_str = ';'.join([str(em_map.get(t, '')) if em_map is not None else '' for t in ids])
                    eq_str = ';'.join([str(equal_map.get(t, '')) if equal_map is not None else '' for t in ids])
                    oth_str = ';'.join([str(other_map.get(t, '')) if other_map is not None else '' for t in ids])
                    rows.append({
                        'Transcripts': ids_str,
                        'allocation_ratios_count_EM': em_str,
                        'allocation_ratios_count_equal': eq_str,
                        'allocation_ratios_count_multi2all': oth_str
                    })
                iso_multi_df = pd.DataFrame(rows)
                iso_multi_filename = self.output_dir / f'{base_name}_isoform_multi_level.counts.csv'
                iso_multi_df.to_csv(iso_multi_filename, index=False)
                isoform_files['isoform_multi'] = iso_multi_filename

            print(f"è½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {len(combined_df)} ä¸ªè½¬å½•æœ¬")

        except Exception as e:
            print(f"ç”Ÿæˆè½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()

        return isoform_files

    def _generate_gene_level_files(self, base_name):
        """ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ - å®Œæ•´ä¿®å¤ç‰ˆ"""
        if self.annotation_df is None:
            print("æ²¡æœ‰æ³¨é‡Šä¿¡æ¯ï¼Œè·³è¿‡åŸºå› æ°´å¹³æ–‡ä»¶ç”Ÿæˆ")
            return {}

        print("ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶...")

        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥åŸºå› æ°´å¹³æ•°æ®
        print(
            f"gene_level_counts_unique_genes å­˜åœ¨: {hasattr(self, 'gene_level_counts_unique_genes')}")
        if hasattr(self, 'gene_level_counts_unique_genes'):
            print(
                f"gene_level_counts_unique_genes ç±»å‹: {type(self.gene_level_counts_unique_genes)}")
            if self.gene_level_counts_unique_genes:
                print(
                    f"gene_level_counts_unique_genes é”®: {list(self.gene_level_counts_unique_genes.keys())}")
                for key, counter in self.gene_level_counts_unique_genes.items():
                    print(f"  {key}: {len(counter)} ä¸ªæ¡ç›®")
            else:
                print("gene_level_counts_unique_genes ä¸ºç©º")

        print(
            f"gene_level_counts_multi_genes å­˜åœ¨: {hasattr(self, 'gene_level_counts_multi_genes')}")
        if hasattr(self, 'gene_level_counts_multi_genes'):
            print(
                f"gene_level_counts_multi_genes ç±»å‹: {type(self.gene_level_counts_multi_genes)}")
            if self.gene_level_counts_multi_genes:
                print(
                    f"gene_level_counts_multi_genes é”®: {list(self.gene_level_counts_multi_genes.keys())}")
                for key, counter in self.gene_level_counts_multi_genes.items():
                    print(f"  {key}: {len(counter)} ä¸ªæ¡ç›®")
            else:
                print("gene_level_counts_multi_genes ä¸ºç©º")

        gene_files = {}

        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸºå› æ°´å¹³è®¡æ•°æ•°æ®
            has_unique_data = False
            has_multi_data = False

            # æ£€æŸ¥ unique genes æ•°æ®
            if (hasattr(self, 'gene_level_counts_unique_genes') and
                self.gene_level_counts_unique_genes and
                    any(len(counter) > 0 for counter in self.gene_level_counts_unique_genes.values())):
                has_unique_data = True
                print("å‘ç° unique genes æ•°æ®")

            # æ£€æŸ¥ multi genes æ•°æ®
            if (hasattr(self, 'gene_level_counts_multi_genes') and
                self.gene_level_counts_multi_genes and
                    any(len(counter) > 0 for counter in self.gene_level_counts_multi_genes.values())):
                has_multi_data = True
                print("å‘ç° multi genes æ•°æ®")

            if not has_unique_data and not has_multi_data:
                print("æ²¡æœ‰åŸºå› æ°´å¹³è®¡æ•°æ•°æ®ï¼Œè·³è¿‡æ–‡ä»¶ç”Ÿæˆ")
                return {}

            # ç”Ÿæˆå•ä¸ªåŸºå› çš„è®¡æ•°æ–‡ä»¶
            if has_unique_data:
                print("å¼€å§‹ç”Ÿæˆå•ä¸ªåŸºå› è®¡æ•°æ–‡ä»¶...")
                single_gene_data = []

                # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„åŸºå› 
                all_genes = set()
                for counter in self.gene_level_counts_unique_genes.values():
                    if counter:  # ç¡®ä¿è®¡æ•°å™¨éç©º
                        all_genes.update(counter.keys())

                print(f"å¤„ç† {len(all_genes)} ä¸ªå”¯ä¸€åŸºå› ")

                # ä¸ºæ¯ä¸ªåŸºå› æ„å»ºæ•°æ®è¡Œ
                for gene in all_genes:
                    gene_row = {'Gene': gene}

                    # æ”¶é›†è¯¥åŸºå› åœ¨æ‰€æœ‰è®¡æ•°ç±»å‹ä¸­çš„å€¼
                    for count_type, counter in self.gene_level_counts_unique_genes.items():
                        if counter:  # ç¡®ä¿è®¡æ•°å™¨éç©º
                            # æå–åŸºç¡€è®¡æ•°ç±»å‹åç§°ï¼ˆå»æ‰ gene_ å‰ç¼€ï¼‰
                            base_count_type = count_type.replace(
                                self.gene_prefix, '')
                            count_value = counter.get(gene, 0)
                            gene_row[f'{base_count_type}_count'] = count_value

                    single_gene_data.append(gene_row)

                if single_gene_data:
                    # è½¬æ¢ä¸ºDataFrame
                    single_gene_df = pd.DataFrame(single_gene_data)
                    print(f"å•ä¸ªåŸºå› æ•°æ®æ¡†å½¢çŠ¶: {single_gene_df.shape}")

                    # æ·»åŠ åŸºå› æ³¨é‡Šä¿¡æ¯
                    gene_annotation = self._get_gene_annotation_data()
                    if gene_annotation is not None:
                        print(f"åˆå¹¶åŸºå› æ³¨é‡Šä¿¡æ¯ï¼Œæ³¨é‡Šæ•°æ®å½¢çŠ¶: {gene_annotation.shape}")
                        single_gene_df = single_gene_df.merge(
                            gene_annotation,
                            left_on='Gene',
                            right_on='geneName',
                            how='left'
                        )

                        # ç§»é™¤é‡å¤çš„geneNameåˆ—
                        if 'geneName' in single_gene_df.columns and 'Gene' in single_gene_df.columns:
                            single_gene_df = single_gene_df.drop(
                                'geneName', axis=1)

                    # ä¿å­˜æ–‡ä»¶
                    gene_filename = self.output_dir / \
                        f'{base_name}_gene_level.counts.csv'
                    single_gene_df.to_csv(
                        gene_filename, index=False, float_format='%.2f')
                    gene_files['gene'] = gene_filename
                    print(f"å•ä¸ªåŸºå› è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {len(single_gene_df)} ä¸ªåŸºå› ")
                else:
                    print("æ²¡æœ‰å•ä¸ªåŸºå› æ•°æ®å¯ç”Ÿæˆæ–‡ä»¶")

            # ç”Ÿæˆå¤šåŸºå› ç»„åˆçš„è®¡æ•°æ–‡ä»¶
            if has_multi_data:
                print("å¼€å§‹ç”Ÿæˆå¤šåŸºå› ç»„åˆè®¡æ•°æ–‡ä»¶...")
                multi_genes_data = []

                # æ”¶é›†æ‰€æœ‰å¤šåŸºå› ç»„åˆ
                all_multi_combinations = set()
                for counter in self.gene_level_counts_multi_genes.values():
                    if counter:  # ç¡®ä¿è®¡æ•°å™¨éç©º
                        all_multi_combinations.update(counter.keys())

                print(f"å¤„ç† {len(all_multi_combinations)} ä¸ªå¤šåŸºå› ç»„åˆ")

                for gene_combo in all_multi_combinations:
                    combo_row = {'Gene_Combination': gene_combo}

                    # æ”¶é›†è¯¥ç»„åˆåœ¨æ‰€æœ‰è®¡æ•°ç±»å‹ä¸­çš„å€¼
                    for count_type, counter in self.gene_level_counts_multi_genes.items():
                        if counter:  # ç¡®ä¿è®¡æ•°å™¨éç©º
                            base_count_type = count_type.replace(
                                self.gene_prefix, '')
                            count_value = counter.get(gene_combo, 0)
                            combo_row[f'{base_count_type}_count'] = count_value

                    ratios = self.gene_level_counts_multi_genes.get(f'{self.gene_prefix}allocation_ratios', {})
                    if ratios:
                        genes = gene_combo.split(',')
                        if gene_combo in ratios:
                            eq_map = ratios[gene_combo].get('equal')
                            em_map = ratios[gene_combo].get('EM')
                            m2a_map = ratios[gene_combo].get('multi2all')
                            
                            if eq_map:
                                combo_row['allocation_ratios_count_equal'] = ';'.join([f"{eq_map.get(g, '') if g in eq_map else ''}" for g in genes])
                            else:
                                combo_row['allocation_ratios_count_equal'] = ';'.join(['' for _ in genes])
                            
                            if em_map is not None:
                                combo_row['allocation_ratios_count_EM'] = ';'.join([f"{em_map.get(g, '') if g in em_map else ''}" for g in genes])
                            else:
                                combo_row['allocation_ratios_count_EM'] = ';'.join(['' for _ in genes])
                            
                            if m2a_map:
                                combo_row['allocation_ratios_count_multi2all'] = ';'.join([f"{m2a_map.get(g, '') if g in m2a_map else ''}" for g in genes])
                            else:
                                combo_row['allocation_ratios_count_multi2all'] = ';'.join(['' for _ in genes])
                    
                    multi_genes_data.append(combo_row)

                if multi_genes_data:
                    multi_genes_df = pd.DataFrame(multi_genes_data)
                    
                    multi_genes_filename = self.output_dir / \
                        f'{base_name}_multi_genes_level.counts.csv'
                    
                    multi_genes_df.to_csv(
                        multi_genes_filename, index=False, float_format='%.2f')
                   
                    gene_files['multi_genes'] = multi_genes_filename
                    print(f"å¤šåŸºå› ç»„åˆè®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {len(multi_genes_df)} ä¸ªç»„åˆ")
                
                else:
                    print("æ²¡æœ‰å¤šåŸºå› ç»„åˆæ•°æ®å¯ç”Ÿæˆæ–‡ä»¶")

        except Exception as e:
            print(f"ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()

        return gene_files

    def _get_gene_annotation_data(self):
        """è·å–åŸºå› æ³¨é‡Šæ•°æ®"""
        if self.annotation_df is None:
            return None

        # æ£€æŸ¥å¯ç”¨çš„æ³¨é‡Šåˆ—
        available_columns = self.annotation_df.columns.tolist()

        # é€‰æ‹©åŸºå› ç›¸å…³çš„æ³¨é‡Šåˆ—
        selected_cols = ['geneName']

        # æ·»åŠ å…¶ä»–å¯èƒ½æœ‰ç”¨çš„åˆ—
        optional_cols = {
            'symbol': ['symbol', 'genename', 'gene_name'],
            'genelongesttxLength': ['genelongesttxLength', 'genelonesttxlength', 'txLength'],
            'genelongestcdsLength': ['genelongestcdsLength', 'genelongestcdslength', 'cdsLength']
        }

        for target_col, source_cols in optional_cols.items():
            for col in source_cols:
                if col in available_columns:
                    selected_cols.append(col)
                    break

        print(f"ä½¿ç”¨çš„åŸºå› æ³¨é‡Šåˆ—: {selected_cols}")

        # è·å–å»é‡çš„åŸºå› æ³¨é‡Š
        gene_annotation = self.annotation_df[selected_cols].drop_duplicates(subset=[
                                                                            'geneName'])

        # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        rename_map = {}
        if 'genename' in gene_annotation.columns:
            rename_map['genename'] = 'symbol'
        if 'genelonesttxlength' in gene_annotation.columns:
            rename_map['genelonesttxlength'] = 'genelongesttxLength'
        if 'genelongestcdslength' in gene_annotation.columns:
            rename_map['genelongestcdslength'] = 'genelongestcdsLength'

        if rename_map:
            gene_annotation = gene_annotation.rename(columns=rename_map)

        return gene_annotation

    # def _generate_multi_mapping_file(self, base_name):
    #     """ç”Ÿæˆå¤šæ˜ å°„ä¿¡æ¯æ–‡ä»¶"""
    #     if not self.multi_mapping_info:
    #         return None

    #     # åˆ›å»ºå¤šæ˜ å°„ä¿¡æ¯æ•°æ®æ¡†
    #     multi_data = []
    #     for transcript_ids, read_names in self.multi_mapping_info.items():
    #         multi_data.append({
    #             'transcript_ids': transcript_ids,
    #             'read_count': len(read_names),
    #             'read_names': ';'.join(read_names)
    #         })

    #     multi_df = pd.DataFrame(multi_data)
    #     multi_filename = self.output_dir / \
    #         f'{base_name}_multi_mapping_info.csv'
    #     multi_df.to_csv(multi_filename, index=False)

    #     return multi_filename

    def generate_count_files(self):
        """
        ç”Ÿæˆisoform å’Œ gene level è®¡æ•°æ–‡ä»¶
        """
        if self.output_filename:
            base_name = Path(self.output_filename).stem
        else:
            base_name = self.input_file.stem

        count_files = {}

        # ç”Ÿæˆè½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶
        if self.level in ['isoform', 'both']:
            try:
                isoform_files = self._generate_isoform_level_files(base_name)
                count_files.update(isoform_files)
                print("isoform æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                print(f"è½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")

        # ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶
        if self.annotation_df is not None and self.level in ['gene', 'both']:
            try:
                # ä¿®å¤ï¼šæ£€æŸ¥åŸºå› æ°´å¹³è®¡æ•°æ•°æ®æ˜¯å¦å­˜åœ¨?æœ‰å¯èƒ½ç”Ÿæˆäº†isoformï¼Œä½†æ˜¯gffæ–‡ä»¶æ²¡æœ‰ç»™åˆé€‚ï¼Œæ‰€ä»¥åŒ¹é…ä¸åˆ°geneåï¼Œå¯¼è‡´è¿™ä¸ªæƒ…å†µã€‚æš‚æ—¶å…ˆè¿™æ ·å§ï¼Œï¼Œä¸€èˆ¬ä¸ä¼šç¢°åˆ°ã€‚æ‰€ä»¥åˆ¤æ–­æ˜¯å¦å…·æœ‰åŸºå› levelæ•°æ®çš„é‚£éƒ¨åˆ†å…ˆåˆ é™¤
                # has_gene_data = True
                # ä¿®å¤ï¼šæ£€æŸ¥æ­£ç¡®çš„åŸºå› æ°´å¹³æ•°æ®ä½ç½®
                has_gene_data = False

                # æ£€æŸ¥ gene_level_counts_unique_genes
                if hasattr(self, 'gene_level_counts_unique_genes') and self.gene_level_counts_unique_genes:
                    for counter in self.gene_level_counts_unique_genes.values():
                        if counter and len(counter) > 0:  # æ£€æŸ¥è®¡æ•°å™¨æ˜¯å¦éç©º
                            has_gene_data = True
                            break

                # æ£€æŸ¥ gene_level_counts_multi_genes
                if not has_gene_data and hasattr(self, 'gene_level_counts_multi_genes') and self.gene_level_counts_multi_genes:
                    for counter in self.gene_level_counts_multi_genes.values():
                        if counter and len(counter) > 0:  # æ£€æŸ¥è®¡æ•°å™¨æ˜¯å¦éç©º
                            has_gene_data = True
                            break

                print(f'has_gene_data: {has_gene_data}')
                if has_gene_data:
                    # print('has_gene_data', has_gene_data)
                    gene_files = self._generate_gene_level_files(base_name)
                    count_files.update(gene_files)
                    print("åŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
                else:
                    print("æ²¡æœ‰åŸºå› æ°´å¹³è®¡æ•°æ•°æ®ï¼Œè·³è¿‡åŸºå› æ°´å¹³æ–‡ä»¶ç”Ÿæˆ")
            except Exception as e:
                print(f"åŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")

        return count_files
##########################################################################################
# æ–°åŠ çš„ï¼Œçœ‹è¡Œä¸è¡Œï¼Œè¡Œå°±æ³¨é‡Šæ‰ä¸Šé¢çš„20251125

    # def generate_count_files(self):
    #     """
    #     ç”Ÿæˆisoformå’Œgene levelè®¡æ•°æ–‡ä»¶ï¼ˆåŒ…å«åˆ†é…æ¯”ä¾‹ï¼‰
    #     """
    #     if self.output_filename:
    #         base_name = Path(self.output_filename).stem
    #     else:
    #         base_name = self.input_file.stem

    #     count_files = {}

    #     # ç”Ÿæˆè½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶
    #     if self.level in ['isoform', 'both']:
    #         try:
    #             isoform_files = self._generate_isoform_level_files(base_name)
    #             count_files.update(isoform_files)
    #             print("isoformæ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    #         except Exception as e:
    #             print(f"è½¬å½•æœ¬æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")

    #     # ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ï¼ˆä½¿ç”¨æ–°ç‰ˆæœ¬ï¼ŒåŒ…å«åˆ†é…æ¯”ä¾‹ï¼‰
    #     if self.annotation_df is not None and self.level in ['gene', 'both']:
    #         try:
    #             # ä½¿ç”¨æ–°ç‰ˆæœ¬çš„åŸºå› æ°´å¹³æ–‡ä»¶ç”Ÿæˆæ–¹æ³•
    #             gene_files = self._generate_gene_level_files_with_allocation(
    #                 base_name)
    #             count_files.update(gene_files)

    #             # ç”Ÿæˆåˆ†é…æ¯”ä¾‹æ‘˜è¦æŠ¥å‘Š
    #             if hasattr(self, 'counts_data') and f'{self.gene_prefix}allocation_ratios' in self.counts_data:
    #                 report_file = self.generate_allocation_summary_report(
    #                     base_name)
    #                 if report_file:
    #                     count_files['allocation_summary'] = report_file

    #             print("åŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ï¼ˆåŒ…å«åˆ†é…æ¯”ä¾‹ï¼‰ç”Ÿæˆå®Œæˆ")
    #         except Exception as e:
    #             print(f"åŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")

    #     return count_files

    # def _generate_gene_level_files_with_allocation(self, base_name):
    #     """ç”ŸæˆåŒ…å«åˆ†é…æ¯”ä¾‹çš„åŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶"""
    #     if self.annotation_df is None:
    #         print("æ²¡æœ‰æ³¨é‡Šä¿¡æ¯ï¼Œè·³è¿‡åŸºå› æ°´å¹³æ–‡ä»¶ç”Ÿæˆ")
    #         return {}

    #     print("ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶ï¼ˆåŒ…å«åˆ†é…æ¯”ä¾‹ï¼‰...")
    #     gene_files = {}

    #     try:
    #         # æ£€æŸ¥æ˜¯å¦æœ‰åŸºå› æ°´å¹³è®¡æ•°æ•°æ®
    #         has_unique_data = hasattr(
    #             self, 'gene_level_counts_unique_genes') and self.gene_level_counts_unique_genes
    #         has_multi_data = hasattr(
    #             self, 'gene_level_counts_multi_genes') and self.gene_level_counts_multi_genes

    #         if not has_unique_data and not has_multi_data:
    #             print("æ²¡æœ‰åŸºå› æ°´å¹³è®¡æ•°æ•°æ®ï¼Œè·³è¿‡æ–‡ä»¶ç”Ÿæˆ")
    #             return {}

    #         # è·å–åˆ†é…æ¯”ä¾‹æ•°æ®
    #         allocation_ratios = {}
    #         if hasattr(self, 'counts_data') and f'{self.gene_prefix}allocation_ratios' in self.counts_data:
    #             allocation_ratios = self.counts_data[f'{self.gene_prefix}allocation_ratios']
    #             print(f"åŠ è½½äº† {len(allocation_ratios)} ä¸ªåˆ†é…æ¯”ä¾‹è®°å½•")

    #         # ç”Ÿæˆå•ä¸ªåŸºå› çš„è®¡æ•°æ–‡ä»¶
    #         if has_unique_data:
    #             print("å¼€å§‹ç”Ÿæˆå•ä¸ªåŸºå› è®¡æ•°æ–‡ä»¶ï¼ˆåŒ…å«åˆ†é…æ¯”ä¾‹ï¼‰...")
    #             single_gene_data = []

    #             # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„åŸºå› 
    #             all_genes = set()
    #             for counter in self.gene_level_counts_unique_genes.values():
    #                 if counter:
    #                     all_genes.update(counter.keys())

    #             print(f"å¤„ç† {len(all_genes)} ä¸ªå”¯ä¸€åŸºå› ")

    #             # ä¸ºæ¯ä¸ªåŸºå› æ„å»ºæ•°æ®è¡Œ
    #             for gene in all_genes:
    #                 gene_row = {'Gene': gene}

    #                 # æ”¶é›†è¯¥åŸºå› åœ¨æ‰€æœ‰è®¡æ•°ç±»å‹ä¸­çš„å€¼
    #                 for count_type, counter in self.gene_level_counts_unique_genes.items():
    #                     if counter:
    #                         # æå–åŸºç¡€è®¡æ•°ç±»å‹åç§°ï¼ˆå»æ‰ gene_ å‰ç¼€ï¼‰
    #                         base_count_type = count_type.replace(
    #                             self.gene_prefix, '')
    #                         count_value = counter.get(gene, 0)
    #                         gene_row[f'{base_count_type}_count'] = count_value

    #                 # æ·»åŠ åˆ†é…æ¯”ä¾‹ä¿¡æ¯ï¼ˆå•åŸºå› çš„åˆ†é…æ¯”ä¾‹é»˜è®¤ä¸º1.0ï¼‰
    #                 gene_row['equal_allocation_ratio'] = 1.0
    #                 gene_row['em_allocation_ratio'] = 1.0
    #                 gene_row['allocation_method'] = 'single_gene'

    #                 single_gene_data.append(gene_row)

    #             if single_gene_data:
    #                 # è½¬æ¢ä¸ºDataFrame
    #                 single_gene_df = pd.DataFrame(single_gene_data)
    #                 print(f"å•ä¸ªåŸºå› æ•°æ®æ¡†å½¢çŠ¶: {single_gene_df.shape}")

    #                 # æ·»åŠ åŸºå› æ³¨é‡Šä¿¡æ¯
    #                 gene_annotation = self._get_gene_annotation_data()
    #                 if gene_annotation is not None:
    #                     print(f"åˆå¹¶åŸºå› æ³¨é‡Šä¿¡æ¯ï¼Œæ³¨é‡Šæ•°æ®å½¢çŠ¶: {gene_annotation.shape}")
    #                     single_gene_df = single_gene_df.merge(
    #                         gene_annotation,
    #                         left_on='Gene',
    #                         right_on='geneName',
    #                         how='left'
    #                     )

    #                     # ç§»é™¤é‡å¤çš„geneNameåˆ—
    #                     if 'geneName' in single_gene_df.columns and 'Gene' in single_gene_df.columns:
    #                         single_gene_df = single_gene_df.drop(
    #                             'geneName', axis=1)

    #                 # ä¿å­˜æ–‡ä»¶
    #                 gene_filename = self.output_dir / \
    #                     f'{base_name}_gene_level.counts.csv'
    #                 single_gene_df.to_csv(
    #                     gene_filename, index=False, float_format='%.2f')
    #                 gene_files['gene'] = gene_filename
    #                 print(f"å•ä¸ªåŸºå› è®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {len(single_gene_df)} ä¸ªåŸºå› ")
    #             else:
    #                 print("æ²¡æœ‰å•ä¸ªåŸºå› æ•°æ®å¯ç”Ÿæˆæ–‡ä»¶")

    #         # ç”Ÿæˆå¤šåŸºå› ç»„åˆçš„è®¡æ•°æ–‡ä»¶ï¼ˆåŒ…å«è¯¦ç»†çš„åˆ†é…æ¯”ä¾‹ï¼‰
    #         if has_multi_data:
    #             print("å¼€å§‹ç”Ÿæˆå¤šåŸºå› ç»„åˆè®¡æ•°æ–‡ä»¶ï¼ˆåŒ…å«åˆ†é…æ¯”ä¾‹ï¼‰...")
    #             multi_genes_data = []

    #             # æ”¶é›†æ‰€æœ‰å¤šåŸºå› ç»„åˆ
    #             all_multi_combinations = set()
    #             for counter in self.gene_level_counts_multi_genes.values():
    #                 if counter:
    #                     all_multi_combinations.update(counter.keys())

    #             print(f"å¤„ç† {len(all_multi_combinations)} ä¸ªå¤šåŸºå› ç»„åˆ")

    #             for gene_combo in all_multi_combinations:
    #                 combo_row = {'Gene_Combination': gene_combo}

    #                 # æ”¶é›†è¯¥ç»„åˆåœ¨æ‰€æœ‰è®¡æ•°ç±»å‹ä¸­çš„å€¼
    #                 for count_type, counter in self.gene_level_counts_multi_genes.items():
    #                     if counter:
    #                         base_count_type = count_type.replace(
    #                             self.gene_prefix, '')
    #                         count_value = counter.get(gene_combo, 0)
    #                         combo_row[f'{base_count_type}_count'] = count_value

    #                 # æ·»åŠ åˆ†é…æ¯”ä¾‹ä¿¡æ¯
    #                 genes = gene_combo.split(',')

    #                 # æŸ¥æ‰¾å¯¹åº”çš„åˆ†é…æ¯”ä¾‹
    #                 equal_ratio_key = f"{gene_combo}_equal"
    #                 em_ratio_key = f"{gene_combo}_EM"

    #                 if equal_ratio_key in allocation_ratios:
    #                     equal_ratios = allocation_ratios[equal_ratio_key]
    #                     # æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œå¦‚ "geneA:0.4,geneB:0.6"
    #                     equal_ratio_str = ','.join(
    #                         [f"{gene}:{ratio:.3f}" for gene, ratio in equal_ratios.items()])
    #                     combo_row['equal_allocation_ratios'] = equal_ratio_str
    #                 else:
    #                     # é»˜è®¤å¹³å‡åˆ†é…
    #                     equal_share = 1.0 / len(genes)
    #                     equal_ratio_str = ','.join(
    #                         [f"{gene}:{equal_share:.3f}" for gene in genes])
    #                     combo_row['equal_allocation_ratios'] = equal_ratio_str

    #                 if em_ratio_key in allocation_ratios and allocation_ratios[em_ratio_key] is not None:
    #                     em_ratios = allocation_ratios[em_ratio_key]
    #                     em_ratio_str = ','.join(
    #                         [f"{gene}:{ratio:.3f}" for gene, ratio in em_ratios.items()])
    #                     combo_row['em_allocation_ratios'] = em_ratio_str
    #                     combo_row['allocation_method'] = 'EM'
    #                 else:
    #                     # æ— æ³•åˆ†é…TPMçš„æƒ…å†µ
    #                     combo_row['em_allocation_ratios'] = 'N/A'
    #                     combo_row['allocation_method'] = 'equal_or_cannot_allocate'

    #                 # è®¡ç®—åˆ†é…æ¯”ä¾‹çš„æ€»å’Œï¼ˆåº”ä¸º1.0ï¼‰
    #                 if equal_ratio_key in allocation_ratios:
    #                     total_equal = sum(
    #                         allocation_ratios[equal_ratio_key].values())
    #                     combo_row['equal_allocation_sum'] = f"{total_equal:.3f}"

    #                 if em_ratio_key in allocation_ratios and allocation_ratios[em_ratio_key] is not None:
    #                     total_em = sum(
    #                         allocation_ratios[em_ratio_key].values())
    #                     combo_row['em_allocation_sum'] = f"{total_em:.3f}"

    #                 multi_genes_data.append(combo_row)

    #             if multi_genes_data:
    #                 multi_genes_df = pd.DataFrame(multi_genes_data)
    #                 multi_genes_filename = self.output_dir / \
    #                     f'{base_name}_multi_genes_level.counts.csv'
    #                 multi_genes_df.to_csv(
    #                     multi_genes_filename, index=False, float_format='%.2f')
    #                 gene_files['multi_genes'] = multi_genes_filename
    #                 print(f"å¤šåŸºå› ç»„åˆè®¡æ•°æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {len(multi_genes_df)} ä¸ªç»„åˆ")
    #             else:
    #                 print("æ²¡æœ‰å¤šåŸºå› ç»„åˆæ•°æ®å¯ç”Ÿæˆæ–‡ä»¶")

    #         # ç”Ÿæˆåˆ†é…æ¯”ä¾‹è¯¦æƒ…æ–‡ä»¶
    #         if allocation_ratios:
    #             print("å¼€å§‹ç”Ÿæˆåˆ†é…æ¯”ä¾‹è¯¦æƒ…æ–‡ä»¶...")
    #             allocation_detail_data = []

    #             for ratio_key, ratios in allocation_ratios.items():
    #                 if ratios is None:
    #                     # æ— æ³•åˆ†é…çš„æƒ…å†µ
    #                     detail_row = {
    #                         'multi_mapping_event': ratio_key.replace('_equal', '').replace('_EM', ''),
    #                         'allocation_method': ratio_key.split('_')[-1],
    #                         'allocation_ratios': 'N/A (cannot allocate)',
    #                         'genes_involved': ratio_key.split('_')[0],
    #                         'ratio_sum': 'N/A'
    #                     }
    #                 else:
    #                     # æ­£å¸¸åˆ†é…æƒ…å†µ
    #                     genes = list(ratios.keys())
    #                     ratio_str = ','.join(
    #                         [f"{gene}:{ratio:.4f}" for gene, ratio in ratios.items()])
    #                     ratio_sum = sum(ratios.values())

    #                     detail_row = {
    #                         'multi_mapping_event': ratio_key.replace('_equal', '').replace('_EM', ''),
    #                         'allocation_method': ratio_key.split('_')[-1],
    #                         'allocation_ratios': ratio_str,
    #                         'genes_involved': ','.join(genes),
    #                         'ratio_sum': f"{ratio_sum:.4f}",
    #                         'gene_count': len(genes)
    #                     }

    #                 allocation_detail_data.append(detail_row)

    #             if allocation_detail_data:
    #                 allocation_df = pd.DataFrame(allocation_detail_data)
    #                 allocation_filename = self.output_dir / \
    #                     f'{base_name}_allocation_details.csv'
    #                 allocation_df.to_csv(allocation_filename, index=False)
    #                 gene_files['allocation_details'] = allocation_filename
    #                 print(f"åˆ†é…æ¯”ä¾‹è¯¦æƒ…æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {len(allocation_df)} æ¡è®°å½•")

    #         return gene_files

    #     except Exception as e:
    #         print(f"ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    #         import traceback
    #         traceback.print_exc()
    #         return {}

    # def _get_gene_annotation_data(self):
    #     """è·å–åŸºå› æ³¨é‡Šæ•°æ®ï¼ˆåŒ…å«åˆ†é…æ¯”ä¾‹ä¿¡æ¯ï¼‰"""
    #     if self.annotation_df is None:
    #         return None

    #     # æ£€æŸ¥å¯ç”¨çš„æ³¨é‡Šåˆ—
    #     available_columns = self.annotation_df.columns.tolist()

    #     # é€‰æ‹©åŸºå› ç›¸å…³çš„æ³¨é‡Šåˆ—
    #     selected_cols = ['geneName']

    #     # æ·»åŠ å…¶ä»–å¯èƒ½æœ‰ç”¨çš„åˆ—
    #     optional_cols = {
    #         'symbol': ['symbol', 'genename', 'gene_name'],
    #         'genelongesttxLength': ['genelongesttxLength', 'genelonesttxlength', 'txLength'],
    #         'genelongestcdsLength': ['genelongestcdsLength', 'genelongestcdslength', 'cdsLength'],
    #         'description': ['description', 'gene_description', 'product']
    #     }

    #     for target_col, source_cols in optional_cols.items():
    #         for col in source_cols:
    #             if col in available_columns:
    #                 selected_cols.append(col)
    #                 break

    #     print(f"ä½¿ç”¨çš„åŸºå› æ³¨é‡Šåˆ—: {selected_cols}")

    #     # è·å–å»é‡çš„åŸºå› æ³¨é‡Š
    #     gene_annotation = self.annotation_df[selected_cols].drop_duplicates(subset=[
    #                                                                         'geneName'])

    #     # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
    #     rename_map = {}
    #     if 'genename' in gene_annotation.columns:
    #         rename_map['genename'] = 'symbol'
    #     if 'genelonesttxlength' in gene_annotation.columns:
    #         rename_map['genelonesttxlength'] = 'genelongesttxLength'
    #     if 'genelongestcdslength' in gene_annotation.columns:
    #         rename_map['genelongestcdslength'] = 'genelongestcdsLength'

    #     if rename_map:
    #         gene_annotation = gene_annotation.rename(columns=rename_map)

    #     return gene_annotation

    # def _format_allocation_ratios(self, ratios_dict):
    #     """æ ¼å¼åŒ–åˆ†é…æ¯”ä¾‹ä¸ºå­—ç¬¦ä¸²"""
    #     if not ratios_dict:
    #         return "N/A"

    #     ratio_strings = []
    #     for gene, ratio in ratios_dict.items():
    #         ratio_strings.append(f"{gene}:{ratio:.3f}")

    #     return ','.join(ratio_strings)

    # def _calculate_allocation_summary(self, allocation_ratios):
    #     """è®¡ç®—åˆ†é…æ¯”ä¾‹çš„ç»Ÿè®¡æ‘˜è¦"""
    #     if not allocation_ratios:
    #         return {}

    #     summary = {
    #         'total_events': len(allocation_ratios),
    #         'equal_events': 0,
    #         'em_events': 0,
    #         'cannot_allocate_events': 0,
    #         'average_genes_per_event': 0,
    #         'ratio_deviation_stats': {}
    #     }

    #     gene_counts = []
    #     ratio_deviations = []

    #     for key, ratios in allocation_ratios.items():
    #         if ratios is None:
    #             summary['cannot_allocate_events'] += 1
    #             continue

    #         if key.endswith('_equal'):
    #             summary['equal_events'] += 1
    #         elif key.endswith('_EM'):
    #             summary['em_events'] += 1

    #         gene_counts.append(len(ratios))

    #         # è®¡ç®—æ¯”ä¾‹ä¸å¹³å‡åˆ†é…çš„åå·®
    #         if len(ratios) > 1:
    #             equal_share = 1.0 / len(ratios)
    #             deviations = [abs(ratio - equal_share)
    #                           for ratio in ratios.values()]
    #             avg_deviation = sum(deviations) / len(deviations)
    #             ratio_deviations.append(avg_deviation)

    #     if gene_counts:
    #         summary['average_genes_per_event'] = sum(
    #             gene_counts) / len(gene_counts)

    #     if ratio_deviations:
    #         summary['ratio_deviation_stats'] = {
    #             'avg_deviation_from_equal': sum(ratio_deviations) / len(ratio_deviations),
    #             'max_deviation': max(ratio_deviations) if ratio_deviations else 0,
    #             'min_deviation': min(ratio_deviations) if ratio_deviations else 0
    #         }

    #     return summary

    # def generate_allocation_summary_report(self, base_name):
    #     """ç”Ÿæˆåˆ†é…æ¯”ä¾‹æ‘˜è¦æŠ¥å‘Š"""
    #     if not hasattr(self, 'counts_data') or f'{self.gene_prefix}allocation_ratios' not in self.counts_data:
    #         return None

    #     allocation_ratios = self.counts_data[f'{self.gene_prefix}allocation_ratios']
    #     summary = self._calculate_allocation_summary(allocation_ratios)

    #     if not summary:
    #         return None

    #     # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    #     report_lines = [
    #         "åŸºå› æ°´å¹³å¤šæ˜ å°„åˆ†é…æ¯”ä¾‹æ‘˜è¦æŠ¥å‘Š",
    #         "=" * 50,
    #         f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}",
    #         f"è¾“å…¥æ–‡ä»¶: {self.input_file.name}",
    #         f"æ³¨é‡Šæ–‡ä»¶: {self.gxf_file if self.gxf_file else 'N/A'}",
    #         "",
    #         "åˆ†é…äº‹ä»¶ç»Ÿè®¡:",
    #         f"  - æ€»å¤šæ˜ å°„äº‹ä»¶æ•°: {summary['total_events']}",
    #         f"  - å¹³å‡åˆ†é…äº‹ä»¶æ•°: {summary['equal_events']}",
    #         f"  - EMåˆ†é…äº‹ä»¶æ•°: {summary['em_events']}",
    #         f"  - æ— æ³•åˆ†é…äº‹ä»¶æ•°: {summary['cannot_allocate_events']}",
    #         f"  - å¹³å‡æ¯ä¸ªäº‹ä»¶çš„åŸºå› æ•°: {summary['average_genes_per_event']:.2f}",
    #         ""
    #     ]

    #     if summary['ratio_deviation_stats']:
    #         stats = summary['ratio_deviation_stats']
    #         report_lines.extend([
    #             "åˆ†é…æ¯”ä¾‹åå·®ç»Ÿè®¡ (ä¸å¹³å‡åˆ†é…ç›¸æ¯”):",
    #             f"  - å¹³å‡åå·®: {stats['avg_deviation_from_equal']:.4f}",
    #             f"  - æœ€å¤§åå·®: {stats['max_deviation']:.4f}",
    #             f"  - æœ€å°åå·®: {stats['min_deviation']:.4f}",
    #             ""
    #         ])

    #     # æ·»åŠ å‰10ä¸ªåˆ†é…äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯
    #     report_lines.append("å‰10ä¸ªå¤šæ˜ å°„äº‹ä»¶åˆ†é…è¯¦æƒ…:")
    #     report_lines.append("-" * 50)

    #     event_count = 0
    #     for key, ratios in list(allocation_ratios.items())[:10]:
    #         if ratios is None:
    #             report_lines.append(f"{key}: æ— æ³•åˆ†é…TPM")
    #         else:
    #             ratio_str = self._format_allocation_ratios(ratios)
    #             report_lines.append(f"{key}: {ratio_str}")
    #         event_count += 1
    #         if event_count >= 10:
    #             break

    #     # å†™å…¥æ–‡ä»¶
    #     report_filename = self.output_dir / \
    #         f'{base_name}_allocation_summary.txt'
    #     with open(report_filename, 'w', encoding='utf-8') as f:
    #         f.write('\n'.join(report_lines))

    #     print(f"åˆ†é…æ¯”ä¾‹æ‘˜è¦æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_filename}")
    #     return report_filename
    # ä¸»è¿è¡Œè¿›ç¨‹

    def run(self):
        """è¿è¡Œå®Œæ•´çš„è®¡æ•°æµç¨‹"""
        print("=" * 60)
        print("fansetools count - Starting processing")
        print("=" * 60)

        if self.level in ['gene', 'both'] and self.annotation_df is None:
            print("æ³¨æ„ï¼šç”Ÿæˆ gene level counts éœ€è¦æä¾› --gxf gff/gtf æ–‡ä»¶")
            return {}

        # 1. è§£æfanse3æ–‡ä»¶å¹¶ç›´æ¥è·å¾—è®¡æ•°
        # counts_data, total_count = self.parse_fanse_file()
        counts_data, total_count = self.parse_fanse_file_optimized_final()

        # 2. ç”Ÿæˆisoformæ°´å¹³è®¡æ•°
        self.generate_isoform_level_counts(counts_data, total_count)

        # 3. ç”ŸæˆåŸºå› æ°´å¹³è®¡æ•°,åŸºå› æ°´å¹³èšåˆï¼ˆå¦‚æœæœ‰æ³¨é‡Šï¼‰
        if self.annotation_df is not None and self.level in ['gene', 'both']:
            gene_level_counts_unique_genes, gene_level_counts_multi_genes = self.aggregate_gene_level_counts()

            # ä¿®å¤ï¼šç¡®ä¿æ­£ç¡®å­˜å‚¨åˆ°å®ä¾‹å˜é‡
            self.gene_level_counts_unique_genes = gene_level_counts_unique_genes
            self.gene_level_counts_multi_genes = gene_level_counts_multi_genes

            if self.gene_level_counts_unique_genes:
                print(
                    f"Gene level aggregation completed: {len(self.gene_level_counts_unique_genes)} unique-gene count types")
            if self.gene_level_counts_multi_genes:
                print(
                    f"Gene level aggregation completed: {len(self.gene_level_counts_multi_genes)} multi-gene count types")
        else:
            print("No annotation provided, skipping gene level aggregation")
            self.gene_level_counts_unique_genes = {}
            self.gene_level_counts_multi_genes = {}

        # 4. ç”Ÿæˆè®¡æ•°æ–‡ä»¶
        count_files = self.generate_count_files()

        # 5. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        self.generate_summary()

        print("fansetools count - Processing completed")
        print("=" * 60)

        return count_files

    def generate_summary(self):
        """ç”Ÿæˆå¤„ç†æ‘˜è¦"""
        summary_file = self.output_dir / f"{self.input_file.stem}_summary.txt"

        with open(summary_file, 'w') as f:
            f.write("fansetools count - Processing Summary\n")
            f.write("=" * 50 + "\n")
            f.write(f"Input file: {self.input_file}\n")
            f.write(f"Output directory: {self.output_dir}\n")
            f.write(
                f"Processing mode: {'Paired-end' if self.paired_end else 'Single-end'}\n")
            f.write(f"Level parameter: {self.level}\n")
            f.write(f"Annotation provided: {self.annotation_df is not None}\n")

            if self.annotation_df is not None:
                f.write(f"Annotation transcripts: {len(self.annotation_df)}\n")
                f.write(
                    f"Annotation genes: {self.annotation_df['geneName'].nunique()}\n")

            f.write("\nStatistics:\n")
            for stat, value in self.summary_stats.items():
                f.write(f"{stat}: {value}\n")

            f.write(f"\nMulti-mapping statistics:\n")
            f.write(
                f"Multi-mapping events: {len(self.counts_data['multi'])}\n")
            if self.counts_data['multi']:
                total_multi_reads = sum(self.counts_data['multi'].values())
                avg_reads_per_event = total_multi_reads / \
                    len(self.counts_data['multi'])
                f.write(f"Total multi-mapped reads: {total_multi_reads}\n")
                f.write(
                    f"Average reads per multi-mapping event: {avg_reads_per_event:.2f}\n")

    def debug_gene_level_data(self):
        """è°ƒè¯•åŸºå› æ°´å¹³æ•°æ®"""
        print("=== è°ƒè¯•åŸºå› æ°´å¹³æ•°æ® ===")

        # æ£€æŸ¥å®ä¾‹å˜é‡
        print(
            f"gene_level_counts_unique_genes å­˜åœ¨: {hasattr(self, 'gene_level_counts_unique_genes')}")
        if hasattr(self, 'gene_level_counts_unique_genes'):
            print(f"ç±»å‹: {type(self.gene_level_counts_unique_genes)}")
            if isinstance(self.gene_level_counts_unique_genes, dict):
                print(f"é”®æ•°é‡: {len(self.gene_level_counts_unique_genes)}")
                for key, value in self.gene_level_counts_unique_genes.items():
                    if hasattr(value, '__len__'):
                        print(f"  {key}: {len(value)} ä¸ªæ¡ç›®")
                    else:
                        print(f"  {key}: {type(value)}")
            else:
                print(f"å€¼: {self.gene_level_counts_unique_genes}")

        print(
            f"gene_level_counts_multi_genes å­˜åœ¨: {hasattr(self, 'gene_level_counts_multi_genes')}")
        if hasattr(self, 'gene_level_counts_multi_genes'):
            print(f"ç±»å‹: {type(self.gene_level_counts_multi_genes)}")
            if isinstance(self.gene_level_counts_multi_genes, dict):
                print(f"é”®æ•°é‡: {len(self.gene_level_counts_multi_genes)}")
                for key, value in self.gene_level_counts_multi_genes.items():
                    if hasattr(value, '__len__'):
                        print(f"  {key}: {len(value)} ä¸ªæ¡ç›®")
                    else:
                        print(f"  {key}: {type(value)}")
            else:
                print(f"å€¼: {self.gene_level_counts_multi_genes}")

# %% some other function


def print_mini_fansetools():
    """
    æœ€å°çš„å¯è¯†åˆ«ç‰ˆæœ¬
    https://www.ascii-art-generator.org/
    """
    # mini_art = [
    #     '''
    #     #######                                #######
    #     #         ##   #    #  ####  ######       #     ####   ####  #       ####
    #     #        #  #  ##   # #      #            #    #    # #    # #      #
    #     #####   #    # # #  #  ####  #####        #    #    # #    # #       ####
    #     #       ###### #  # #      # #            #    #    # #    # #           #
    #     #       #    # #   ## #    # #            #    #    # #    # #      #    #
    #     #       #    # #    #  ####  ######       #     ####   ####  ######  ####
    #     '''
    # ]

    mini_art = ['''
     FANSeTools - Summary the RNA-seq Count
     ''']

    for line in mini_art:
        print(line)


def load_annotation_data(args):
    """åŠ è½½æ³¨é‡Šæ•°æ®"""
    if not args.gxf:
        print("é”™è¯¯: éœ€è¦æä¾› --gxf å‚æ•°")
        return None

    print(f"\nLoading annotation from {args.gxf}")

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåçš„refflatæ–‡ä»¶
    refflat_file = os.path.splitext(args.gxf)[0] + ".genomic.refflat"

    if os.path.exists(refflat_file):
        print(f"Found existing refflat file: {refflat_file}")
        try:
            annotation_df = read_refflat_with_commented_header(refflat_file)
            print(
                f"Successfully loaded {len(annotation_df)} transcripts from existing refflat file")
            return annotation_df
        except Exception as e:
            print(f"Error loading refflat file: {e}")
            print("Converting GXF file instead...")

    print(f"No existing refflat file found at {refflat_file}")
    print("Converting GXF file to refflat format...")

    if args.annotation_output:
        # Generate both genomic and RNA coordinate files
        genomic_df, rna_df = convert_gxf_to_refflat(
            args.gxf, args.annotation_output, add_header=True
        )
        return genomic_df
    else:
        # Just load the data without saving
        genomic_df = load_annotation_to_dataframe(args.gxf)
        return genomic_df


# æ–¹æ³•1ï¼šå…ˆè¯»å–æ³¨é‡Šè¡Œè·å–åˆ—åï¼Œç„¶åè¯»å–æ•°æ®
def read_refflat_with_commented_header(file_path):
    """è¯»å–å¸¦æœ‰æ³¨é‡Šå¤´éƒ¨çš„refflatæ–‡ä»¶"""
    # é¦–å…ˆè¯»å–æ³¨é‡Šè¡Œè·å–åˆ—å
    with open(file_path, 'r') as f:
        header_line = None
        for line in f:
            if line.startswith('#'):
                header_line = line.strip()
                break

    if header_line:
        # æå–åˆ—åï¼ˆå»æ‰#å’Œç©ºæ ¼ï¼‰
        columns = header_line[1:].strip().split('\t')
        # è¯»å–æ•°æ®ï¼Œè·³è¿‡æ³¨é‡Šè¡Œ
        df = pd.read_csv(file_path, sep='\t', comment='#',
                         header=None, names=columns)
    else:
        # å¦‚æœæ²¡æœ‰æ³¨é‡Šå¤´éƒ¨ï¼Œä½¿ç”¨é»˜è®¤åˆ—å
        default_columns = [
            "geneName", "txname", "chrom", "strand", "txStart", "txEnd",
            "cdsStart", "cdsEnd", "exonCount", "exonStarts", "exonEnds",
            "symbol", "g_biotype", "t_biotype", "description", "protein_id",
            "txLength", "cdsLength", "utr5Length", "utr3Length",
            "genelongesttxLength", "genelongestcdsLength", "geneEffectiveLength"
        ]
        df = pd.read_csv(file_path, sep='\t', header=None,
                         names=default_columns)

    return df


def add_count_subparser(subparsers):
    """å‘½ä»¤è¡Œä¸»å‡½æ•°"""

    parser = subparsers.add_parser(
        'count',
        help='è¿è¡ŒFANSe to countï¼Œè¾“å‡ºreadcount',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
        ä½¿ç”¨ç¤ºä¾‹:
            é»˜è®¤isoform level
          å•ä¸ªæ–‡ä»¶å¤„ç†:
            fanse count -i sample.fanse3 -o results/ --gxf annotation.gtf

          æ‰¹é‡å¤„ç†ç›®å½•ä¸­æ‰€æœ‰fanse3æ–‡ä»¶:
            fanse count -i /data/*.fanse3 -o /output/ --gxf annotation.gtf

          åŒç«¯æµ‹åºæ•°æ®:
            fanse count -i R1.fanse3 -r R2.fanse3 -o results/ --gxf annotation.gtf

        **å¦‚éœ€è¦åŸºå› æ°´å¹³è®¡æ•°ï¼Œéœ€è¦è¾“å…¥gtf/gff/refflat/ç®€å•g-tå¯¹åº”æ–‡ä»¶ï¼Œ--gxféƒ½å¯ä»¥è§£æ
          åŸºå› æ°´å¹³è®¡æ•°:
            fanse count -i *.fanse3 -o results/ --gxf annotation.gtf --level gene

          åŒæ—¶è¾“å‡ºåŸºå› å’Œè½¬å½•æœ¬æ°´å¹³:
            fanse count -i *.fanse3 -o results/ --gxf annotation.gtf --level both

          å¤„ç†ä¸­æ–­åé‡æ–°è¿è¡Œï¼ˆè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶[è¾“å‡ºæ–‡ä»¶å¤¹ä¸­å­˜åœ¨å¯¹åº”ç»“æœæ–‡ä»¶ï¼Œéœ€é‡æ–°è¿è¡Œè¯·åˆ é™¤]ï¼‰
            fanse count -i *.fanse3 -o results/ --gxf annotation.gtf --resume

            # æŒ‡å®š4ä¸ªå¹¶è¡Œè¿›ç¨‹
            fanse count -i "*.fanse3" -o results --gxf annotation.gtf --p 4

            ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒå¹¶è¡Œå¤„ç†:
            fanse count -i *.fanse3 -o results --gxf annotation.gtf -p 0
                """
    )

    parser.add_argument('-i', '--input', required=True,
                        help='Input fanse3 file,è¾“å…¥FANSe3æ–‡ä»¶/ç›®å½•/é€šé…ç¬¦ï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰')
    parser.add_argument('-r', '--paired-end',
                        help='Paired-end fanse3 file (optional)')
    parser.add_argument('-o', '--output', required=False,
                        help='Output directory,è¾“å‡ºè·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰')

    # parser.add_argument('--minreads', type=int, default=0,
    #                     help='Minimum reads threshold for filtering')
    parser.add_argument('--rpkm', type=float, default=0,
                        help='RPKM threshold for filteringï¼Œå°šæœªå®Œæˆ')

    parser.add_argument('--gxf', required=False,
                        help='Input GXF file (GTF or GFF3),if not provided, just give out isoform level readcounts')
    parser.add_argument('--annotation-output',
                        help='Output refFlat file prefix (optional)')

    parser.add_argument('--level', choices=['gene', 'isoform', 'both'], default='gene',
                        help='Counting level')

    parser.add_argument('--resume', required=False, action='store_true',
                        help='å¯ä»ä¸Šæ¬¡è¿è¡Œæ–­æ‰çš„åœ°æ–¹è‡ªåŠ¨å¼€å§‹ï¼Œè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰è¾“å…¥æ–‡ä»¶å¯¹åº”çš„ç»“æœæ–‡ä»¶ï¼Œæœ‰åˆ™è·³è¿‡')

    parser.add_argument('-p', '--processes',  type=int, default=1,
                        help='å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°, 1=ä¸²è¡Œ)')

    # æ ¹æ®æ˜¯å¦å¹¶è¡Œé€‰æ‹©æ‰§è¡Œå‡½æ•°
    def count_main_wrapper(args):
        if getattr(args, 'processes', None) != 1:  # ä¸æ˜¯æ˜ç¡®è®¾ç½®ä¸º1
            return count_main_parallel(args)
        else:
            return count_main(args)  # åŸæœ‰çš„ä¸²è¡Œç‰ˆæœ¬

    # å…³é”®ä¿®å¤ï¼šè®¾ç½®å¤„ç†å‡½æ•°ï¼Œè€Œä¸æ˜¯ç›´æ¥è§£æå‚æ•°
    parser.set_defaults(func=count_main)


def main():
    """ä¸»å‡½æ•° - ç”¨äºç›´æ¥è¿è¡Œæ­¤è„šæœ¬"""
    parser = argparse.ArgumentParser(
        description='fansetools count - Process fanse3 files for read counting'
    )

    # æ·»åŠ å­å‘½ä»¤
    subparsers = parser.add_subparsers(
        dest='command', help='Available commands')
    add_count_subparser(subparsers)

    args = parser.parse_args()

    if hasattr(args, 'func'):
        args.func(args)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()

    def test_gene_level_counting():
        """æµ‹è¯•åŸºå› æ°´å¹³è®¡æ•°åŠŸèƒ½"""
        import tempfile
        import shutil

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        print(f"æµ‹è¯•ç›®å½•: {temp_dir}")

        try:
            # æµ‹è¯•æ–‡ä»¶è·¯å¾„
            # fanse_file = r"\\fs2\D\DATA\Zhaojing\3.fanse3_result\old_s14\26.9311-Endosperm_RNC_R1_trimmed.fanse3"
            fanse_file = r'\\fs2\D\DATA\ZhaoJing\0.test\test1.fanse3'
            refflat_file = r'\\fs2\D\DATA\Zhaojing\202209æ•°æ®æ±‡ç¼´\0.Ref_seqs\20251024Oryza_sativa.IRGSP_9311.rna.refflat'
            # åˆ›å»ºæ¨¡æ‹Ÿçš„æ³¨é‡Šæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰

            # annotation_df = load_annotation_data(gtf_file )
            annotation_df = read_refflat_with_commented_header(refflat_file)

            # åˆ›å»ºè®¡æ•°å™¨å®ä¾‹
            counter = FanseCounter(
                input_file=fanse_file,
                output_dir=temp_dir,
                level='both',
                # minreads=0,
                gxf_file=None,
                annotation_df=annotation_df
            )

            print("å¼€å§‹è§£æfanseæ–‡ä»¶...")
            # counter.parse_fanse_file()
            # counter.generate_isoform_level_counts()
            counts_data, total_count = counter.parse_fanse_file_optimized_final()
            # counter.generate_isoform_level_counts(counts_data, total_count)  # ä¼ é€’å‚æ•°

            print(f"è§£æå®Œæˆï¼Œå…± {total_count} æ¡è®°å½•")
            print(f"è®¡æ•°æ•°æ®åŒ…å« {len(counts_data)} ç§è®¡æ•°ç±»å‹")

            # # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            # for count_type, counter_obj in counts_data.items():
            #     if counter_obj:  # åªæ˜¾ç¤ºéç©ºçš„è®¡æ•°å™¨
            #         print(f"{count_type}: {len(counter_obj)} ä¸ªè½¬å½•æœ¬")
            #         # æ˜¾ç¤ºå‰5ä¸ªæœ€é«˜è®¡æ•°çš„è½¬å½•æœ¬
            #         top5 = counter_obj.most_common(5)
            #         print(f"  å‰5ä¸ªè½¬å½•æœ¬: {top5}")

            print("\nå¼€å§‹ç”Ÿæˆisoformæ°´å¹³è®¡æ•°...")
            # æ­£ç¡®è°ƒç”¨ï¼šä¼ é€’å‚æ•°
            counter.generate_isoform_level_counts(counts_data, total_count)

            print("å¼€å§‹åŸºå› æ°´å¹³èšåˆ...")
            counter.gene_level_counts_unique_genes, counter.gene_level_counts_multi_genes = counter.aggregate_gene_level_counts()

            # if gene_counts_unique_genes:
            #     print("\nåŸºå› æ°´å¹³è®¡æ•°ç»Ÿè®¡:")
            #     for count_type, gene_counter in gene_counts_unique_genes.items():
            #         if gene_counter:  # åªæ˜¾ç¤ºéç©ºçš„è®¡æ•°å™¨
            #             print(f"{count_type}: {len(gene_counter)} ä¸ªåŸºå› ")
            #             top5_genes = gene_counter.most_common(5)
            #             print(f"  å‰5ä¸ªåŸºå› : {top5_genes}")

            # åœ¨ generate_count_files æ–¹æ³•å¼€å§‹å¤„æ·»åŠ 
            counter.debug_gene_level_data()
            print("\nç”Ÿæˆè®¡æ•°æ–‡ä»¶...")
            count_files = counter.generate_count_files()
            print(f"ç”Ÿæˆçš„æ–‡ä»¶: {list(count_files.keys())}")

            print("\nè§£æç»Ÿè®¡:")
            print(f"æ€»readsæ•°: {counter.summary_stats['total_reads']}")
            print(f"å”¯ä¸€æ˜ å°„reads: {counter.summary_stats['unique_mapped']}")
            print(f"å¤šæ˜ å°„reads: {counter.summary_stats['multi_mapped']} PEI25k ")

            print("\nè½¬å½•æœ¬æ°´å¹³è®¡æ•°ç»Ÿè®¡:")
            for count_type, counter_data in counter.counts_data.items():
                print(f"{count_type}: {len(counter_data)} ç»„è½¬å½•æœ¬ID")
            #     if len(counter_data) > 0:
            #         top5 = counter_data.most_common(5)
            #         print(f"  å‰5ä¸ª: {top5}")
###################################################################################################
            print("\nè¿›è¡ŒåŸºå› æ°´å¹³èšåˆ...")
            counter.annotation_df = annotation_df
            gene_counts = counter.aggregate_gene_level_counts()

            if gene_counts:
                print("\nåŸºå› æ°´å¹³è®¡æ•°ç»Ÿè®¡:")
                for count_type, gene_counter in gene_counts.items():
                    print(f"{count_type}: {len(gene_counter)} ä¸ªåŸºå› ")
                    # if len(gene_counter) > 0:
                    #     top5 = gene_counter.most_common(5)
                    #     print(f"  å‰5ä¸ªåŸºå› : {top5}")

            print("\nç”Ÿæˆè®¡æ•°æ–‡ä»¶...")
            count_files = counter.generate_count_files()
            print(f"ç”Ÿæˆçš„æ–‡ä»¶: {list(count_files.keys())}")

            # éªŒè¯æ–‡ä»¶å†…å®¹
            for file_type, file_path in count_files.items():
                if file_path.exists():
                    df = pd.read_csv(file_path)
                    print(f"\n{file_type} æ–‡ä»¶ä¿¡æ¯:")
                    print(f"  è¡Œæ•°: {len(df)}")
                    print(f"  åˆ—æ•°: {len(df.columns)}")
                    if len(df) > 0:
                        print(f"  å‰3è¡Œ:")
                        print(df.head(3))

            # æ£€æŸ¥å¤šæ˜ å°„ä¿¡æ¯
            if counter.multi_mapping_info:
                print(f"\nå¤šæ˜ å°„äº‹ä»¶æ•°é‡: {len(counter.multi_mapping_info)}")
                multi_events = list(counter.multi_mapping_info.items())[:3]
                for transcript_ids, read_names in multi_events:
                    print(
                        f"  è½¬å½•æœ¬: {transcript_ids}, readsæ•°: {len(read_names)}")

            print("\næµ‹è¯•å®Œæˆ!")

        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_dir, ignore_errors=True)
            print(f"\næ¸…ç†æµ‹è¯•ç›®å½•: {temp_dir}")

    # def debug_gene_aggregation():
    #     """è°ƒè¯•åŸºå› èšåˆåŠŸèƒ½"""
    #     # åˆ›å»ºæµ‹è¯•æ•°æ®
    #     test_counts = {
    #         'firstID': Counter({
    #             'transcript1': 100,
    #             'transcript2': 50,
    #             'transcript3': 75,
    #             'transcript4': 25
    #         }),
    #         'multi': Counter({
    #             'transcript1,transcript2': 30,
    #             'transcript3,transcript4': 20
    #         })
    #     }

    #     test_annotation = pd.DataFrame({
    #         'txname': ['transcript1', 'transcript2', 'transcript3', 'transcript4'],
    #         'geneName': ['geneA', 'geneA', 'geneB', 'geneB']
    #     })

    #     # æ‰‹åŠ¨æµ‹è¯•èšåˆé€»è¾‘
    #     transcript_to_gene = dict(zip(test_annotation['txname'], test_annotation['geneName']))
    #     print("è½¬å½•æœ¬åˆ°åŸºå› çš„æ˜ å°„:", transcript_to_gene)

    #     for count_type, counter in test_counts.items():
    #         print(f"\nå¤„ç† {count_type}:")
    #         gene_counter = Counter()

    #         for transcript_ids_str, count in counter.items():
    #             print(f"  å¤„ç† '{transcript_ids_str}': è®¡æ•°={count}")

    #             if ',' in transcript_ids_str:
    #                 transcript_ids = transcript_ids_str.split(',')
    #                 print(f"    å¤šæ˜ å°„è½¬å½•æœ¬: {transcript_ids}")

    #                 gene_counts = {}
    #                 for tid in transcript_ids:
    #                     gene = transcript_to_gene.get(tid)
    #                     if gene:
    #                         gene_counts[gene] = gene_counts.get(gene, 0) + 1

    #                 print(f"    åŸºå› åˆ†å¸ƒ: {gene_counts}")

    #                 if gene_counts:
    #                     for gene, gene_count in gene_counts.items():
    #                         allocation = count * (gene_count / len(transcript_ids))
    #                         gene_counter[gene] += allocation
    #                         print(f"    åˆ†é…ç»™åŸºå›  {gene}: {allocation}")
    #             else:
    #                 gene = transcript_to_gene.get(transcript_ids_str)
    #                 if gene:
    #                     gene_counter[gene] += count
    #                     print(f"    å•æ˜ å°„: åŸºå›  {gene} å¢åŠ  {count}")

    #         print(f"  æœ€ç»ˆåŸºå› è®¡æ•°: {dict(gene_counter)}")

    # if __name__ == '__main__':
    #     # è¿è¡Œæµ‹è¯•
    #     print("=" * 60)
    #     print("å¼€å§‹æµ‹è¯•åŸºå› æ°´å¹³è®¡æ•°åŠŸèƒ½")
    #     print("=" * 60)

    #     # å…ˆè¿è¡Œè°ƒè¯•
    #     debug_gene_aggregation()

    #     print("\n" + "=" * 60)
    #     print("å¼€å§‹å®Œæ•´æµ‹è¯•")
    #     print("=" * 60)

    #     # è¿è¡Œå®Œæ•´æµ‹è¯•
    #     test_gene_level_counting()
